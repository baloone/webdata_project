{"id":"319186942_Faculty_citation_measures_are_highly_correlated_with_peer_assessment_of_computer_science_doctoral_programs","abstract":"We study relationship between peer assessment of quality of U.S. Computer Science (CS) doctoral programs and objective measures of research strength of those programs. In Fall 2016 we collected Google Scholar citation data for 4,352 tenure-track CS faculty from 173 U.S. universities. The citations are measured by the t10 index, which represents the number of citations received by the 10th highest cited paper of a faculty. To measure the research strength of a CS doctoral program we use 2 groups of citation measures. The first group of measures averages t10 of faculty in a program. Pearson correlation of those measures with the peer assessment of U.S. CS doctoral programs published by the U.S. News in 2014 is as high as 0.890. The second group of measures counts the number of well cited faculty in a program. Pearson correlation of those measures with the peer assessment is as high as 0.909. By combining those two groups of measures using linear regression, we create the Scholar score whose Pearson correlation with the peer assessment is 0.933 and which explains 87.2% of the variance in the peer assessment. Our evaluation shows that the highest 62 ranked CS doctoral programs by the U.S. News peer assessment are much higher correlated with the Scholar score than the next 57 ranked programs, indicating the deficiencies of peer assessment of less-known CS programs. Our results also indicate that university reputation might have a sizeable impact on peer assessment of CS doctoral programs. To promote transparency, the raw data and the codes used in this study are made available to research community at http://www.dabi.temple.edu/~vucetic/CSranking/.","authors":["Slobodan Vucetic","Ashis Kumar Chanda","Shanshan Zhang","Tian Bai"],"meta":["August 2017"],"references":["275335177_The_Leiden_Manifesto_for_research_metrics","248876087_Learning_to_Live_with_League_Tables_and_Ranking_The_Experience_of_Institutional_Leaders","241802208_Are_Rankings_Reshaping_Higher_Education","313408676_The_Dilemmas_of_Ranking","279968736_A_review_of_the_literature_on_citation_impact_indicators","271867831_The_Academic_Ranking_of_World_Universities","260641650_Editorial_The_Ranking_Game","257692380_The_influences_of_counting_methods_on_university_rankings_based_on_paper_count_and_citation_count","256477214_The_Google_Scholar_Experiment_How_to_Index_False_Papers_and_Manipulate_Bibliometric_Indicators","247557936_On_The_Rationale_of_Group_Decision_Making","221670159_Reproducibility_of_the_Shanghai_academic_ranking_of_world_universities","220365497_Comparing_university_rankings","220365050_Which_h-index-A_comparison_of_WoS_Scopus_and_Google_Scholar","227348253_Rickety_numbers_Volatility_of_university_rankings_and_policy_implications","220425105_Viewpoint_Doctoral_Program_Rankings_for_US_Computing_Programs_The_National_Research_Council_Strikes_Out"]}