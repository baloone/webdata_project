{"id":"273067823_Improved_Semantic_Representations_From_Tree-Structured_Long_Short-Term_Memory_Networks","abstract":"A Long Short-Term Memory (LSTM) network is a type of recurrent neural network\narchitecture which has recently obtained strong results on a variety of\nsequence modeling tasks. The only underlying LSTM structure that has been\nexplored so far is a linear chain. However, natural language exhibits syntactic\nproperties that would naturally combine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to tree-structured network topologies.\nTree-LSTMs outperform all existing systems and strong LSTM baselines on two\ntasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task\n1) and sentiment classification (Stanford Sentiment Treebank).","authors":["Kai Sheng Tai","Richard Socher","Christopher D. Manning"],"meta":["February 2015","DOI:10.3115/v1/P15-1150","SourcearXiv"],"references":["319770160_Show_and_Tell_A_Neural_Image_Caption_Generator","307747289_Show_and_tell_A_neural_image_caption_generator","284576917_Glove_Global_Vectors_for_Word_Representation","329975395_Grounded_Compositional_Semantics_for_Finding_and_Describing_Images_with_Sentences","319770465_Sequence_to_Sequence_Learning_with_Neural_Networks","301409028_ECNU_One_Stone_Two_Birds_Ensemble_of_Heterogenous_Measures_for_Semantic_Relatedness_and_Textual_Entailment","301408986_Illinois-LH_A_Denotational_and_Distributional_Approach_to_Semantics","299487682_A_Fast_and_Accurate_Dependency_Parser_using_Neural_Networks","286964721_Deep_recursive_neural_networks_for_compositionality_in_language","284039049_Recursive_deep_models_for_semantic_compositionality_over_a_sentiment_treebank"]}