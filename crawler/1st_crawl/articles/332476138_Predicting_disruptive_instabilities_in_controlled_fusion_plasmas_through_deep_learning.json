{"id":"332476138_Predicting_disruptive_instabilities_in_controlled_fusion_plasmas_through_deep_learning","abstract":"Nuclear fusion power delivered by magnetic-confinement tokamak reactors holds the promise of sustainable and clean energy¹. The avoidance of large-scale plasma instabilities called disruptions within these reactors2,3 is one of the most pressing challenges4,5, because disruptions can halt power production and damage key components. Disruptions are particularly harmful for large burning-plasma systems such as the multibillion-dollar International Thermonuclear Experimental Reactor (ITER) project⁶ currently under construction, which aims to be the first reactor that produces more power from fusion than is injected to heat the plasma. Here we present a method based on deep learning for forecasting disruptions. Our method extends considerably the capabilities of previous strategies such as first-principles-based⁵ and classical machine-learning7–11 approaches. In particular, it delivers reliable predictions for machines other than the one on which it was trained—a crucial requirement for future large reactors that cannot afford training disruptions. Our approach takes advantage of high-dimensional training data to boost predictive performance while also engaging supercomputing resources at the largest scale to improve accuracy and speed. Trained on experimental data from the largest tokamaks in the United States (DIII-D¹²) and the world (Joint European Torus, JET¹³), our method can also be applied to specific tasks such as prediction with long warning times: this opens up the possibility of moving from passive disruption prediction to active reactor control and optimization. These initial results illustrate the potential for deep learning to accelerate progress in fusion-energy science and, more generally, in the understanding and prediction of complex physical systems.","authors":["Julian Kates-Harbeck","Alexey Svyatkovskiy","William Tang"],"meta":["April 2019Nature 568(7753)","DOI:10.1038/s41586-019-1116-4"],"references":["317606221_Overview_of_the_JET_results_in_support_to_ITER","309155047_Accelerating_recurrent_neural_network_training_using_sequence_bucketing_and_multi-GPU_data_parallelization","320759925_Training_distributed_deep_recurrent_neural_networks_with_mixed_precision_on_GPU_clusters","319770315_Deep_Image_Scaling_up_Image_Recognition","319770252_TensorFlow_Large-Scale_Machine_Learning_on_Heterogeneous_Distributed_Systems","317418674_Accurate_Large_Minibatch_SGD_Training_ImageNet_in_1_Hour","316946026_Cyclical_Learning_Rates_for_Training_Neural_Networks","312607081_Building_a_large_annotated_corpus_of_english_The_penn_treebank","310824798_XGBoost_A_Scalable_Tree_Boosting_System","308152498_An_overview_of_gradient_descent_optimization_algorithms","305386496_Scikit-Iearn_Machine_learning_in_python","301844673_Distributed_Deep_Learning_Using_Synchronous_Stochastic_Gradient_Descent","307881957_LIBSVM_A_library_for_support_vector_machines","303859227_Solving_the_Quantum_Many-Body_Problem_with_Artificial_Neural_Networks","302377136_Machine_learning_phases_of_matter"]}