{"id":"332123809_Backpropagation_through_time_and_the_brain","abstract":"It has long been speculated that the backpropagation-of-error algorithm (backprop) may be a model of how the brain learns. Backpropagation-through-time (BPTT) is the canonical temporal-analogue to backprop used to assign credit in recurrent neural networks in machine learning, but there's even less conviction about whether BPTT has anything to do with the brain. Even in machine learning the use of BPTT in classic neural network architectures has proven insufficient for some challenging temporal credit assignment (TCA) problems that we know the brain is capable of solving. Nonetheless, recent work in machine learning has made progress in solving difficult TCA problems by employing novel memory-based and attention-based architectures and algorithms, some of which are brain inspired. Importantly, these recent machine learning methods have been developed in the context of, and with reference to BPTT, and thus serve to strengthen BPTT's position as a useful normative guide for thinking about temporal credit assignment in artificial and biological systems alike.","authors":["Timothy P Lillicrap","Adam Santoro"],"meta":["April 2019Current Opinion in Neurobiology 55:82-89","DOI:10.1016/j.conb.2019.01.011"],"references":["321654649_Towards_deep_learning_with_segregated_dendrites","320920312_Cortical_microcircuits_as_gated-recurrent_neural_networks","309551291_Scaling_Memory-Augmented_Neural_Networks_with_Sparse_Reads_and_Writes","308026508_WaveNet_A_Generative_Model_for_Raw_Audio","350486299_RUDDER_Return_Decomposition_for_Delayed_Rewards","319770257_Exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks","317558625_Attention_Is_All_You_Need","309091100_Hybrid_computing_using_a_neural_network_with_dynamic_external_memory","307455495_Reverse_Replay_of_Hippocampal_Place_Cells_Is_Uniquely_Modulated_by_Changing_Reward","303921752_Memory-Efficient_Backpropagation_Through_Time"]}