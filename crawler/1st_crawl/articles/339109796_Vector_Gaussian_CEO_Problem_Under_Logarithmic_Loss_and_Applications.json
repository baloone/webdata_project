{"id":"339109796_Vector_Gaussian_CEO_Problem_Under_Logarithmic_Loss_and_Applications","abstract":"In this paper, we study the vector Gaussian Chief Executive Officer (CEO) problem under logarithmic loss distortion measure. Specifically, K â‰¥ 2 agents observe independently corrupted Gaussian noisy versions of a remote vector Gaussian source, and communicate independently with a decoder or CEO over rate-constrained noise-free links. The CEO also has its own Gaussian noisy observation of the source and wants to reconstruct the remote source to within some prescribed distortion level where the incurred distortion is measured under the logarithmic loss penalty criterion. We find an explicit characterization of the rate-distortion region of this model. The result can be seen as the counterpart to the vector Gaussian setting of that by Courtade-Weissman which provides the rate-distortion region of the model in the discrete memoryless setting. For the proof of this result, we obtain an outer bound by means of a technique that relies on the de Bruijn identity and the properties of Fisher information. The approach is similar to Ekrem-Ulukus outer bounding technique for the vector Gaussian CEO problem under quadratic distortion measure, for which it was there found generally non-tight; but it is shown here to yield a complete characterization of the region for the case of logarithmic loss measure. Also, we show that Gaussian test channels with time-sharing exhaust the Berger-Tung inner bound, which is optimal. Furthermore, application of our results allows us to find the complete solutions of two related problems: a quadratic vector Gaussian CEO problem with determinant constraint and the vector Gaussian distributed Information Bottleneck problem. Finally, we develop Blahut-Arimoto type algorithms that allow to compute numerically the regions provided in this paper, for both discrete and Gaussian models. With the known relevance of the logarithmic loss fidelity measure in the context of learning and prediction, the proposed algorithms may find usefulness in a variety of applications where learning is performed distributively. We illustrate the efficiency of our algorithms through some numerical examples.","authors":["Yigit Ugur","Inaki Estella Aguerri","Abdellatif Zaidi"],"meta":["February 2020IEEE Transactions on Information Theory PP(99):1-1","DOI:10.1109/TIT.2020.2972348"],"references":["334584918_Distributed_Variational_Representation_Learning","320055251_Distributed_Information_Bottleneck_Method_for_Discrete_and_Gaussian_Sources","336336399_How_Much_Does_Your_Data_Exploration_Overfit_Controlling_Bias_via_Information_Usage","330860469_On_the_Capacity_of_Cloud_Radio_Access_Networks_With_Oblivious_Relaying","330478211_Vector_Gaussian_CEO_Problem_Under_Logarithmic_Loss","327088062_Minimax_Learning_for_Remote_Prediction","327084045_On_the_Universality_of_the_Logistic_Loss_Function","323000650_A_generalization_of_blahut-arimoto_algorithm_to_compute_rate-distortion_regions_of_multiterminal_source_coding_under_logarithmic_loss","321505896_A_Strong_Entropy_Power_Inequality","319463756_Universality_of_Logarithmic_Loss_in_Lossy_Compression"]}