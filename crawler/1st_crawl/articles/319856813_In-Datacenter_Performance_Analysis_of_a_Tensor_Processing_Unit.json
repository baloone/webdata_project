{"id":"319856813_In-Datacenter_Performance_Analysis_of_a_Tensor_Processing_Unit","abstract":"Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.","authors":["Norman P. Jouppi","Al Borchers","Rick Boyle","Pierre-luc Cantin"],"meta":["June 2017ACM SIGARCH Computer Architecture News 45(2):1-12","DOI:10.1145/3140659.3080246"],"references":["319770414_Identity_Mappings_in_Deep_Residual_Networks","319770323_EIE_Efficient_Inference_Engine_on_Compressed_Deep_Neural_Network","319770183_Imagenet_classification_with_deep_convolutional_neural_networks","317631934_Eyeriss_A_Spatial_Architecture_for_Energy-Efficient_Dataflow_for_Convolutional_Neural_Networks","311753658_A_cloud-scale_acceleration_architecture","311488432_Large-Scale_Deep_Learning_For_Building_Intelligent_Computer_Systems","309595022_If_I_could_only_design_one_circuit_technical_perspective","309594118_A_reconfigurable_fabric_for_accelerating_large-scale_datacenter_services","309594021_DianNao_family_energy-efficient_hardware_accelerators_for_machine_learning","309332937_Fathom_reference_workloads_for_modern_deep_learning_methods"]}