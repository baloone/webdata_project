{"id":"254211576_High-dimensional_covariance_estimation_by_minimizing_-penalized_log-determinant_divergence","abstract":"Given i.i.d. observations of a random vector X∈ℝp, we study the problem of estimating both its covariance matrix Σ*, and its inverse covariance or concentration matrix Θ*=(Σ*)−1. When X is multivariate Gaussian, the non-zero structure of Θ* is specified by the graph of an associated Gaussian Markov random field; and a popular estimator for such sparse Θ* is the ℓ1-regularized Gaussian MLE. This estimator is sensible even for for non-Gaussian X, since it corresponds to minimizing an ℓ1-penalized log-determinant Bregman divergence. We analyze its performance under high-dimensional scaling, in which the number of nodes in the graph p, the number of edges s, and the maximum node degree d, are allowed to grow as a function of the sample size n. In addition to the parameters (p,s,d), our analysis identifies other key quantities that control rates: (a) the ℓ∞-operator norm of the true covariance matrix Σ*; and (b) the ℓ∞ operator norm of the sub-matrix Γ*SS, where S indexes the graph edges, and Γ*=(Θ*)−1⊗(Θ*)−1; and (c) a mutual incoherence or irrepresentability measure on the matrix Γ* and (d) the rate of decay 1/f(n,δ) on the probabilities {|Σ̂nij−Σ*ij|>δ}, where Σ̂n is the sample covariance based on n samples. Our first result establishes consistency of our estimate Θ̂ in the elementwise maximum-norm. This in turn allows us to derive convergence rates in Frobenius and spectral norms, with improvements upon existing results for graphs with maximum node degrees $\\ensuremath{d}=o(\\sqrt{\\ensuremath{s}})$ . In our second result, we show that with probability converging to one, the estimate Θ̂ correctly specifies the zero pattern of the concentration matrix Θ*. We illustrate our theoretical results via simulations for various graphs and problem parameters, showing good correspondences between the theoretical predictions and behavior in simulations.","authors":["Pradeep Ravikumar","Martin J. Wainwright","Garvesh Raskutti","B. Yu"],"meta":["January 2011Electronic Journal of Statistics 5(2011)","DOI:10.1214/11-EJS631"],"references":["292215307_Just_relax_Convex_programming_methods_for_identifying_sparse_signals_in_noise","281792271_Estimation_of_high-dimensional_prior_and_posterior_covariance_matrices_in_Kalman_filter_variants","273002355_Convex_Optimization","272860727_Matrix_Analysis","270254683_Limit_Theorems_of_Probability_Theory_Sequences_of_Independent_Random_Variables","266009713_Limit_Theorem_of_Probability_Theory_Sequences_of_Independent_Random_Variables","265366228_Metric_characteristics_of_random_variables_and_processes","265347833_Metric_characterization_of_random_variables_and_random_processes_Transl_from_the_Russian_by_V_Zaiats","246567598_Iterative_Solution_of_Nonlinear_Equations_in_Several_Variables","243770214_Fundamentals_of_Statistical_Exponential_Families"]}