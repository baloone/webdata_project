{"id":"315456390_On_Consistency_of_Graph-based_Semi-supervised_Learning","abstract":"Graph-based semi-supervised learning is one of the most popular methods in machine learning. Some of its theoretical properties such as bounds for the generalization error and the convergence of the graph Laplacian regularizer have been studied in computer science and statistics literatures. However, a fundamental statistical property, the consistency of the estimator from this method has not been proved. In this article, we study the consistency problem under a non-parametric framework. We prove the consistency of graph-based learning in the case that the estimated scores are enforced to be equal to the observed responses for the labeled data. The sample sizes of both labeled and unlabeled data are allowed to grow in this result. When the estimated scores are not required to be equal to the observed responses, a tuning parameter is used to balance the loss function and the graph Laplacian regularizer. We give a counterexample demonstrating that the estimator for this case can be inconsistent. The theoretical findings are supported by numerical studies.","authors":["Chengan Du","Yunpeng Zhao"],"meta":["March 2017"],"references":["222545663_Weighted_Nadaraya-Watson_regression_estimation","324392416_Handbook_of_Econometrics","307963128_On_estimating_regression","280566512_Networks_An_Introduction","260406452_Smooth_Regression_Analysis","228614786_Semi-Supervised_Learning_with_the_Graph_Laplacian_The_Limit_of_Infinite_Unlabelled_Data","227668339_The_uniform_convergence_of_the_Nadaraya-Watson_regression_function_estimate","224750875_Regularization_and_semi-supervised_learning_on_large_graphs","221497630_Uniform_Convergence_of_Adaptive_Graph-Based_Regularization","221346528_Semi-supervised_Clustering_by_Seeding"]}