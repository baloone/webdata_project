{"id":"335679040_CNN-Cert_An_Efficient_Framework_for_Certifying_Robustness_of_Convolutional_Neural_Networks","abstract":"Verifying robustness of neural network classifiers has attracted great interests and attention due to the success of deep neural networks and their unexpected vulnerability to adversarial perturbations. Although finding minimum adversarial distortion of neural networks (with ReLU activations) has been shown to be an NP-complete problem, obtaining a non-trivial lower bound of minimum distortion as a provable robustness guarantee is possible. However, most previous works only focused on simple fully-connected layers (multilayer perceptrons) and were limited to ReLU activations. This motivates us to propose a general and efficient framework, CNN-Cert, that is capable of certifying robustness on general convolutional neural networks. Our framework is general – we can handle various architectures including convolutional layers, max-pooling layers, batch normalization layer, residual blocks, as well as general activation functions; our approach is efficient – by exploiting the special structure of convolutional layers, we achieve up to 17 and 11 times of speed-up compared to the state-of-the-art certification algorithms (e.g. Fast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach while our algorithm obtains similar or even better verification bounds. In addition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and CROWN. We demonstrate by extensive experiments that our method outperforms state-of-the-art lowerbound-based certification algorithms in terms of both bound quality and speed.","authors":["Akhilan Boopathy","Tsui-Wei Weng","Pin-Yu Chen","Sijia Liu"],"meta":["July 2019Proceedings of the AAAI Conference on Artificial Intelligence 33:3240-3247","DOI:10.1609/aaai.v33i01.33013240"],"references":["313394663_Reluplex_An_Efficient_SMT_Solver_for_Verifying_Deep_Neural_Networks","269935591_Explaining_and_Harnessing_Adversarial_Examples","356147862_Efficient_Neural_Network_Robustness_Certification_with_General_Activation_Functions","335800744_AutoZOOM_Autoencoder-Based_Zeroth_Order_Optimization_Method_for_Attacking_Black-Box_Neural_Networks","328326898_Wild_Patterns_Ten_Years_After_the_Rise_of_Adversarial_Machine_Learning","326495330_Adversarial_Attacks_on_Neural_Networks_for_Graph_Data","324717278_Black-box_Adversarial_Attacks_with_Limited_Queries_and_Information","322886181_Obfuscated_Gradients_Give_a_False_Sense_of_Security_Circumventing_Defenses_to_Adversarial_Examples","322851764_Evaluating_the_Robustness_of_Neural_Networks_An_Extreme_Value_Theory_Approach","322787995_Certified_Defenses_against_Adversarial_Examples","321714838_Lower_bounds_on_the_robustness_to_adversarial_perturbations","320837033_ZOO_Zeroth_Order_Optimization_Based_Black-box_Attacks_to_Deep_Neural_Networks_without_Training_Substitute_Models","320821437_Provable_defenses_against_adversarial_examples_via_the_convex_outer_adversarial_polytope","320038448_Maximum_Resilience_of_Artificial_Neural_Networks","320037488_Formal_Verification_of_Piece-Wise_Linear_Feed-Forward_Neural_Networks","319700553_EAD_Elastic-Net_Attacks_to_Deep_Neural_Networks_via_Adversarial_Examples","317919653_Towards_Evaluating_the_Robustness_of_Neural_Networks","317673614_Towards_Deep_Learning_Models_Resistant_to_Adversarial_Attacks","317101045_Formal_Guarantees_on_the_Robustness_of_a_Classifier_against_Adversarial_Manipulation","309076254_Gradientbased_learning_applied_to_document_recognition"]}