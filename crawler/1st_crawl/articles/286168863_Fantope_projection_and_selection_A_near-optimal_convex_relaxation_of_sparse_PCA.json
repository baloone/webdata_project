{"id":"286168863_Fantope_projection_and_selection_A_near-optimal_convex_relaxation_of_sparse_PCA","abstract":"We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The convex problem can be solved efficiently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of d = 1, our result implies the near-optimality of DSPCA (d'Aspremont et al. [1]) even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices that extends the applicability and provable guarantees to a wide array of settings. We demonstrate this with an application to Kendall's tau correlation matrices and transelliptical component analysis.","authors":["V.Q. Vu","J. Cho","J. Lei","K. Rohe"],"meta":["January 2013Advances in Neural Information Processing Systems"],"references":["247048386_On_the_Sum_of_the_Largest_Eigenvalues_of_a_Symmetric_Matrix","246601489_Kendall's_Tau_for_Elliptical_Distributions","232805150_Minimax_Sparse_Principal_Subspace_Estimation_in_High_Dimensions","221668780_Minimax_bounds_for_sparse_PCA_with_noisy_high-dimensional_data","220343759_A_majorization-minimization_approach_to_the_sparse_generalized_eigenvalue_problem","51963791_Truncated_Power_Method_for_Sparse_Eigenvalue_Problems","7191088_On_a_Theorem_of_Weyl_Concerning_Eigenvalues_of_Linear_Transformations_II"]}