{"id":"338661855_Assessing_the_importance_of_magnetic_resonance_contrasts_using_collaborative_generative_adversarial_networks","abstract":"A unique advantage of magnetic resonance imaging (MRI) is its mechanism for generating various image contrasts depending on tissue-specific parameters, which provides useful clinical information. Unfortunately, a complete set of MR contrasts is often difficult to obtain in a real clinical environment. Recently, there have been claims that generative models such as generative adversarial networks (GANs) can synthesize MR contrasts that are not acquired. However, the poor scalability of existing GAN-based image synthesis poses a fundamental challenge to understanding the nature of MR contrasts: which contrasts matter, and which cannot be synthesized by generative models? Here, we show that these questions can be addressed systematically by learning the joint manifold of multiple MR contrasts using collaborative generative adversarial networks. Our experimental results show that the exogenous contrast provided by contrast agents is not replaceable, but endogenous contrasts such as T1 and T2 can be synthesized from other contrasts. These findings provide important guidance for the acquisition-protocol design of MR in clinical environments. Magnetic resonance scans use different contrast agents to generate different images, each giving specific clinical information. Lee et al. use a collaborative generative model to synthesize some magnetic resonance contrasts from others, providing guidance for how clinical imaging times can be reduced.","authors":["Dongwook Lee","Won-Jin Moon","Jong Chul Ye"],"meta":["January 2020Nature Machine Intelligence 2(1)","DOI:10.1038/s42256-019-0137-x","Project: AI and Deep Learning for NeuroImaging"],"references":["329016663_Improving_the_Quality_of_Synthetic_FLAIR_Images_with_Deep_Learning_Using_a_Conditional_Generative_Adversarial_Network_for_Pixel-by-Pixel_Image_Translation","323440662_Classifying_magnetic_resonance_image_modalities_with_convolutional_neural_networks","338507130_CollaGAN_Collaborative_GAN_for_Missing_Image_Data_Imputation","331360852_Image_Synthesis_in_Multi-Contrast_MRI_With_Conditional_Generative_Adversarial_Networks","330645404_3D_MRI_Brain_Tumor_Segmentation_Using_Autoencoder_Regularization_4th_International_Workshop_BrainLes_2018_Held_in_Conjunction_with_MICCAI_2018_Granada_Spain_September_16_2018_Revised_Selected_Papers_P","329743528_StarGAN_Unified_Generative_Adversarial_Networks_for_Multi-domain_Image-to-Image_Translation","328161324_Group_Normalization_15th_European_Conference_Munich_Germany_September_8-14_2018_Proceedings_Part_XIII","327749365_Unpaired_Brain_MR-to-CT_Synthesis_Using_a_Structure-Constrained_CycleGAN_4th_International_Workshop_DLMIA_2018_and_8th_International_Workshop_ML-CDS_2018_Held_in_Conjunction_with_MICCAI_2018_Granada_S","327585972_Cross-Modality_Image_Synthesis_from_Unpaired_Data_Using_CycleGAN_Third_International_Workshop_SASHIMI_2018_Held_in_Conjunction_with_MICCAI_2018_Granada_Spain_September_16_2018_Proceedings","323867727_Cross-modality_image_synthesis_from_unpaired_data_using_CycleGAN_Effects_of_gradient_consistency_loss_and_training_data_size","322060458_Least_Squares_Generative_Adversarial_Networks","322060135_Unpaired_Image-to-Image_Translation_Using_Cycle-Consistent_Adversarial_Networks","320966887_Image-to-Image_Translation_with_Conditional_Adversarial_Networks","320968363_Photo-Realistic_Single_Image_Super-Resolution_Using_a_Generative_Adversarial_Network","320964974_Learning_from_Simulated_and_Unsupervised_Images_through_Adversarial_Training"]}