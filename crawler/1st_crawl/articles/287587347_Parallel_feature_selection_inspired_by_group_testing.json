{"id":"287587347_Parallel_feature_selection_inspired_by_group_testing","abstract":"This paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes. Our original method is inspired by group testing theory, under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel. Each test corresponds to a subset of features, for which a scoring function may be applied to measure the relevance of the features in a classification task. We develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified. Superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions. We present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets, for which our method exhibits competitive performance in terms of running time and accuracy. Moreover, it also yields substantial speedup when used as a pre-processing step for most other existing methods.","authors":["Y. Zhou","U. Porwal","Ce Zhang","H. Ngo"],"meta":["January 2014Advances in Neural Information Processing Systems 4:3554-3562"],"references":["221305296_Conditional_Infomax_Learning_An_Integrated_Framework_for_Feature_Extraction_and_Fusion","221173759_A_stability_index_for_feature_selection","220320493_Distributional_Word_Clusters_vs_Words_for_Text_Categorization","220320487_Ranking_a_Random_Feature_For_Variable_And_Feature_Selection","2123254_ON_surrogate_loss_functions_and_f-divergences","221345490_Learning_Sparse_SVM_for_Feature_Selection_on_Very_High_Dimensional_Datasets","220995184_Efficiently_Decodable_Compressed_Sensing_by_List-Recoverable_Codes_and_Recursion"]}