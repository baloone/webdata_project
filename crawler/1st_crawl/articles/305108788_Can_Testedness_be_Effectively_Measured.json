{"id":"305108788_Can_Testedness_be_Effectively_Measured","abstract":"Among the major questions that a practicing tester faces are deciding where to focus additional testing effort, and deciding when to stop testing. Test the least-tested code, and stop when all code is well-tested, is a reasonable answer. Many measures of \"testedness\" have been proposed; unfortunately, we do not know whether these are truly effective. In this paper we propose a novel evaluation of two of the most important and widely-used measures of test suite quality. The first measure is statement coverage, the simplest and best-known code coverage measure. The second measure is mutation score, a supposedly more powerful, though expensive, measure.\nWe evaluate these measures using the actual criteria of interest: if a program element is (by these measures) well tested at a given point in time, it should require fewer future bug-fixes than a \"poorly tested\" element. If not, then it seems likely that we are not effectively measuring testedness. Using a large number of open source Java programs from Github and Apache, we show that both statement coverage and mutation score have only a weak negative correlation with bug-fixes. Despite the lack of strong correlation, there are statistically and practically significant differences between program elements for various binary criteria. Program elements (other than classes) covered by any test case see about half as many bug-fixes as those not covered, and a similar line can be drawn for mutation score thresholds. Our results have important implications for both software engineering practice and research evaluation.","authors":["Iftekhar Ahmed","Rahul Gopinath","Caius Brindescu","Alex Groce"],"meta":["November 2016","DOI:10.1145/2950290.2950324","Conference: FSE","Project: Code Coverage"],"references":["305869980_Relating_Code_Coverage_Mutation_Score_and_Test_Suite_Reducibility_to_Defect_Density","282895362_Guidelines_for_Coverage-Based_Comparisons_of_Non-Adequate_Test_Suites","281640540_Do_Automatically_Generated_Unit_Tests_Find_Real_Faults_An_Empirical_Study_of_Effectiveness_and_Challenges","281276047_An_Empirical_Study_of_Design_Degradation_How_Software_Projects_Get_Worse_Over_Time","281050342_Fine-grained_and_Accurate_Source_Code_Differencing","277664637_Are_Mutants_a_Valid_Substitute_for_Real_Faults_in_Software_Testing","277017031_Coverage_and_Its_Discontents","266656271_Code_coverage_for_suite_evaluation_by_developers","262222409_Comparing_non-adequate_test_suites_using_coverage_criteria","254040874_Identifying_Linux_bug_fixing_patches","242460646_On_the_use_of_software_artifacts_to_evaluate_the_effectiveness_of_mutation_analysis_for_detecting_errors_in_production_software","224468520_An_Experimental_Comparison_of_Four_Unit_Test_Criteria_Mutation_Edge-Pair_All-Uses_and_Prime_Path_Coverage","221560720_Fair_and_Balanced_Bias_in_Bug-Fix_Datasets","221554380_Is_Mutation_an_Appropriate_Tool_for_Testing_Experiments","221553870_Experiments_of_the_Effectiveness_of_Dataflow-_and_Controlflow-Based_Test_Adequacy_Criteria","221507075_The_effect_of_code_coverage_on_fault_detection_under_different_testing_profiles","221051161_Is_Branch_Coverage_a_Good_Measure_of_Testing_Effectiveness","220854552_The_influence_of_size_and_coverage_on_test_suite_effectiveness","51969319_Scikit-learn_Machine_Learning_in_Python","4200521_Is_mutation_an_appropriate_tool_for_testing_experiments","3407068_A_Critique_of_Cyclomatic_Complexity_as_a_Software_Metric","3189678_Using_Mutation_Analysis_for_Assessing_and_Comparing_Testing_Coverage_Criteria","301392321_Balancing_trade-offs_in_test-suite_reduction","270906892_Selecting_a_software_engineering_tool_Lessons_learnt_from_mutation_analysis","268237854_Software_error_analysis","266656203_Coverage_is_not_strongly_correlated_with_test_suite_effectiveness","266500619_Predicting_Test_Suite_Effectiveness_for_Java_Programs","265668053_The_Art_of_Software_Testing","265542098_An_Analysis_of_Parameters_Influencing_Test_Suite_Effectiveness","259543154_A_Hitchhiker's_guide_to_statistical_tests_for_assessing_randomized_algorithms_in_software_engineering","234805233_Mutation_Analysis_of_Program_Test_Data","221494980_Test_coverage_and_post-verification_defects_A_multiple_case_study","220854753_Software_Error_Analysis_A_Real_Case_Study_Involving_Real_Faults_and_Mutations","220854405_The_use_of_mutation_in_testing_experiments_and_its_sensitivity_to_external_threats","220719862_Semantic_Mutation_Testing","220643561_An_approach_for_experimentally_evaluating_effectiveness_and_efficiency_of_coverage_criteria_for_software_testing","220070444_A_Complexity_Measure","3189025_A_Complexity_Measure","3187586_Experimental_comparison_of_the_effectiveness_of_branch_testing_and_data_flow_testing","2762262_An_Empirical_Comparison_of_Mutation_and_Data_Flow_Based_Test_Adequacy_Criteria","2410166_Further_Empirical_Studies_of_Test_Effectiveness","2394289_All-Uses_versus_Mutation_Testing_An_Experimental_Comparison_of_Effectiveness","2318501_Subsumption_of_Condition_Coverage_Techniques_by_Mutation_Testing"]}