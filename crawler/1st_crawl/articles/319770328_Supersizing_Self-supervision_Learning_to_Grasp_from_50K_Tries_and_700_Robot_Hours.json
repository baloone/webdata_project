{"id":"319770328_Supersizing_Self-supervision_Learning_to_Grasp_from_50K_Tries_and_700_Robot_Hours","abstract":"Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.","authors":["Lerrel Pinto","Abhinav Gupta"],"meta":["September 2016","DOI:10.1109/ICRA.2016.7487517","Conference: ICRA"],"references":["319770233_Deep_Learning_for_Detecting_Robotic_Grasps","282856821_Leveraging_Big_Data_for_Grasp_Planning","264979485_Caffe_Convolutional_Architecture_for_Fast_Feature_Embedding","258140407_Planning_Collision-_Free_Motions_for_Pick-and-Place_Operations","257909554_VisGraB_A_Benchmark_for_Vision-Based_Grasping","256487932_Data-Driven_Grasp_Synthesis_-_A_Survey","254041261_Using_depth_and_appearance_features_for_informed_robot_grasping_of_highly_wrinkled_clothes","239763436_Pose_error_robust_grasping_from_contact_wrench_space_metrics","221072191_The_Columbia_grasp_database","221072021_Efficient_grasping_from_RGBD_images_Learning_using_a_new_rectangle_representation","221070088_Automatic_grasp_planning_using_shape_primitives","220065711_Using_Experience_for_Assessing_Grasp_Reliability","216053684_Learning_object-specific_grasp_affordance_densities","319770820_Histograms_of_Oriented_Gradients_for_Human_Detection","319770430_Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation","314578524_Perceiving_Learning_and_Exploiting_Object_Affordances_for_Autonomous_Pile_Manipulation","308312219_Deep_Learning_for_Detecting_Robotic_Grasps","281327886_Histograms_of_Oriented_Gradients_for_Human_Detection","274572264_End-to-End_Training_of_Deep_Visuomotor_Policies","272837232_Human-level_control_through_deep_reinforcement_learning","271218264_Learning_Contact-Rich_Manipulation_Skills_with_Guided_Policy_Search","269417378_Real-Time_Grasp_Detection_Using_Convolutional_Neural_Networks","267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks","265084003_A_Data-Driven_Statistical_Framework_for_Post-Grasp_Manipulation","258374356_Rich_Feature_Hierarchies_for_Accurate_Object_Detection_and_Semantic_Segmentation","258140856_Robot_Grasp_Synthesis_Algorithms_A_Survey","220142531_Active_learning_of_visual_descriptors_for_grasping_using_non-parametric_smoothed_beta_distributions","220122236_Constructing_Force-_Closure_Grasps","220121825_Robotic_Grasping_of_Novel_Objects_using_Vision","47646552_A_Reduction_of_Imitation_Learning_and_Structured_Prediction_to_No-RegretOnline_Learning","13803865_Sparse_coding_with_an_over-complete_basis_set_A_strategy_employed_by_V1_Vision_Research","3344710_GraspIt_A_versatile_simulator_for_robotic_grasping","2953822_Task-level_planning_of_pick-and-place_robot_motions","2855409_Robotic_Grasping_and_Contact_A_Review"]}