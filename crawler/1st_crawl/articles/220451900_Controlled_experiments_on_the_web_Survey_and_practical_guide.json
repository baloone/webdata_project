{"id":"220451900_Controlled_experiments_on_the_web_Survey_and_practical_guide","abstract":"The web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments, also called randomized experiments, A/B tests (and their generalizations), split tests, Control/Treatment tests, MultiVariable Tests (MVT) and parallel flights. Controlled experiments embody the best scientific design for establishing a causal relationship between changes and their influence on user-observable behavior. We provide a practical guide to conducting online experiments, where end-users can help guide the development of features. Our experience indicates that significant learning and return-on-investment (ROI) are seen when development teams listen to their customers, not to the Highest Paid Personâ€™s Opinion (HiPPO). We provide several examples of controlled experiments with surprising results. We review the important ingredients of running controlled experiments, and discuss their limitations (both technical and organizational). We focus on several areas that are critical to experimentation, including statistical power, sample size, and techniques for variance reduction. We describe common architectures for experimentation systems and analyze their advantages and disadvantages. We evaluate randomization and hashing techniques, which we show are not as simple in practice as is often assumed. Controlled experiments typically generate large amounts of data, which can be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest, leading to new hypotheses and creating a virtuous cycle of improvements. Organizations that embrace controlled experiments with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses. Based on our extensive practical experience with multiple systems and organizations, we share key lessons that will help practitioners in running trustworthy controlled experiments.","authors":["Ron Kohavi","Roger Longbotham","Dan Sommerfield","Randal M. Henne"],"meta":["February 2009Data Mining and Knowledge Discovery 18(1):140-181","DOI:10.1007/s10618-008-0114-1","SourceDBLP"],"references":["305238137_Evaluation_a_systematic_approach_Peter_Rossi_Evaluation_A_systematic_approach_et_al_Sage_470_35_0761908943_0761908943","302998576_The_Design_of_Optimum_Multifactorial_Experiments","280965348_Statistics_for_Experimenters_Design_Innovation_and_Discovery","280113401_Make_Data_Useful","275689172_Statistical_Design_and_Analysis_of_Experiments_With_Applications_to_Engineering_and_Science_Second_Edition","270391492_The_Progress_of_Experiment_Science_and_Therapeutic_Reform_in_the_United_States_1900-1990","266559023_experimentation_matters_unlocking_the_potential_of_new_technologies_for_innovation","265615747_Statistical_Rules_of_Thumb","265465293_The_Statistical_Analysis_of_Cost-Effectiveness_Data","265198828_Scientific_Advertising_Scientific_Advertising"]}