{"id":"308278474_ell_0_l_0_-Sparse_Subspace_Clustering","abstract":"Subspace clustering methods with sparsity prior, such as Sparse Subspace Clustering (SSC) [1], are effective in partitioning the data that lie in a union of subspaces. Most of those methods require certain assumptions, e.g. independence or disjointness, on the subspaces. These assumptions are not guaranteed to hold in practice and they limit the application of existing sparse subspace clustering methods. In this paper, we propose \\(\\ell ^{0}\\)-induced sparse subspace clustering (\\(\\ell ^{0}\\)-SSC). In contrast to the required assumptions, such as independence or disjointness, on subspaces for most existing sparse subspace clustering methods, we prove that subspace-sparse representation, a key element in subspace clustering, can be obtained by \\(\\ell ^{0}\\)-SSC for arbitrary distinct underlying subspaces almost surely under the mild i.i.d. assumption on the data generation. We also present the “no free lunch” theorem that obtaining the subspace representation under our general assumptions can not be much computationally cheaper than solving the corresponding \\(\\ell ^{0}\\) problem of \\(\\ell ^{0}\\)-SSC. We develop a novel approximate algorithm named Approximate \\(\\ell ^{0}\\)-SSC (\\(\\hbox {A}\\ell ^{0}\\)-SSC) that employs proximal gradient descent to obtain a sub-optimal solution to the optimization problem of \\(\\ell ^{0}\\)-SSC with theoretical guarantee, and the sub-optimal solution is used to build a sparse similarity matrix for clustering. Extensive experimental results on various data sets demonstrate the superiority of \\(\\hbox {A}\\ell ^{0}\\)-SSC compared to other competing clustering methods.","authors":["Yingzhen Yang","Jiashi Feng","Nebojsa Jojic","Jianchao Yang"],"meta":["October 2016","DOI:10.1007/978-3-319-46475-6_45","Conference: European Conference on Computer Vision"],"references":["267695689_Greedy_Subspace_Clustering","262009610_Low-Rank_Sparse_Coding_for_Image_Classification","244989628_Greedy_Feature_Selection_for_Subspace_Clustering","224092448_Learning_With_-Graph_for_Image_Analysis","224057740_L0-Norm-Based_Sparse_Representation_Through_Alternate_Projections","221573871_Locality_preserving_clustering_for_image_database","221364080_Linear_spatial_pyramid_matching_using_sparse_coding_for_image_classification","221344931_Proximal_Methods_for_Sparse_Hierarchical_Dictionary_Learning","51933012_A_General_Theory_of_Concave_Regularization_for_High_Dimensional_SparseEstimation_Problems","45865402_Online_Learning_for_Matrix_Factorization_and_Sparse_Coding","1775282_Supervised_Dictionary_Learning","313423378_Sparse_manifold_clustering_and_embedding","306173097_Noisy_sparse_subspace_clustering","301701662_Robust_subspace_clustering_via_thresholding_ridge_regression","289541573_Provable_subspace_clustering_When_LRR_meets_SSC","289541483_Noisy_sparse_subspace_clustering","273458540_Data_Clustering_by_Laplacian_Regularized_l1-Graph","273458457_Regularized_l1-Graph_for_Data_Clustering","256993892_Sparse_representation_and_learning_in_visual_recognition_Theory_and_applications","236061588_Sparse_Subspace_Clustering_Algorithm_Theory_and_Applications","234113701_Robust_Subspace_Clustering","233786463_Sparse_Manifold_Clustering_and_Embedding","224461335_An_approximate_L0_norm_minimization_algorithm_for_compressed_sensing","224219309_Subspace_Clustering","223969598_Robust_Recovery_of_Subspace_Structures_by_Low-Rank_Representation","221996566_On_Spectral_Clustering_Analysis_and_an_Algorithm","221344982_Robust_Subspace_Segmentation_by_Low-Rank_Representation","220907222_Semi-supervised_Learning_by_Sparse_Representation","200524187_The_restricted_isometry_property_and_its_implications_for_compressed_sensing","200033852_UCI_Machine_Learning_Repository_Irvine","51966188_A_geometric_analysis_of_subspace_clustering_with_outliers","3085640_Decoding_by_Linear_Programming","3085174_Greed_is_Good_Algorithmic_Results_for_Sparse_Approximation"]}