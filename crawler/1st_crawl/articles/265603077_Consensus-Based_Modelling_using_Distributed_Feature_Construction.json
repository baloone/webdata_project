{"id":"265603077_Consensus-Based_Modelling_using_Distributed_Feature_Construction","abstract":"A particularly successful role for Inductive Logic Programming (ILP) is as a\ntool for discovering useful relational features for subsequent use in a\npredictive model. Conceptually, the case for using ILP to construct relational\nfeatures rests on treating these features as functions, the automated discovery\nof which necessarily requires some form of first-order learning. Practically,\nthere are now several reports in the literature that suggest that augmenting\nany existing features with ILP-discovered relational features can substantially\nimprove the predictive power of a model. While the approach is straightforward\nenough, much still needs to be done to scale it up to explore more fully the\nspace of possible features that can be constructed by an ILP system. This is in\nprinciple, infinite and in practice, extremely large. Applications have been\nconfined to heuristic or random selections from this space. In this paper, we\naddress this computational difficulty by allowing features to be constructed in\na distributed manner. That is, there is a network of computational units, each\nof which employs an ILP engine to construct some small number of features and\nthen builds a (local) model. We then employ a consensus-based algorithm, in\nwhich neighboring nodes share information to update local models. For a\ncategory of models (those with convex loss functions), it can be shown that the\nalgorithm will result in all nodes converging to a consensus model. In\npractice, it may be slow to achieve this convergence. Nevertheless, our results\non synthetic and real datasets that suggests that in relatively short time the\n\"best\" node in the network reaches a model whose predictive accuracy is\ncomparable to that obtained using more computational effort in a\nnon-distributed setting (the best node is identified as the one whose weights\nconverge first).","authors":["Haimonti Dutta","Ashwin Srinivasan"],"meta":["May 2018Machine Learning 107(10)","DOI:10.1007/s10994-017-5672-2","SourcearXiv"],"references":["311360322_An_empirical_study_of_on-line_models_for_relational_data_streams","339503044_Gossip_Algorithms","313665793_Inverse_entailment_and_Progol","312973075_The_tradeoffs_of_large_scale_learning","292449684_Automatic_Methods_of_Inductive_Inference","287587347_Parallel_feature_selection_inspired_by_group_testing","285329047_Parallel_Feature_Selection_Based_on_MapReduce","284688444_Mining_sequential_patterns_by_pattern-growth_The_prefixspan_approach","268486013_Geographic_gossip","266310934_Motoda_H_Feature_Selection_for_Knowledge_Discovery_and_Data_Mining_Kluwer_Academic_USA"]}