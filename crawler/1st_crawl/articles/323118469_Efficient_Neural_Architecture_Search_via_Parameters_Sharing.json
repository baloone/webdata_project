{"id":"323118469_Efficient_Neural_Architecture_Search_via_Parameters_Sharing","abstract":"We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.","authors":["Hieu Pham","Melody Y. Guan","Barret Zoph","Quoc V. Le"],"meta":["February 2018"],"references":["317300069_Learning_Time-Efficient_Deep_Architectures_with_Budgeted_Super_Networks","316598820_DeepArchitect_Automatically_Designing_and_Training_Deep_Architectures","311222630_Capacity_and_Trainability_in_Recurrent_Neural_Networks","309738510_Designing_Neural_Network_Architectures_using_Reinforcement_Learning","306885833_Densely_Connected_Convolutional_Networks","306187421_SGDR_Stochastic_Gradient_Descent_with_Warm_Restarts","305229133_Recurrent_Highway_Networks","261100864_CNN_Features_Off-the-Shelf_An_Astounding_Baseline_for_Recognition","221489926_Recurrent_neural_network_based_language_model","13853244_Long_Short-term_Memory","321745057_Peephole_Predicting_Network_Performance_Before_Training","321025432_Breaking_the_Softmax_Bottleneck_A_High-Rank_RNN_Language_Model","320968382_Xception_Deep_Learning_with_Depthwise_Separable_Convolutions","320798539_Hierarchical_Representations_for_Efficient_Architecture_Search","319977201_Dynamic_Evaluation_of_Neural_Sequence_Models","319977197_Neural_Optimizer_Search_with_Reinforcement_Learning","319770272_Delving_Deep_into_Rectifiers_Surpassing_Human-Level_Performance_on_ImageNet_Classification","319769994_A_Theoretically_Grounded_Application_of_Dropout_in_Recurrent_Neural_Networks","319186950_Practical_Network_Blocks_Design_with_Q-Learning","319164270_SMASH_One-Shot_Model_Architecture_Search_through_HyperNetworks","319135616_Improved_Regularization_of_Convolutional_Neural_Networks_with_Cutout","318981690_Regularizing_and_Optimizing_LSTM_Language_Models","318671172_Learning_Transferable_Architectures_for_Scalable_Image_Recognition","318527982_On_the_State_of_the_Art_of_Evaluation_in_Neural_Language_Models","311990793_Transfer_Learning_for_Low-Resource_Neural_Machine_Translation","311648230_Improving_Neural_Language_Models_with_a_Continuous_Cache","309738632_Neural_Architecture_Search_with_Reinforcement_Learning","309730114_Tying_Word_Vectors_and_Word_Classifiers_A_Loss_Framework_for_Language_Modeling","308277201_Identity_Mappings_in_Deep_Residual_Networks","303520977_FractalNet_Ultra-Deep_Neural_Networks_without_Residuals","286512696_Deep_Residual_Learning_for_Image_Recognition","284218905_Multi-task_Sequence_to_Sequence_Learning","278733843_Gradient_Estimation_Using_Stochastic_Computation_Graphs","272194743_Batch_Normalization_Accelerating_Deep_Network_Training_by_Reducing_Internal_Covariate_Shift","269935079_Adam_A_Method_for_Stochastic_Optimization","265748773_Learning_Multiple_Layers_of_Features_from_Tiny_Images","265469170_Recurrent_Neural_Network_Regularization","257291640_A_method_of_solving_a_convex_programming_problem_with_convergence_rate_O1k2","2426555_Simple_Statistical_Gradient-Following_Algorithms_for_Connectionist_Reinforcement_Learning"]}