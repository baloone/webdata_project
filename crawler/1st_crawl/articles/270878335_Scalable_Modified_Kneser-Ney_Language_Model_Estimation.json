{"id":"270878335_Scalable_Modified_Kneser-Ney_Language_Model_Estimation","abstract":"We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM.","authors":["Kenneth Heafield","Ivan Pouzyrevsky","Jonathan H Clark","Philipp Koehn"],"meta":["August 2013","Conference: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)"],"references":["221486318_IRSTLM_An_open_source_toolkit_for_handling_large_scale_language_models","266538084_Duplicate_record_elimination_in_large_data_files","227618979_STXXL_standard_template_library_for_XXL_data_sets"]}