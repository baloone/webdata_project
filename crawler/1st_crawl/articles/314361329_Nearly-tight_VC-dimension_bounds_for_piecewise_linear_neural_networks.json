{"id":"314361329_Nearly-tight_VC-dimension_bounds_for_piecewise_linear_neural_networks","abstract":"We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting $W$ be the number of weights and $L$ be the number of layers, we prove that the VC-dimension is $O(W L \\log(W))$ and $\\Omega( W L \\log(W/L) )$. This improves both the previously known upper bounds and lower bounds. In terms of the number $U$ of non-linear units, we prove a tight bound $\\Theta(W U)$ on the VC-dimension. All of these results generalize to arbitrary piecewise linear activation functions.","authors":["Nick Harvey","Chris Liaw","Abbas Mehrabian"],"meta":["March 2017"],"references":["309131837_Why_Deep_Neural_Networks","308896188_Error_bounds_for_approximations_with_deep_ReLU_networks","281895600_On_the_Expressive_Power_of_Deep_Learning_A_Tensor_Analysis","277411157_Deep_Learning","2809085_Almost_Linear_VC_Dimension_Bounds_for_Piecewise_Polynomial_Networks","319770106_On_the_Expressive_Power_of_Deep_Learning_A_Tensor_Analysis","309572399_Depth_Separation_in_ReLU_Networks_for_Approximating_Smooth_Non-Linear_Functions","301857086_Benefits_of_depth_in_neural_networks","287250773_The_Power_of_Depth_for_Feedforward_Neural_Networks","246732421_Lower_Bounds_for_Approximation_by_Nonlinear_Manifolds","226821868_Bounding_the_Vapnik-Chervonenkis_dimension_of_concept_classes_parameterized_by_real_numbers","221497807_Bounding_the_Vapnik-Chervonenkis_Dimension_of_Concept_Classes_Parameterized_by_Real_Numbers","220695511_Neural_Network_Learning_Theoretical_Foundations","220430853_Learnability_and_the_Vapnik-Chervonenkis_Dimension","2764834_Neural_Nets_with_Superlinear_VC-Dimension"]}