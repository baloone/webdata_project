{"id":"328033163_Entropy_Power_Autoregressive_Models_and_Mutual_Information","abstract":"Autoregressive processes play a major role in speech processing (linear prediction), seismic signal processing, biological signal processing, and many other applications. We consider the quantity defined by Shannon in 1948, the entropy rate power, and show that the log ratio of entropy powers equals the difference in the differential entropy of the two processes. Furthermore, we use the log ratio of entropy powers to analyze the change in mutual information as the model order is increased for autoregressive processes. We examine when we can substitute the minimum mean squared prediction error for the entropy power in the log ratio of entropy powers, thus greatly simplifying the calculations to obtain the differential entropy and the change in mutual information and therefore increasing the utility of the approach. Applications to speech processing and coding are given and potential applications to seismic signal processing, EEG classification, and ECG classification are described.","authors":["Jerry Gibson"],"meta":["September 2018Entropy 20(10):750","DOI:10.3390/e20100750"],"references":["320553787_Log_Likelihood_Spectral_Distance_Entropy_Rate_Power_and_Mutual_Information_with_Applications_to_Speech_Coding","316600454_Introduction_to_Data_Compression","306157547_Time_Series_Analysis_Forecasting_and_Control","303801733_Speech_Compression","286258004_Speech_Coding_for_Wireless_Communications","281741468_Overview_of_the_EVS_codec_architecture","274189441_Density_Estimation_for_Statistics_and_Data_Analysis","269501286_Information_and_Information_Stability_of_Random_Variables_and_Processes","266908024_Network_Information_Theory","265428522_Toeplitz_forms_and_their_applications_2nd_ed"]}