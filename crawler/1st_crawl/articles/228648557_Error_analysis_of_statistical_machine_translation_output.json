{"id":"228648557_Error_analysis_of_statistical_machine_translation_output","abstract":"Evaluation of automatic translation output is a difficult task. Several performance measures like Word Error Rate, Position Independent Word Error Rate and the BLEU and NIST scores are widely use and provide a useful tool for comparing different systems and to evaluate improvements within a system. However the interpretation of all of these measures is not at all clear, and the identification of the most prominent source of errors in a given system using these measures alone is not possible. Therefore some analysis of the generated translations is needed in order to identify the main problems and to focus the research efforts. This area is however mostly unexplored and few works have dealt with it until now. In this paper we will present a framework for classification of the errors of a machine translation system and we will carry out an error analysis of the system used by the RWTH in the first TC-STAR evaluation.","authors":["David Vilar","Jia Xu","Luis Fernando D'Haro","D ' Haro"],"meta":["May 2006"],"references":["250299524_A_Framework_for_Interactive_and_Automatic_Refinement_of_Transfer-Based_Machine_Translation","228790859_Statistical_machine_translation_of_european_parliamentary_speeches","2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation","234812513_Automatic_evaluation_of_machine_translation_quality_using_n-gram_co-occurrence_statistics"]}