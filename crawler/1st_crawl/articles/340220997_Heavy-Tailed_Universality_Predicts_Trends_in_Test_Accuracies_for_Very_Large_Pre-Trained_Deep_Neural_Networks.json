{"id":"340220997_Heavy-Tailed_Universality_Predicts_Trends_in_Test_Accuracies_for_Very_Large_Pre-Trained_Deep_Neural_Networks","abstract":"Given two or more Deep Neural Networks (DNNs)\nwith the same or similar architectures, and trained on\nthe same dataset, but trained with different solvers,\nparameters, hyper-parameters, regularization, etc., can\nwe predict which DNN will have the best test accuracy,\nand can we do so without peeking at the test data?\nIn this paper, we show how to use a new Theory of\nHeavy-Tailed Self-Regularization (HT-SR) to answer\nthis. HT-SR suggests, among other things, that modern\nDNNs exhibit what we call Heavy-Tailed Mechanistic\nUniversality (HT-MU), meaning that the correlations in\nthe layer weight matrices can be fit to a power law (PL)\nwith exponents that lie in common Universality classes\nfrom Heavy-Tailed Random Matrix Theory (HT-RMT).\nFrom this, we develop a Universal capacity control metric\nthat is a weighted average of PL exponents. Rather\nthan considering small toy NNs, we examine over 50\ndifferent, large-scale pre-trained DNNs, ranging over 15\ndifferent architectures, trained on ImagetNet, each of\nwhich has been reported to have different test accuracies.\nWe show that this new capacity metric correlates very\nwell with the reported test accuracies of these DNNs,\nlooking across each architecture (VGG16/.../VGG19,\nResNet10/.../ResNet152, etc.). We also show how to\napproximate the metric by the more familiar Product\nNorm capacity measure, as the average of the log\nFrobenius norm of the layer weight matrices. Our\napproach requires no changes to the underlying DNN or\nits loss function, it does not require us to train a model\n(although it could be used to monitor training), and it\ndoes not even require access to the ImageNet data.","authors":["Charles Martin","Michael W. Mahoney"],"meta":["January 2020","DOI:10.1137/1.9781611976236.57","In book: Proceedings of the 2020 SIAM International Conference on Data Mining (pp.505-513)"],"references":["330638462_Heavy-Tailed_Universality_Predicts_Trends_in_Test_Accuracies_for_Very_Large_Pre-Trained_Deep_Neural_Networks","328107188_Implicit_Self-Regularization_in_Deep_Neural_Networks_Evidence_from_Random_Matrix_Theory_and_Implications_for_Learning","325396687_Understanding_Generalization_and_Optimization_Performance_of_Deep_CNNs","311762129_Powerlaw_A_python_package_for_analysis_of_heavy-tailed_distributions","272195366_Counterfactual_Risk_Minimization_Learning_from_Logged_Bandit_Feedback","344472691_Statistical_Mechanics_of_Learning","321620378_Critical_Phenomena_in_Natural_Sciences_Chaos_Fractals_Selforganization_and_Disorder_Concepts_and_Tools","287899158_Random_Matrix_Theory_and_Its_Innovative_Applications","279068893_Extreme_eigenvalues_of_sparse_heavy_tailed_random_matrices","266548609_Heavy-Tail_Phenomena_Probabilistic_and_Statistical_Modeling"]}