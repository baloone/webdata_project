{"id":"312532410_Incremental_and_decremental_support_vector_machine_learning","abstract":"An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decrementai \"unlearning\" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decrementai unlearning in feature space sheds light on the relationship between generalization and geometry of the data.","authors":["Tomaso Poggio","Gert Cauwenberghs"],"meta":["January 2001Advances in Neural Information Processing Systems 13(5):409-412"],"references":["40498219_Sparse_representation_for_Gaussian_process_models","51345355_Properties_of_Support_Vector_Machines"]}