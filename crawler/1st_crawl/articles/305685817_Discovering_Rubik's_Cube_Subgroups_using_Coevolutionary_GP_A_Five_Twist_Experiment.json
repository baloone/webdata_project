{"id":"305685817_Discovering_Rubik's_Cube_Subgroups_using_Coevolutionary_GP_A_Five_Twist_Experiment","abstract":"This work reports on an approach to direct policy discovery (a form of reinforcement learning) using genetic programming (GP) for the 3 by 3 by 3 Rubik's Cube. Specifically, a synthesis of two approaches is proposed: 1) a previous group theoretic formulation is used to suggest a sequence of objectives for developing solutions to different stages of the overall task; and 2) a hierarchical formulation of GP policy search is utilized in which policies adapted for an earlier objective are explicitly transferred to aid the construction of policies for the next objective. The resulting hierarchical organization of policies explicitly demonstrates task decomposition and policy reuse. Algorithmically, the process makes use of a recursive call to a common approach for maintaining a diverse population of GP individuals and then learns how to reuse subsets of programs (policies) developed against the earlier objective. Other than the two objectives, we do not explicitly identify how to decompose the task or mark specific policies for reuse. Moreover, at the end of evolution we return a population solving 100% of 17,675,698 different initial Cubes for the two objectives currently in use.","authors":["Robert J. Smith","Stephen Kelly","Malcolm I. Heywood"],"meta":["July 2016","DOI:10.1145/2908812.2908887","Conference: the 2016"],"references":["280043948_High-Dimensional_Function_Approximation_for_Knowledge-Free_Reinforcement_Learning_a_Case_Study_in_SZ-Tetris","220867821_An_Evolutionary_Approach_for_Solving_the_Rubik's_Cube_Incorporating_Exact_Methods","220660780_Critical_factors_in_the_empirical_performance_of_temporal_difference_and_evolutionary_methods_for_reinforcement_learning","220604900_The_Special_Issue_of_AI_Magazine_on_Structured_Knowledge_Transfer","220421119_Solving_Rubik's_Cube_disk_is_the_new_RAM","220321070_Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses","220320794_Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning","51913939_Algorithms_for_Solving_Rubik's_Cubes","51909347_Evolutionary_Algorithms_for_Reinforcement_Learning","2816904_Incremental_Evolution_of_Complex_General_Behavior","329928244_Layered_Learning_in_Multiagent_Systems_A_Winning_Approach_to_Robotic_Soccer","300334238_On_Diversity_Teaming_and_Hierarchical_Policies_Observations_from_the_Keepaway_Soccer_Task","300228068_Knowledge_Transfer_from_Keepaway_Soccer_to_Half-field_Offense_through_Program_Symbiosis","296607220_Special_Issue_on_Structured_Knowledge_Transfer_Introduction","272161307_Reinforcement_Learning_An_Introduction","261343079_On_run_time_libraries_and_hierarchical_symbiosis","254462203_Hierarchical_Task_Decomposition_through_Symbiosis_in_Reinforcement_Learning","235709806_Reinforcement_Learning_A_Survey","227199035_The_Rubik_Cube_and_GP_Temporal_Sequence_Learning_An_Initial_Study","221604616_Layered_Learning_in_Multiagent_Systems","220739633_Managing_team-based_problem_solving_with_symbiotic_bid-based_genetic_programming","220285892_Symbiotic_coevolutionary_genetic_programming_A_benchmarking_study_under_large_attribute_spaces","201841077_Behavior_chaining_Incremental_behavior_integration_for_evolutionary_robotics","31692666_Layered_Learning_in_Multiagent_Systems_A_Winning_Approach_to_Robotic_Soccer_P_Stone","13267252_New_Methods_for_Competitive_Coevolution","11207536_Evolving_Neural_Networks_through_Augmenting_Topologies","6418297_A_Monotonic_Archive_for_Pareto-Coevolution","5596000_Reinforcement_Learning_An_Introduction","2633086_Evolution_of_Cooperative_Problem-Solving_in_an_Artificial_Economy","2382898_Finding_Optimal_Solutions_to_Rubik's_Cube_Using_Pattern_Databases"]}