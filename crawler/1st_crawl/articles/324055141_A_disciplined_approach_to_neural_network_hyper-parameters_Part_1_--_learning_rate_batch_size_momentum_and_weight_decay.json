{"id":"324055141_A_disciplined_approach_to_neural_network_hyper-parameters_Part_1_--_learning_rate_batch_size_momentum_and_weight_decay","abstract":"Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums.","authors":["Leslie Smith"],"meta":["March 2018"],"references":["221344862_Curriculum_learning","323184393_Toward_Deeper_Understanding_of_Nonconvex_Stochastic_Optimization_with_Momentum_using_Diffusion_Approximations","321620746_Neural_Networks_Tricks_of_the_Trade","320821415_Don't_Decay_the_Learning_Rate_Increase_the_Batch_Size","320726611_Regularization_for_Deep_Learning_A_Taxonomy","320486652_Understanding_Generalization_and_Stochastic_Gradient_Descent","319770174_Practical_recommendations_for_gradient-based_training_of_deep_architectures","319256075_Super-Convergence_Very_Fast_Training_of_Residual_Networks_Using_Large_Learning_Rates","316946026_Cyclical_Learning_Rates_for_Training_Neural_Networks","311609041_Deep_Residual_Learning_for_Image_Recognition","301874967_Inception-v4_Inception-ResNet_and_the_Impact_of_Residual_Connections_on_Learning","286794765_Dropout_A_Simple_Way_to_Prevent_Neural_Networks_from_Overfitting","277722804_No_More_Pesky_Learning_Rate_Guessing_Games","269935079_Adam_A_Method_for_Stochastic_Optimization","262395872_Random_Search_for_Hyper-Parameter_Optimization","221669778_Simulated_Annealing_and_Boltzman_Machine","9004053_The_general_inefficiency_of_batch_training_for_gradient_descent_learning"]}