{"id":"322675073_Mitigating_Unwanted_Biases_with_Adversarial_Learning","abstract":"Machine learning is a tool for building models that accurately represent input training data. When undesired biases concerning demographic groups are in the training data, well-trained models will reflect those biases. We present a framework for mitigating such biases by including a variable for the group of interest and simultaneously learning a predictor and an adversary. The input to the network X, here text or census data, produces a prediction Y, such as an analogy completion or income bracket, while the adversary tries to model a protected variable Z, here gender or zip code. The objective is to maximize the predictor's ability to predict Y while minimizing the adversary's ability to predict Z. Applied to analogy completion, this method results in accurate predictions that exhibit less evidence of stereotyping Z. When applied to a classification task using the UCI Adult (Census) Dataset, it results in a predictive model that does not lose much accuracy while achieving very close to equality of odds (Hardt, et al., 2016). The method is flexible and applicable to multiple definitions of fairness as well as a wide range of gradient-based learning models, including both regression and classification tasks.","authors":["Brian Hu Zhang","Blake Lemoine","Margaret Mitchell"],"meta":["January 2018"],"references":["308327297_Inherent_Trade-Offs_in_the_Fair_Determination_of_Risk_Scores","305615978_Man_is_to_Computer_Programmer_as_Woman_is_to_Homemaker_Debiasing_Word_Embeddings","257882504_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality","319770355_Generative_Adversarial_Nets","318128810_Data_Decisions_and_Theoretical_Implications_when_Adversarially_Learning_Fair_Representations","309460611_A_statistical_framework_for_fair_predictive_algorithms","308980568_Equality_of_Opportunity_in_Supervised_Learning","269935079_Adam_A_Method_for_Stochastic_Optimization","200033852_UCI_Machine_Learning_Repository_Irvine"]}