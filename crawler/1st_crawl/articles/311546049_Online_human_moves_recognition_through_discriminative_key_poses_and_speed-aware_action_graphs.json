{"id":"311546049_Online_human_moves_recognition_through_discriminative_key_poses_and_speed-aware_action_graphs","abstract":"Recognizing user-defined moves serves a large number of applications including sport monitoring, virtual reality or natural user interfaces (NUI). However, many of the efficient human move recognition methods are still limited to specific situations, such as straightforward NUI gestures or everyday human actions. In particular, most methods depend on a prior segmentation of recordings to both train and recognize moves. This segmentation step is generally performed manually or based on heuristics such as neutral poses or short pauses, limiting the range of applications. Besides, speed is generally not considered as a criterion to distinguish moves. We present an approach composed of a simplified move training phase that requires minimal user intervention, together with a novel online method to robustly recognize moves online from unsegmented data without requiring any transitional pauses or neutral poses, and additionally considering human move speed. Trained gestures are automatically segmented in real time by a curvature-based method that detects small pauses during a training session. A set of most discriminant key poses between different moves is also extracted in real time, optimizing the number of key poses. All together, this semi-supervised learning approach only requires continuous move performances from the user with small pauses. Key pose transitions and moves execution speeds are used as input to a novel human move recognition algorithm that recognizes unsegmented moves online, achieving high robustness and very low latency in our experiments, while also effective in distinguishing moves that differ only in speed.","authors":["Thales Vieira","Romain Faugeroux","Dimas Mart√≠nez","Thomas Lewiner"],"meta":["February 2017Machine Vision and Applications 28(1):185-200","DOI:10.1007/s00138-016-0818-y"],"references":["300483468_Temporal_Order-Preserving_Dynamic_Quantization_for_Human_Action_Recognition_from_Multimodal_Sensor_Streams","282193521_Grassmannian_Representation_of_Motion_Depth_for_3D_Human_Gesture_and_Action_Recognition","278767979_Space-Time_Pose_Representation_for_3D_Human_Action_Recognition","275986227_3D_Gestural_Interaction_The_State_of_the_Field","309309732_Discriminative_orderlet_mining_for_real-time_recognition_of_human-object_interaction","292019817_Discriminative_Orderlet_Mining_for_Real-Time_Recognition_of_Human-Object_Interaction","288989718_Robust_3D_Action_Recognition_with_Random_Occupancy_Patterns","288186393_Clustered_Spatio-temporal_Manifolds_for_Online_Action_Recognition","278653717_STOP_Space-Time_Occupancy_Patterns_for_3D_Action_Recognition_from_Depth_Map_Sequences","275657932_Simplified_Training_for_Gesture_Recognition","266030665_Continuous_Human_Action_Recognition_in_Ambient_Assisted_Living_Scenarios","264312484_A_discussion_on_the_validation_tests_employed_to_compare_human_action_recognition_methods_using_the_MSR_Action3D_dataset","274738816_Fusing_Multiple_Features_for_Depth-Based_Action_Recognition","268750262_TriViews_A_general_framework_to_use_3D_depth_data_effectively_for_action_recognition","264741841_Accurate_3D_Action_Recognition_using_Learning_on_the_Grassmann_Manifold"]}