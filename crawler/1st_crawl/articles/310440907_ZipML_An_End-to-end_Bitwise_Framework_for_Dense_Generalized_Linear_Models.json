{"id":"310440907_ZipML_An_End-to-end_Bitwise_Framework_for_Dense_Generalized_Linear_Models","abstract":"We present ZipML, the first framework for training dense generalized linear models using end-to-end low-precision representation--in ZipML, all movements of data, including those for input samples, model, and gradients, are represented using as little as two bits per component. Within our framework, we have successfully compressed, separately, the input data by 16x, gradient by 16x, and model by 16x while still getting the same training result. Even for the most challenging datasets, we find that robust convergence can be ensured using only an end-to-end 8-bit representation or a 6-bit representation if only samples are quantized. Our work builds on previous research on using low-precision representations for gradient and model in the context of stochastic gradient descent. Our main technical contribution is a new set of techniques which allow the training samples to be processed with low precision, without affecting the convergence of the algorithm. In turn, this leads to a system where all data items move in a quantized, low precision format. In particular, we first establish that randomized rounding, while sufficient when quantizing the model and the gradients, is biased when quantizing samples, and thus leads to a different training result. We propose two new data representations which converge to the same solution as in the original data representation both in theory and empirically and require as little as 2-bits per component. As a result, if the original data is stored as 32-bit floats, we decrease the bandwidth footprint for each training iteration by up to 16x. Our results hold for models such as linear regression and least squares SVM. ZipML raises interesting theoretical questions related to the robustness of SGD to approximate data, model, and gradient representations. We conclude this working paper by a description of ongoing work extending these preliminary results.","authors":["Hantian Zhang","Kaan Kara","Jerry Li","Dan Alistarh"],"meta":["November 2016"],"references":["308964551_QSGD_Randomized_Quantization_for_Communication-Optimal_Stochastic_Gradient_Descent","300621719_Fast_and_Near-Optimal_Algorithms_for_Approximating_Distributions_by_Histograms","319770342_Compressing_Deep_Convolutional_Networks_using_Vector_Quantization","319770334_Deep_Compression_Compressing_Deep_Neural_Networks_with_Pruning_Trained_Quantization_and_Huffman_Coding","319770230_XNOR-Net_ImageNet_Classification_Using_Binary_Convolutional_Neural_Networks","319770111_Improving_the_speed_of_neural_networks_on_CPUs","318126780_FPGA-Accelerated_Dense_Linear_Machine_Learning_A_Precision-Convergence_Trade-Off","313501588_Accelerating_stochastic_gradient_descent_using_predictive_variance_reduction","311609205_Quantized_Convolutional_Neural_Networks_for_Mobile_Devices","298421377_Chebyshev_polynomial_approximation_for_activation_sigmoid_function"]}