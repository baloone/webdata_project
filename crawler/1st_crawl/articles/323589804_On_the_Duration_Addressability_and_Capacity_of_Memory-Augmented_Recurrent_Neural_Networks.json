{"id":"323589804_On_the_Duration_Addressability_and_Capacity_of_Memory-Augmented_Recurrent_Neural_Networks","abstract":"Memory-augmented recurrent neural networks (M-RNNs) have demonstrated empirically that they are very attractive for many applications, but a good theoretical understanding of their behaviors is unclear as yet. In this study, three analytical indicators named duration, addressability, and capacity of general forms of the additional memory in M-RNNs are formalized. The analysis results of the interactions among these indicators reveal that it is hard for a M-RNN to simultaneously provide good performance on more than two out of three of indicators. Meanwhile, the duration, addressability, and capacity are applied to analyze and compare two M-RNNs: long short term memory (LSTM) and neural Turing machine (NTM) for different cases. The comparison results show that none of the models has better performance on one indicator than the other model all the time. Moreover, it is found that separating memory system into submemories can bring the increasing duration and addressability, and the decreasing capacity for the whole memory system.","authors":["Quan Zhibin","Zhiqiang Gao","Weili Zeng","Xuelian Li"],"meta":["March 2018IEEE Access PP(99):1-1","DOI:10.1109/ACCESS.2018.2812766"],"references":["321352236_Action_Recognition_in_Video_Sequences_using_Deep_Bi-directional_LSTM_with_CNN_Features","313100943_Memory_Augmented_Neural_Networks_with_Wormhole_Connections","309551291_Scaling_Memory-Augmented_Neural_Networks_with_Sparse_Reads_and_Writes","327766029_Statistics_for_Long-Memory_Processes","319769995_End-To-End_Memory_Networks","318739728_Neural_Semantic_Encoders","310752282_Long_range_dependence","309317293_Using_Fast_Weights_to_Attend_to_the_Recent_Past","309091100_Hybrid_computing_using_a_neural_network_with_dynamic_external_memory","305388779_Neural_Semantic_Encoders"]}