{"id":"303980153_Exploiting_Hierarchical_Locality_in_Deep_Parallel_Architectures","abstract":"Parallel computers are becoming deeply hierarchical. Locality-aware programming models allow programmers to control locality at one level through establishing affinity between data and executing activities. This, however, does not enable locality exploitation at other levels. Therefore, we must conceive an efficient abstraction of hierarchical locality and develop techniques to exploit it. Techniques applied directly by programmers, beyond the first level, burden the programmer and hinder productivity. In this article, we propose the Parallel Hierarchical Locality Abstraction Model for Execution (PHLAME). PHLAME is an execution model to abstract and exploit machine hierarchical properties through locality-aware programming and a runtime that takes into account machine characteristics, as well as a data sharing and communication profile of the underlying application. This article presents and experiments with concepts and techniques that can drive such runtime system in support of PHLAME. Our experiments show that our techniques scale up and achieve performance gains of up to 88&percnt;.","authors":["Ahmad Anbar","Olivier Serres","Engin Kayraklioglu","Abdel-Hameed Badawy"],"meta":["June 2016ACM Transactions on Architecture and Code Optimization 13(2):1-25","DOI:10.1145/2897783","Project: Hierarchical Locality"],"references":["308496529_PHLAME_Hierarchical_Locality_Exploitation_Using_the_PGAS_Model","284738401_Where_should_the_threads_go_Leveraging_hierarchical_data_locality_to_solve_the_thread_affinity_dilemma","277549809_The_Tau_Parallel_Performance_System","261495727_Using_the_Translation_Lookaside_Buffer_to_Map_Threads_in_Parallel_Applications_Based_on_Shared_Memory","260525752_Process_Placement_in_Multicore_ClustersAlgorithmic_Issues_and_Practical_Techniques","249863742_Parallel_Performance_Wizard_A_Performance_Analysis_Tool_for_Global-Address-Space_Applications","242479489_Kumar_V_A_Fast_and_High_Quality_Multilevel_Scheme_for_Partitioning_Irregular_Graphs_SIAM_Journal_on_Scientific_Computing_201_359-392","232653420_Zoltan_Data_Management_Service_for_Parallel_Dynamic_Applications","221597359_Open_MPI_Goals_Concept_and_Design_of_a_Next_Generation_MPI_Implementation","221201975_Design_and_Evaluation_of_Network_Topology-Speed-_Aware_Broadcast_Algorithms_for_InfiniBand_Clusters","220949187_Parallel_performance_wizard_A_performance_analysis_tool_for_partitioned_global-address-space_programming","220781955_The_NAS_parallel_benchmarks--Summary_and_preliminary_results","44256084_hwloc_A_Generic_Framework_for_Managing_Hardware_Affinities_in_HPC_Applications","3300222_Efficient_algorithms_for_all-to-all_communications_in_multiport_message-passing_systems","2797268_A_Fast_And_High_Quality_Multilevel_Scheme_For_Partitioning_Irregular_Graphs","300829120_Compiler-Driven_Data_Layout_Transformation_for_Heterogeneous_Platforms","282994140_A_Priority-Based_Scheduling_Heuristic_to_Maximize_Parallelism_of_Ready_Tasks_for_DAG_Applications","282983099_Maximizing_Throughput_on_a_Dragonfly_Network","262283002_Bandwidth-optimal_all-to-all_exchanges_in_fat_tree_networks","261272241_Hierarchical_task_mapping_of_cell-based_AMR_cosmology_simulations","258139318_Parallel_Programmability_and_the_Chapel_Language","236527896_OpenSHMEM_Towards_a_Unified_RMA_Model","224130591_Cache_Hierarchy_and_Memory_Subsystem_of_the_AMD_Opteron_Processor","221643452_Faster_Topology-aware_Collective_Algorithms_Through_Non-minimal_Communication","220950379_Using_Memory_Access_Traces_to_Map_Threads_and_Data_on_Hierarchical_Multi-core_Platforms","4222053_UPC_performance_and_potential_a_NPB_experimental_study","3344403_OpenMP_An_Industry-Standard_API_for_Shared-Memory_Programming","2955586_Simics_A_Full_System_Simulation_Platform","2534946_UPC_performance_and_potential_A_NPB_experimental_study"]}