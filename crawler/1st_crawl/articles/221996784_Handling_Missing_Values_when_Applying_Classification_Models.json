{"id":"221996784_Handling_Missing_Values_when_Applying_Classification_Models","abstract":"Much work has studied the effect of different treatments of missing values on model induction, but little work has analyzed treatments for the common case of missing values at prediction time. This paper first compares several different methods — predictive value imputation, the distribution-based imputation used by C4.5, and using reduced models — for applying classification trees to instances with missing values (and also shows evidence that the results generalize to bagged trees and to logistic regression). The results show that for the two most popular treatments, each is preferable under different conditions. Strikingly the reduced-models approach, seldom mentioned or used, consistently outperforms the other two methods, sometimes by a large margin. The lack of attention to reduced modeling may be due in part to its (perceived) expense in terms of computation or storage. Therefore, we then introduce and evaluate alternative, hybrid approaches that allow users to balance between more accurate but computationally expensive reduced modeling and the other, less accurate but less computationally expensive treatments. The results show that the hybrid methods can scale gracefully to the amount of investment in computation/storage, and that they outperform imputation even for small investments.","authors":["Maytal Saar-Tsechansky","Foster Provost"],"meta":["July 2007Journal of Machine Learning Research 8:1625-1657","SourceDBLP"],"references":["290824736_An_Empirical_Comparison_of_Voting_Classification_Algorithms_Bagging_Boosting_and_Variants","223713209_Wrappers_for_Feature_Subset_Selection","221344679_Predicting_good_probabilities_with_supervised_learning","220355769_An_Analysis_of_Four_Missing_Data_Treatment_Methods_for_Supervised_Learning","220343893_An_Empirical_Comparison_of_Voting_Classification_Algorithms_Bagging_Boosting_and_Variants","220321037_Learning_Probabilistic_Models_of_Link_Structure","215991438_Relational_Dependency_Networks","215990041_Tree_Induction_vs_Logistic_Regression_A_Learning-Curve_Analysis","2761391_Learning_to_Classify_Incomplete_Examples","2648965_Supervised_learning_from_incomplete_data_via_an_EM_approach","2487474_Personalization_from_Incomplete_Data_What_You_Don't_Know_Can_Hurt","2245293_Handling_Missing_Data_in_Trees_Surrogate_Splits_Or_Statistical_Imputation","285021015_Learning_Bayesian_networks_with_local_structure","275340906_Random_Forests","265620140_Statistical_Analysis_With_Missing_Data","262246632_Mixture_models_for_learning_from_incomplete_data","244446198_UCI_Repository_of_machine_learning_databases_Machine-readable_data_repository","242358078_The_Elements_Of_Statistical_Learning","242327373_Dependency_networks_for_inference","235961479_Classification_and_Regression_Trees","234793335_A_theory_of_the_learnable","233896194_Analysis_of_Incomplete_Data","222476846_Concept_learning_and_heuristic_classification_in_weak-theory_domains","222161782_Knowing_what_doesn't_matter_Exploiting_the_omission_of_irrelevant_data","221900847_Data_Mining_Practical_Machine_Learning_Tools_And_Techniques","221345469_Decision_trees_with_minimal_costs","220688794_C45_Programs_for_Machine_Learning","215721461_Maximum_Likelihood_From_Incomplete_Data_Via_The_EM_algorithm","44500847_Multiple_Imputation_For_Nonresponse_In_Surveys","33051652_Logistic_Model_Trees","30875797_A_Theory_of_the_Learnable","21299237_Algorithms_for_Bayesian_Belief-Network_Precomputation","2637367_Unknown_Attribute_Values_In_Induction","2457211_Combining_Labeled_and_Unlabeled_Data_with_Co-Training","2243517_Understanding_the_Behavior_of_Co-training"]}