{"id":"326023170_Making_machine_learning_robust_against_adversarial_inputs","abstract":"Such inputs distort how machine-learning-based systems are able to function in the world as it is.","authors":["Ian Goodfellow","Patrick McDaniel","Nicolas Papernot"],"meta":["June 2018Communications of the ACM 61(7):56-66","DOI:10.1145/3134599"],"references":["326276006_SoK_Security_and_Privacy_in_Machine_Learning","318370372_Safety_Verification_of_Deep_Neural_Networks","317062059_Ensemble_Adversarial_Training_Attacks_and_Defenses","316911609_Safety_Verification_of_Deep_Neural_Networks","320672071_Evading_Classifiers_by_Morphing_in_the_Dark","319770387_Deep_Sparse_Rectifier_Neural_Networks","319770378_Explaining_and_harnessing_adversarial_examples","319769909_Distilling_the_Knowledge_in_a_Neural_Network","319394978_Artificial_Intelligence_---_A_Modern_Approach","317919653_Towards_Evaluating_the_Robustness_of_Neural_Networks"]}