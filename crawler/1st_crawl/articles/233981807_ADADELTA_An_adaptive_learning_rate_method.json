{"id":"233981807_ADADELTA_An_adaptive_learning_rate_method","abstract":"We present a novel per-dimension learning rate method for gradient descent\ncalled ADADELTA. The method dynamically adapts over time using only first order\ninformation and has minimal computational overhead beyond vanilla stochastic\ngradient descent. The method requires no manual tuning of a learning rate and\nappears robust to noisy gradient information, different model architecture\nchoices, various data modalities and selection of hyperparameters. We show\npromising results compared to other methods on the MNIST digit classification\ntask using a single machine and on a large scale voice dataset in a distributed\ncluster environment.","authors":["Matthew D. Zeiler"],"meta":["December 2012","SourcearXiv"],"references":["266225209_Large_Scale_Distributed_Deep_Networks","236735729_No_More_Pesky_Learning_Rates","225273758_No_More_Pesky_Learning_Rates","216792889_Improving_the_Convergence_of_Back-Propagation_Learning_with_Second-Order_Methods","265350973_Application_of_Pretrained_Deep_Neural_Networks_to_Large_Vocabulary_Conversational_Speech_Recognition","236736791_A_Stochastic_Approximation_Method","229091480_Learning_Representations_by_Back_Propagating_Errors","221497515_Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization"]}