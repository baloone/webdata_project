{"id":"317230380_Convergence_Analysis_of_Two-layer_Neural_Networks_with_ReLU_Activation","abstract":"In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing. In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called \"identity mapping\". We prove that, if input follows from Gaussian distribution, with standard $O(1/\\sqrt{d})$ initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the \"identity mapping\" makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks. Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in \"two phases\": In phase I, the gradient points to the wrong direction, however, a potential function $g$ gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.","authors":["Yuanzhi Li","Yang Yuan"],"meta":["May 2017"],"references":["284787964_Learning_Halfspaces_and_Neural_Networks_with_Random_Initialization","269935498_Qualitatively_characterizing_neural_network_optimization_problems","269040844_The_Loss_Surface_of_Multilayer_Networks","260126867_On_the_Number_of_Linear_Regions_of_Deep_Neural_Networks","259400019_On_the_number_of_response_regions_of_deep_feed_forward_networks_with_piece-wise_linear_activations","220320677_Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization","215616968_Understanding_the_difficulty_of_training_deep_feedforward_neural_networks","3078296_Barron_AE_Universal_approximation_bounds_for_superpositions_of_a_sigmoidal_function_IEEE_Trans_on_Information_Theory_39_930-945","2719277_Dynamics_of_On-Line_Gradient_Descent_Learning_for_Multilayer_Neural_Networks","332183957_Provable_Methods_for_Training_Neural_Networks_with_Sparse_Connectivity","332138022_Beating_the_Perils_of_Non-Convexity_Guaranteed_Training_of_Neural_Networks_using_Tensor_Methods","319770387_Deep_Sparse_Rectifier_Neural_Networks","319770257_Exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks","319770239_Identifying_and_attacking_the_saddle_point_problem_in_high-dimensional_non-convex_optimization","319770166_Provable_Bounds_for_Learning_Some_Deep_Representations","310235954_Identity_Matters_in_Deep_Learning","309854393_Diversity_Leads_to_Generalization_in_Neural_Networks","307636746_Distribution-Specific_Hardness_of_Learning_Neural_Networks","303449182_Deep_Learning_without_Poor_Local_Minima","301844206_Toward_Deeper_Understanding_of_Neural_Networks_The_Power_of_Initialization_and_a_Dual_View_on_Expressivity","289406223_Learning_polynomials_with_neural_networks","286512696_Deep_Residual_Learning_for_Image_Recognition","286271944_On_the_importance_of_initialization_and_momentum_in_deep_learning","284219543_Expressiveness_of_Rectifier_Networks","283986578_On_the_Quality_of_the_Initial_Basin_in_Overspecified_Neural_Networks","273388357_Escaping_From_Saddle_Points_---_Online_Stochastic_Gradient_for_Tensor_Decomposition","272027131_Delving_Deep_into_Rectifiers_Surpassing_Human-Level_Performance_on_ImageNet_Classification","269935079_Adam_A_Method_for_Stochastic_Optimization","266560880_On_the_Computational_Efficiency_of_Training_Neural_Networks","246899051_Multilayer_feedforward_networks_are_universal_approximator","226439292_Approximation_by_superpositions_of_a_sigmoidal_function_Math_Cont_Sig_Syst_MCSS_2303-314","221345737_Rectified_Linear_Units_Improve_Restricted_Boltzmann_Machines_Vinod_Nair","45906377_Non-asymptotic_Theory_of_Random_Matrices_Extreme_Singular_Values","11034752_Training_a_Single_Sigmoidal_Neuron_Is_Hard"]}