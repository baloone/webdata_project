{"id":"314259698_Generative_Poisoning_Attack_Method_Against_Neural_Networks","abstract":"Poisoning attack is identified as a severe security threat to machine learning algorithms. In many applications, for example, deep neural network (DNN) models collect public data as the inputs to perform re-training, where the input data can be poisoned. Although poisoning attack against support vector machines (SVM) has been extensively studied before, there is still very limited knowledge about how such attack can be implemented on neural networks (NN), especially DNNs. In this work, we first examine the possibility of applying traditional gradient-based method (named as the direct gradient method) to generate poisoned data against NNs by leveraging the gradient of the target model w.r.t. the normal data. We then propose a generative method to accelerate the generation rate of the poisoned data: an auto-encoder (generator) used to generate poisoned data is updated by a reward function of the loss, and the target NN model (discriminator) receives the poisoned data to calculate the loss w.r.t. the normal data. Our experiment results show that the generative method can speed up the poisoned data generation rate by up to 239.38x compared with the direct gradient method, with slightly lower model accuracy degradation. A countermeasure is also designed to detect such poisoning attack methods by checking the loss of the target model.","authors":["Chaofei Yang","Qing wu","Hai Li","Yiran Chen"],"meta":["March 2017"],"references":["286134669_MXNet_A_Flexible_and_Efficient_Machine_Learning_Library_for_Heterogeneous_Distributed_Systems","259440613_Intriguing_properties_of_neural_networks","220320948_Online_Anomaly_Detection_under_Adversarial_Impact","2985446_Gradient-Based_Learning_Applied_to_Document_Recognition","354166644_Accurate_and_compact_large_vocabulary_speech_recognition_on_mobile_devices","319770378_Explaining_and_harnessing_adversarial_examples","319770355_Generative_Adversarial_Nets","310823743_Security_of_neuromorphic_computing_thwarting_learning_attacks_using_memristor's_obsolescence_effect","306218037_Learning_multiple_layers_of_features_from_tiny_images","303521174_An_Analysis_of_Deep_Neural_Network_Models_for_Practical_Applications","264503005_Systematic_Poisoning_Attacks_on_and_Defenses_for_Machine_Learning_in_Healthcare","262173234_Adversarial_machine_learning","228095591_Poisoning_Attacks_against_Support_Vector_Machines","221611962_ANTIDOTE_understanding_and_defending_against_poisoning_of_anomaly_detectors","220343885_The_security_of_machine_learning"]}