{"id":"320597233_Wider_and_Deeper_Cheaper_and_Faster_Tensorized_LSTMs_for_Sequence_Learning","abstract":"Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.","authors":["Zhen He","Shaobing Gao","Liang Xiao","Daxue Liu"],"meta":["December 2017","Conference: Advances In Neural Information Processing Systems (NIPS)At: Long Beach, CA, USA"],"references":["320280030_Dilated_Recurrent_Neural_Networks","309606984_Full-Capacity_Unitary_Recurrent_Neural_Networks","308026508_WaveNet_A_Generative_Model_for_Raw_Audio","319770411_Torch7_A_Matlab-like_Environment_for_Machine_Learning","319770235_Spatio-temporal_video_autoencoder_with_differentiable_memory","319622458_Training_RNNs_as_Fast_as_CNNs","317100692_Fast-Slow_Recurrent_Neural_Networks","309738583_Quasi-Recurrent_Neural_Networks","309484464_Can_Active_Memory_Replace_Attention","308277417_Recurrent_Instance_Segmentation"]}