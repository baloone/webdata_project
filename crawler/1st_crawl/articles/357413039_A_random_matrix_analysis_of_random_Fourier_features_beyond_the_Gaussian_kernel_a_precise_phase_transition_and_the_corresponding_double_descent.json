{"id":"357413039_A_random_matrix_analysis_of_random_Fourier_features_beyond_the_Gaussian_kernel_a_precise_phase_transition_and_the_corresponding_double_descent","abstract":"This article characterizes the exact asymptotics of random Fourier feature (RFF) regression, in the realistic setting where the number of data samples n , their dimension p , and the dimension of feature space N are all large and comparable. In this regime, the random RFF Gram matrix no longer converges to the well-known limiting Gaussian kernel matrix (as it does when N → ∞ alone), but it still has a tractable behavior that is captured by our analysis. This analysis also provides accurate estimates of training and test regression errors for large n , p , N . Based on these estimates, a precise characterization of two qualitatively different phases of learning, including the phase transition between them, is provided; and the corresponding double descent test error curve is derived from this phase transition behavior. These results do not depend on strong assumptions on the data distribution, and they perfectly match empirical results on real-world data sets.","authors":["Zhenyu Liao","Romain Couillet","Michael W. Mahoney"],"meta":["December 2021Journal of Statistical Mechanics Theory and Experiment 2021(12):124006","DOI:10.1088/1742-5468/ac3a77"],"references":["330638462_Heavy-Tailed_Universality_Predicts_Trends_in_Test_Accuracies_for_Very_Large_Pre-Trained_Deep_Neural_Networks","330638379_Traditional_and_Heavy-Tailed_Self_Regularization_in_Neural_Network_Models","323444450_The_Emergence_of_Spectral_Universality_in_Deep_Networks","321070341_Resurrecting_the_sigmoid_in_deep_learning_through_dynamical_isometry_theory_and_practice","320626962_Rethinking_generalization_requires_revisiting_old_ideas_statistical_mechanics_approaches_and_complex_learning_behavior","320321651_High-dimensional_dynamics_of_generalization_error_in_neural_networks","319312259_Fashion-MNIST_a_Novel_Image_Dataset_for_Benchmarking_Machine_Learning_Algorithms","313845060_A_Random_Matrix_Approach_to_Neural_Networks","294871659_Generalization_Properties_of_Learning_with_Random_Features","280033961_High-Dimensional_Asymptotics_of_Prediction_Ridge_Regression_and_Classification","13380838_Statistical_mechanics_of_learning_from_examples","3233585_A_Framework_for_Uplink_Power_Control_in_Cellular_Radio_Systems","2985446_Gradient-Based_Learning_Applied_to_Document_Recognition","352183506_The_Generalization_Error_of_Random_Features_Regression_Precise_Asymptotics_and_the_Double_Descent_Curve","350720472_A_model_of_double_descent_for_high-dimensional_binary_linear_classification","343009795_Just_interpolate_Kernel_Ridgeless_regression_can_generalize","340921055_Benign_overfitting_in_linear_regression","338082096_Nonlinear_random_matrix_theory_for_deep_learning","337850255_Statistical_Mechanics_of_Deep_Learning","334712461_Statistical_Mechanics_Methods_for_Discovering_Knowledge_from_Modern_Production_Quality_Neural_Networks","334663035_Reconciling_modern_machine-learning_practice_and_the_classical_bias-variance_trade-off","319770355_Generative_Adversarial_Nets","317933208_On_the_equivalence_between_kernel_quadrature_rules_random_feature_expansions","310827970_Weighted_sums_of_random_kitchen_sinks_replacing_minimization_with_randomization_in_learning","221620515_Random_Features_for_Large-Scale_Kernel_Machines","220320733_On_the_Impact_of_Kernel_Approximation_on_Learning_Accuracy","216706062_The_statistical_mechanics_of_learning_a_rule","51539295_Efficient_Additive_Kernels_via_Explicit_Feature_Maps","2813278_Computing_With_Infinite_Networks","2303451_Rigorous_Learning_Curve_Bounds_from_Statistical_Mechanics","2120949_Deterministic_equivalents_for_certain_functionals_of_large_random_matrices"]}