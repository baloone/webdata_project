{"id":"335781169_Transformer-XL_Attentive_Language_Models_beyond_a_Fixed-Length_Context","authors":["Zihang Dai","Zhilin Yang","Yiming Yang","Jaime Carbonell"],"meta":["January 2019","DOI:10.18653/v1/P19-1285","Conference: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"],"references":["323735015_Independently_Recurrent_Neural_Network_IndRNN_Building_A_Longer_and_Deeper_RNN","315748314_Factorization_tricks_for_LSTM_networks","304226007_On_Multiplicative_Integration_with_Recurrent_Neural_Networks","269116401_Skip-gram_Language_Modeling_Using_Sparse_Non-negative_Matrix_Probability_Estimation","265252627_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate","259239818_One_Billion_Word_Benchmark_for_Measuring_Progress_in_Statistical_Language_Modeling","221489926_Recurrent_neural_network_based_language_model","13853244_Long_Short-term_Memory","335685749_Character-Level_Language_Modeling_with_Deeper_Self-Attention","323956750_An_Analysis_of_Neural_Language_Modeling_at_Multiple_Scales","323570759_An_Empirical_Evaluation_of_Generic_Convolutional_and_Recurrent_Networks_for_Sequence_Modeling","323118469_Efficient_Neural_Architecture_Search_via_Parameters_Sharing","319769994_A_Theoretically_Grounded_Application_of_Dropout_in_Recurrent_Neural_Networks","317558625_Attention_Is_All_You_Need","317100692_Fast-Slow_Recurrent_Neural_Networks","312619873_Outrageously_Large_Neural_Networks_The_Sparsely-Gated_Mixture-of-Experts_Layer","311648230_Improving_Neural_Language_Models_with_a_Continuous_Cache","309730114_Tying_Word_Vectors_and_Word_Classifiers_A_Loss_Framework_for_Language_Modeling","308646548_Multiplicative_LSTM_for_sequence_modelling","308646525_Pointer_Sentinel_Mixture_Models","301847993_Exploring_the_Limits_of_Language_Modeling","283531914_Semi-supervised_Sequence_Learning","255563511_Context_Dependent_Recurrent_Neural_Network_Language_Model"]}