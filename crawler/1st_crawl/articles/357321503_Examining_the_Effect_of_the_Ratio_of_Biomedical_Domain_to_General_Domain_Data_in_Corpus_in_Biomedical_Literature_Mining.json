{"id":"357321503_Examining_the_Effect_of_the_Ratio_of_Biomedical_Domain_to_General_Domain_Data_in_Corpus_in_Biomedical_Literature_Mining","abstract":"Biomedical terms extracted using Word2vec, the most popular word embedding model in recent years, serve as the foundation for various natural language processing (NLP) applications, such as biomedical information retrieval, relation extraction, and recommendation systems. The objective of this study is to examine how changes in the ratio of the biomedical domain to general domain data in the corpus affect the extraction of similar biomedical terms using Word2vec. We downloaded abstracts of 214,892 articles from PubMed Central (PMC) and the 3.9 GB Billion Word (BW) benchmark corpus from the computer science community. The datasets were preprocessed and grouped into 11 corpora based on the ratio of BW to PMC, ranging from 0:10 to 10:0, and then Word2vec models were trained on these corpora. The cosine similarities between the biomedical terms obtained from the Word2vec models were then compared in each model. The results indicated that the models trained with both BW and PMC data outperformed the model trained only with medical data. The similarity between the biomedical terms extracted by the Word2vec model increased when the ratio of the biomedical domain to general domain data was 3:7 to 5:5. This study allows NLP researchers to apply Word2vec based on more information and increase the similarity of extracted biomedical terms to improve their effectiveness in NLP applications, such as biomedical information extraction.","authors":["Ziheng Zhang","Feng Han","Hongjian Zhang","Tomohiro Aoki"],"meta":["December 2021Applied Sciences 12(1):154","DOI:10.3390/app12010154"],"references":["337741099_Deep_learning_in_clinical_natural_language_processing_A_methodical_review","335750430_BioBERT_a_pre-trained_biomedical_language_representation_model_for_biomedical_text_mining","326555328_Evaluating_semantic_relations_in_neural_word_embeddings_with_biomedical_and_general_domain_knowledge_bases","323664116_Deep_learning_with_word_embeddings_improves_biomedical_named_entity_recognition","318189009_Semantic_relatedness_and_similarity_of_biomedical_terms_Examining_the_effects_of_recency_size_and_section_of_biomedical_publications_on_the_performance_of_word2vec","306099868_How_to_Train_good_Word_Embeddings_for_Biomedical_NLP","354184190_One_billion_word_benchmark_for_measuring_progress_in_statistical_language_modeling","319770439_Efficient_Estimation_of_Word_Representations_in_Vector_Space","307955489_Distributed_representations_of_words_and_phrases_and_their_compositionality","306247934_Corpus_Domain_Effects_on_Distributional_Semantic_Modeling_of_Medical_Terms"]}