{"id":"259400035_Auto-Encoding_Variational_Bayes","abstract":"Can we efficiently learn the parameters of directed probabilistic models, in\nthe presence of continuous latent variables with intractable posterior\ndistributions, and in case of large datasets? We introduce a novel learning and\napproximate inference method that works efficiently, under some mild\nconditions, even in the on-line and intractable case. The method involves\noptimization of a stochastic objective function that can be straightforwardly\noptimized w.r.t. all parameters, using standard gradient-based optimization\nmethods.\nThe method does not require the typically expensive sampling loops per\ndatapoint required for Monte Carlo EM, and all parameter updates correspond to\noptimization of the variational lower bound of the marginal likelihood, unlike\nthe wake-sleep algorithm. These theoretical advantages are reflected in\nexperimental results.","authors":["Diederik P Kingma","Max Welling"],"meta":["December 2013","SourcearXiv"],"references":["240308775_Representation_Learning_A_Review_and_New_Perspectives","237053970_Deep_Generative_Stochastic_Networks_Trainable_by_Backprop","220320677_Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization","47438419_Fast_Inference_in_Sparse_Coding_Algorithms_with_Applications_to_Object_Recognition","262991675_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models","234817582_Sample-based_Non-uniform_random_variate_generation","225092312_Hybrid_Monte_Carlo","221620248_An_Application_of_the_Principle_of_Maximum_Information_Preservation_to_Linear_Systems","15437023_The_Wake-Sleep_Algorithm_for_Unsupervised_Neural_Networks","2491114_Em_algorithms_for_pca_and_spca"]}