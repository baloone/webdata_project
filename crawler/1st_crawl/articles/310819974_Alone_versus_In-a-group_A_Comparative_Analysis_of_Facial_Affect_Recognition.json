{"id":"310819974_Alone_versus_In-a-group_A_Comparative_Analysis_of_Facial_Affect_Recognition","abstract":"Automatic affect analysis and understanding has become a well established research area in the last two decades. Recent works have started moving from individual to group scenarios. However, little attention has been paid to comparing the affect expressed in individual and group settings. This paper presents a framework to investigate the differences in affect recognition models along arousal and valence dimensions in individual and group settings. We analyse how a model trained on data collected from an individual setting performs on test data collected from a group setting, and vice versa. A third model combining data from both individual and group settings is also investigated. A set of experiments is conducted to predict the affective states along both arousal and valence dimensions on two newly collected databases that contain sixteen participants watching affective movie stimuli in individual and group settings, respectively. The experimental results show that (1) the affect model trained with group data performs better on individual test data than the model trained with individual data tested on group data, indicating that facial behaviours expressed in a group setting capture more variation than in an individual setting; and (2) the combined model does not show better performance than the affect model trained with a specific type of data (i.e., individual or group), but proves a good compromise. These results indicate that in settings where multiple affect models trained with different types of data are not available, using the affect model trained with group data is a viable solution.","authors":["Wenxuan Mou","Hatice Gunes","Ioannis Patras"],"meta":["October 2016","DOI:10.1145/2964284.2967276","Conference: the 2016 ACM"],"references":["284593613_A_Temporally_Piece-wise_Fisher_Vector_Approach_for_Depression_Analysis","272789660_Comparing_Models_of_Disengagement_in_Individual_and_Group_Interactions","272433866_Automatic_Group_Happiness_Intensity_Analysis","267502070_Automatic_Analysis_of_Facial_Affect_A_Survey_of_Registration_Representation_and_Recognition","265284566_Local_Zernike_Moment_Representation_for_Facial_Affect_Recognition","257672334_Dense_Trajectories_and_Motion_Boundary_Descriptors_for_Action_Recognition","257672261_Image_Classification_with_the_Fisher_Vector_Theory_and_Practice","220147568_IEMOCAP_Interactive_emotional_dyadic_motion_capture_database","23493444_A_Survey_of_Affect_Recognition_Methods_Audio_Visual_and_Spontaneous_Expressions","307881957_LIBSVM_A_library_for_support_vector_machines","305429477_Automatic_Recognition_of_Emotions_and_Membership_in_Group_Videos","276014525_Group-level_Arousal_and_Valence_Recognition_from_Static_Images_Face_Body_and_Context","274056476_Social_Facilitation","272623654_Libsvm","261479309_Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment","257093268_Categorical_and_dimensional_affect_analysis_in_continuous_input_Current_trends_and_future_directions","257091783_Fusion_of_facial_expressions_and_EEG_for_implicit_affective_tagging","221291986_Continuous_emotion_detection_in_response_to_music_videos","6397809_Dynamic_Texture_Recognition_Using_Local_Binary_Patterns_with_an_Application_to_Facial_Expressions"]}