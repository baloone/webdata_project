{"id":"328973293_Contrasting_temporal_difference_and_opportunity_cost_reinforcement_learning_in_an_empirical_money-emergence_paradigm","abstract":"Money is a fundamental and ubiquitous institution in modern economies. However, the question of its emergence remains a central one for economists. The monetary search-theoretic approach studies the conditions under which commodity money emerges as a solution to override frictions inherent to interindividual exchanges in a decentralized economy. Although among these conditions, agents’ rationality is classically essential and a prerequisite to any theoretical monetary equilibrium, human subjects often fail to adopt optimal strategies in tasks implementing a search-theoretic paradigm when these strategies are speculative, i.e., involve the use of a costly medium of exchange to increase the probability of subsequent and successful trades. In the present work, we hypothesize that implementing such speculative behaviors relies on reinforcement learning instead of lifetime utility calculations, as supposed by classical economic theory. To test this hypothesis, we operationalized the Kiyotaki and Wright paradigm of money emergence in a multistep exchange task and fitted behavioral data regarding human subjects performing this task with two reinforcement learning models. Each of them implements a distinct cognitive hypothesis regarding the weight of future or counterfactual rewards in current decisions. We found that both models outperformed theoretical predictions about subjects’ behaviors regarding the implementation of speculative strategies and that the latter relies on the degree of the opportunity costs consideration in the learning process. Speculating about the marketability advantage of money thus seems to depend on mental simulations of counterfactual events that agents are performing in exchange situations.","authors":["Germain Lefebvre","Aurélien Nioche","Sacha Bourgeois-Gironde","Stefano Palminteri"],"meta":["November 2018Proceedings of the National Academy of Sciences 115(49):201813197","DOI:10.1073/pnas.1813197115"],"references":["312176098_Reinforcement_learning_accounts_for_moody_conditional_cooperation_behavior_Experimental_results","282126362_Counterfactual_Thought","281294431_Contextual_modulation_of_value_signals_in_reward_and_punishment_learning","259919533_VBA_A_Probabilistic_Treatment_of_Nonlinear_Models_for_Neurobiological_and_Behavioural_Data","316639384_The_Importance_of_Falsification_in_Computational_Cognitive_Modeling","313562063_Designing_economic_agents_that_act_like_human_agents_A_behavioral_approach_to_bounded_rationality","313135362_Q-learning","276951245_Learning_to_Be_Overconfident","246854087_Modelling_Predicting_How_People_Play_Games_Reinforcement_learning_in_experimental_games_with_unique","244450954_A_Suggestion_for_Simplifying_the_Theory_of_Money"]}