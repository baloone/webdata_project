{"id":"324805875_Recurrent_networks_with_attention_and_convolutional_networks_for_sentence_representation_and_classification","abstract":"In this paper, we propose a bi-attention, a multi-layer attention and an attention mechanism and convolution neural network based text representation and classification model (ACNN). The bi-attention have two attention mechanism to learn two context vectors, forward RNN with attention to learn forward context vector \\(\\overrightarrow {\\mathbf {c}}\\) and backward RNN with attention to learn backward context vector \\(\\overleftarrow {\\mathbf {c}}\\), and then concatenation \\(\\overrightarrow {\\mathbf {c}}\\) and \\(\\overleftarrow {\\mathbf {c}}\\) to get context vector c. The multi-layer attention is the stack of the bi-attention. In the ACNN, the context vector c is obtained by the bi-attention, then the convolution operation is performed on the context vector c, and the max-pooling operation is used to reduce the dimension. After max-pooling operation the text is converted to low-dimensional sentence vector m. Finally, the Softmax classifier be used for text classification. We test our model on 8 benchmarks text classification datasets, and our model achieved a better or the same performance compare with the state-of-the-art methods.","authors":["Tengfei Liu","Shuangyuan Yu","Baomin Xu","Hongfeng Yin"],"meta":["October 2018Applied Intelligence 48(8)","DOI:10.1007/s10489-018-1176-4"],"references":["305334586_Learning_Distributed_Representations_of_Sentences_from_Unlabelled_Data","305334401_Hierarchical_Attention_Networks_for_Document_Classification","322590138_Supervised_Learning_of_Universal_Sentence_Representations_from_Natural_Language_Inference_Data","319770439_Efficient_Estimation_of_Word_Representations_in_Vector_Space","312451745_Document_modeling_with_gated_recurrent_neural_network_for_sentiment_classification_In_EMNLP","301446024_Document_Modeling_with_Gated_Recurrent_Neural_Network_for_Sentiment_Classification","301408986_Illinois-LH_A_Denotational_and_Distributional_Approach_to_Semantics","287034172_Fast_dropout_training","286794765_Dropout_A_Simple_Way_to_Prevent_Neural_Networks_from_Overfitting","284039049_Recursive_deep_models_for_semantic_compositionality_over_a_sentiment_treebank","281607724_Character-level_Convolutional_Networks_for_Text_Classification","279068396_Skip-Thought_Vectors","275280239_Self-Adaptive_Hierarchical_Sentence_Model","273067823_Improved_Semantic_Representations_From_Tree-Structured_Long_Short-Term_Memory_Networks","283531914_Semi-supervised_Sequence_Learning"]}