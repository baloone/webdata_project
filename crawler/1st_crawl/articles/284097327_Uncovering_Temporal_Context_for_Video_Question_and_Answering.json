{"id":"284097327_Uncovering_Temporal_Context_for_Video_Question_and_Answering","abstract":"In this work, we introduce Video Question Answering in temporal domain to\ninfer the past, describe the present and predict the future. We present an\nencoder-decoder approach using Recurrent Neural Networks to learn temporal\nstructures of videos and introduce a dual-channel ranking loss to answer\nmultiple-choice questions. We explore approaches for finer understanding of\nvideo content using question form of \"fill-in-the-blank\", and managed to\ncollect 109,895 video clips with duration over 1,000 hours from TACoS, MPII-MD,\nMEDTest 14 datasets, while the corresponding 390,744 questions are generated\nfrom annotations. Extensive experiments demonstrate that our approach\nsignificantly outperforms the compared baselines.","authors":["Linchao Zhu","Zhongwen Xu","Yi Yang","Alexander G. Hauptmann"],"meta":["September 2017International Journal of Computer Vision 124(2)","DOI:10.1007/s11263-017-1033-7","SourcearXiv"],"references":["319770438_Long-Term_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description","319770237_Revisiting_Visual_Question_Answering_Baselines","329976047_Grounding_Action_Descriptions_in_Videos","319770465_Sequence_to_Sequence_Learning_with_Neural_Networks","319770430_Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation","319770411_Torch7_A_Matlab-like_Environment_for_Machine_Learning","319770369_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality","319770353_Are_You_Talking_to_a_Machine_Dataset_and_Methods_for_Multilingual_Image_Question_Answering","319770345_Exploring_Models_and_Data_for_Image_Question_Answering","319770249_Deep_Visual-Semantic_Alignments_for_Generating_Image_Descriptions"]}