{"id":"315006561_Why_and_when_can_deep-but_not_shallow-networks_avoid_the_curse_of_dimensionality_A_review","abstract":"The paper reviews and extends an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. A class of deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Implications of a few key theorems are discussed, together with new results, open problems and conjectures.","authors":["Tomaso A. Poggio","Hrushikesh Mhaskar","Lorenzo Rosasco","Brando Miranda"],"meta":["December 2017International Journal of Automation and Computing 14(7553):1-17","DOI:10.1007/s11633-017-1054-2"],"references":["307473101_Why_Does_Deep_and_Cheap_Learning_Work_So_Well","301841654_Learning_Real_and_Boolean_Functions_When_Is_Deep_Better_Than_Shallow","281895600_On_the_Expressive_Power_of_Deep_Learning_A_Tensor_Analysis","332089096_Visual_Cortex_and_Deep_Networks_Learning_Invariant_Representations","319770106_On_the_Expressive_Power_of_Deep_Learning_A_Tensor_Analysis","309572399_Depth_Separation_in_ReLU_Networks_for_Approximating_Smooth_Non-Linear_Functions","287250773_The_Power_of_Depth_for_Feedforward_Neural_Networks","286764893_Human-level_concept_learning_through_probabilistic_program_induction","282403870_Representation_Benefits_of_Deep_Feedforward_Networks","281139622_Unsupervised_learning_of_invariant_representations"]}