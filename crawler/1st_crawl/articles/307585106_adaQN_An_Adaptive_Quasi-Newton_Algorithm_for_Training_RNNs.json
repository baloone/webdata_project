{"id":"307585106_adaQN_An_Adaptive_Quasi-Newton_Algorithm_for_Training_RNNs","abstract":"Recurrent Neural Networks, or RNNs, are powerful models that achieve exceptional performance on a plethora pattern recognition problems. However, the training of RNNs is a computationally difficult task owing to the well-known “vanishing/exploding” gradient problem. Algorithms proposed for training RNNs either exploit no (or limited) curvature information and have cheap per-iteration complexity, or attempt to gain significant curvature information at the cost of increased per-iteration cost. The former set includes diagonally-scaled first-order methods such as Adagrad and Adam, while the latter consists of second-order algorithms like Hessian-Free Newton and K-FAC. In this paper, we present adaQN, a stochastic quasi-Newton algorithm for training RNNs. Our approach retains a low per-iteration cost while allowing for non-diagonal scaling through a stochastic L-BFGS updating scheme. The method uses a novel L-BFGS scaling initialization scheme and is judicious in storing and retaining L-BFGS curvature pairs. We present numerical experiments on two language modeling tasks and show that adaQN is competitive with popular RNN training algorithms.","authors":["Nitish Shirish Keskar","Albert S. Berahas"],"meta":["September 2016","DOI:10.1007/978-3-319-46128-1_1","Conference: Joint European Conference on Machine Learning and Knowledge Discovery in Databases"],"references":["269116686_Stochastic_Quasi-Newton_Methods_for_Nonconvex_Stochastic_Optimization","265469570_Global_Convergence_of_Online_Limited_Memory_BFGS","262877889_Learning_Phrase_Representations_using_RNN_Encoder-Decoder_for_Statistical_Machine_Translation","259954350_RES_Regularized_stochastic_BFGS_algorithm","258816388_Revisiting_Natural_Gradient_for_Deep_Networks","243778956_The_Use_of_Recurrent_Neural_Networks_in_Continuous_Speech_Recognition","233730646_On_the_difficulty_of_training_Recurrent_Neural_Networks","224246503_Extensions_of_recurrent_neural_network_language_model","221620610_Offline_Arabic_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks","220320677_Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization","220319999_A_Stochastic_Quasi-Newton_Method_for_Online_Convex_Optimization","13853244_Long_Short-term_Memory","5583935_Learning_long-term_dependencies_with_gradient_descent_is_difficult","321620224_Numerical_Optimization","319770184_Speech_Recognition_With_Deep_Recurrent_Neural_Networks","304858104_Stochastic_Quasi-Newton_Methods_for_Nonconvex_Stochastic_Optimization","286271944_On_the_importance_of_initialization_and_momentum_in_deep_learning","285906939_Numerical_optimization","277959376_Visualizing_and_Understanding_Recurrent_Networks","274645044_A_Simple_Way_to_Initialize_Recurrent_Networks_of_Rectified_Linear_Units","273787871_Optimizing_Neural_Networks_with_Kronecker-factored_Approximate_Curvature","269935079_Adam_A_Method_for_Stochastic_Optimization","269116685_New_perspectives_on_the_natural_gradient_method","265469170_Recurrent_Neural_Network_Regularization","259933342_A_Stochastic_Quasi-Newton_Method_for_Large-Scale_Optimization","258818168_Speech_Recognition_with_Deep_Recurrent_Neural_Networks","257291640_A_method_of_solving_a_convex_programming_problem_with_convergence_rate_O1k2","221497515_Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization","221345102_Deep_learning_via_Hessian-free_optimization","221344668_Learning_Recurrent_Neural_Networks_with_Hessian-Free_Optimization","36419563_Supervised_Sequence_Labelling_with_Recurrent_Neural_Networks"]}