{"id":"331063377_Cooperative_Visual-Inertial_Sensor_Fusion_Fundamental_Equations_and_State_Determination_in_Closed-Form","abstract":"This paper investigates the visual and inertial sensor fusion problem in the cooperative case and provides new theoretical and basic results. Specifically, the case of two agents is investigated. Each agent is equipped with inertial sensors (accelerometer and gyroscope) and with a monocular camera. By using the monocular camera, each agent can observe the other agent. No additional camera observations (e.g., of external point features in the environment) are considered. First, the entire observable state is analytically derived. This state contains the relative position between the two agents (which includes the absolute scale), the relative velocity, the three Euler angles that express the rotation between the two local frames and all the accelerometer and gyroscope biases. Then, the basic equations that describe this system are analytically obtained. The last part of the paper describes the use of these equations to obtain a closed-form solution that provides the observable state in terms of the visual and inertial measurements provided in a short time interval. This last contribution is the extension of the results presented in Kaiser et al. (IEEE Robot Autom Lett 2(1):18–25, 2017), Martinelli (IEEE Trans Robot 28(1):44–60, 2012; Int J Comput Vis 106(2):138–152, 2014) to the cooperative case. The impact of the presence of the bias on the performance of this closed-form solution is also investigated and a simple and effective method to obtain the gyroscope bias is proposed. Extensive simulations clearly show that the proposed method is successful. It is worth noting that it is possible to automatically retrieve the absolute scale and simultaneously calibrate the gyroscopes not only without any prior knowledge (as in Kaiser et al. IEEE Robot Autom Lett 2(1):18–25, 2017), but also without external point features in the environment.","authors":["Agostino Martinelli","Alessandro Renzaglia","Alexander Oliva"],"meta":["March 2020Autonomous Robots 44(6)","DOI:10.1007/s10514-019-09841-8"],"references":["314578434_IMU_Preintegration_on_Manifold_for_Efficient_Visual-Inertial_Maximum-a-Posteriori_Estimation","312113696_Bearing_rigidity_theory_in_SE3","304411036_Bearing-Only_Formation_Control_Using_an_SE2_Rigidity_Theory","291953690_Simultaneous_State_Initialization_and_Gyroscope_Bias_Calibration_in_Visual_Inertial_Aided_Navigation","322413263_Cooperative_visual-inertial_sensor_fusion_Fundamental_equations","321814394_Inertial-based_scale_estimation_for_structure_from_motion_on_mobile_devices","318692459_High_altitude_monocular_visual-inertial_state_estimation_Initialization_and_sensor_fusion","318329465_Spline-Based_Initialization_of_Monocular_Visual-Inertial_State_Estimators_at_High_Altitude","308672870_Collaborative_stereo","304533059_Recursive_Decentralized_Collaborative_Localization_for_Sparsely_Communicating_Robots","290180954_IMU_Preintegration_on_Manifold_for_Efficient_Visual-Inertial_Maximum-a-Posteriori_Estimation","282983943_Rigid_components_identification_and_rigidity_control_in_bearing-only_localization_using_the_graph_cycle_basis","274195547_I_see_you_you_see_me_Cooperative_Localization_through_Bearing-Only_Mutually_Observing_Robots","286552535_Towards_consistent_visual-inertial_navigation","277145143_Cooperative_Localization_for_Mobile_Agents_A_Recursive_Decentralized_Algorithm_Based_on_Kalman-Filter_Decoupling"]}