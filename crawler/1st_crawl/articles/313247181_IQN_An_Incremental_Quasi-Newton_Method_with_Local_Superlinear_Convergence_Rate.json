{"id":"313247181_IQN_An_Incremental_Quasi-Newton_Method_with_Local_Superlinear_Convergence_Rate","abstract":"This paper studies the problem of minimizing a global objective function which can be written as the average of a set of $n$ smooth and strongly convex functions. Quasi-Newton methods, which build on the idea of approximating the Newton step using the first-order information of the objective function, are successful in reducing the computational complexity of Newton's method by avoiding the Hessian and its inverse computation at each iteration, while converging at a superlinear rate to the optimal argument. However, quasi-Newton methods are impractical for solving the finite sum minimization problem since they operate on the information of all $n$ functions at each iteration. This issue has been addressed by incremental quasi-Newton methods which use the information of a subset of functions at each iteration. Although incremental quasi-Newton methods are able to reduce the computational complexity of traditional quasi-Newton methods significantly, they fail to converge at a superlinear rate. In this paper, we propose the IQN method as the first incremental quasi-Newton method with a local superlinear convergence rate. In IQN, we compute and update the information of only a single function at each iteration and use the gradient information to approximate the Newton direction without a computationally expensive inversion. IQN differs from state-of-the-art incremental quasi-Newton methods in three criteria. First, the use of aggregated information of variables, gradients, and quasi-Newton Hessian approximations; second, the approximation of each individual function by its Taylor's expansion in which the linear and quadratic terms are evaluated with respect to the same iterate; and third, the use of a cyclic scheme to update the functions in lieu of a random selection routine. We use these fundamental properties of IQN to establish its local superlinear convergence rate.","authors":["Aryan Mokhtari","Mark Eisen","Alejandro Ribeiro"],"meta":["February 2017SIAM Journal on Optimization 28(2)","DOI:10.1137/17M1122943"],"references":["309593135_Surpassing_Gradient_Descent_Provably_A_Cyclic_Incremental_Method_with_Linear_Convergence_Rate","301879394_Stochastic_Block_BFGS_Squeezing_More_Curvature_out_of_Data","280912215_A_Linearly-Convergent_Stochastic_L-BFGS_Algorithm","277959432_On_the_Convergence_Rate_of_Incremental_Aggregated_Gradient_Algorithms","265469570_Global_Convergence_of_Online_Limited_Memory_BFGS","264864242_Convex_Optimization_for_Big_Data_Scalable_randomized_and_parallel_algorithms_for_big_data_analytics","263582586_SAGA_A_Fast_Incremental_Gradient_Method_With_Support_for_Non-Strongly_Convex_Composite_Objectives","313501588_Accelerating_stochastic_gradient_descent_using_predictive_variance_reduction","274319964_A_Variance_Reduced_Stochastic_Newton_Method","265325644_A_Characterization_of_Superlinear_Convergence_and_Its_Application_to_Quasi-Newton_Methods"]}