{"id":"326862335_A_neural_approach_for_inducing_multilingual_resources_and_natural_language_processing_tools_for_low-resource_languages","abstract":"This work focuses on the rapid development of linguistic annotation tools for low-resource languages (languages that have no labeled training data). We experiment with several cross-lingual annotation projection methods using recurrent neural networks (RNN) models. The distinctive feature of our approach is that our multilingual word representation requires only a parallel corpus between source and target languages. More precisely, our approach has the following characteristics: (a) it does not use word alignment information, (b) it does not assume any knowledge about target languages (one requirement is that the two languages (source and target) are not too syntactically divergent), which makes it applicable to a wide range of low-resource languages, (c) it provides authentic multilingual taggers (one tagger for N languages). We investigate both uni and bidirectional RNN models and propose a method to include external information (for instance, low-level information from part-of-speech tags) in the RNN to train higher level taggers (for instance, Super Sense taggers). We demonstrate the validity and genericity of our model by using parallel corpora (obtained by manual or automatic translation). Our experiments are conducted to induce cross-lingual part-of-speech and Super Sense taggers. We also use our approach in a weakly supervised context, and it shows an excellent potential for very low-resource settings (less than 1k training utterances).","authors":["Othman Zennaki","Nasredine Semmar","Laurent Besacier"],"meta":["August 2018Natural Language Engineering 25(1):1-25","DOI:10.1017/S1351324918000293"],"references":["308547358_Amelioration_de_la_traduction_automatique_d'un_corpus_annote","319770465_Sequence_to_Sequence_Learning_with_Neural_Networks","319770369_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality","319641605_Translating_Low-Resource_Languages_by_Vocabulary_Adaptation_from_Close_Counterparts","312375903_Neural_Probabilistic_Language_Models","311469848_Recurrent_neural_network_based_language_model","302469019_Supervised_Sequence_Labelling","301405420_Bilingual_Word_Representations_with_Monolingual_Quality_in_Mind","301404680_Simple_task-specific_bilingual_word_embeddings","278670441_Moses"]}