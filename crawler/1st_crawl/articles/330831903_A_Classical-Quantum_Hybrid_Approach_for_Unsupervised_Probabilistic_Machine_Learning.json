{"id":"330831903_A_Classical-Quantum_Hybrid_Approach_for_Unsupervised_Probabilistic_Machine_Learning","abstract":"For training unsupervised probabilistic machine learning models, matrix computation and sample generation are the two key steps. While GPUs excel at matrix computation, they use pseudo-random numbers to generate samples. Contrarily, Adiabatic Quantum Processors (AQP) use quantum mechanical systems to generate samples accurately and quickly, but are not suited for matrix computation. We present a Classical-Quantum Hybrid Approach for training unsupervised probabilistic machine learning models, leveraging GPUs for matrix computations and the D-Wave quantum sampling library for sample generation. We compare this approach to classical and quantum approaches across four performance metrics. Our results indicate that while the hybrid approach–which uses one AQP and one GPU–outperforms quantum and one of the classical approaches, it performs comparably to the GPU approach, and is outperformed by the CPU approach, which uses 56 high-end CPUs. Lastly, we compare sampling on AQP versus sampling library and show that AQP performs better.","authors":["Prasanna Date","Catherine D. Schuman","Robert Patton","Thomas E Potok"],"meta":["January 2020","DOI:10.1007/978-3-030-12385-7_9","Conference: Future of Information and Communication Conference"],"references":["323846369_Experimentally_Generated_Randomness_Certified_by_the_Impossibility_of_Superluminal_Signals","311610099_FireCaffe_Near-Linear_Acceleration_of_Deep_Neural_Network_Training_on_Compute_Clusters","311067084_Quantum_Machine_Learning","303657108_TensorFlow_A_system_for_large-scale_machine_learning","324704618_Quantum_supremacy_here_we_come","323373005_A_Future_with_Quantum_Machine_Learning","322001484_IBM_Q_-_Introduction_into_quantum_computing_with_live_demo","313110430_A_Study_of_Complex_Deep_Learning_Networks_on_High_Performance_Neuromorphic_and_Quantum_Computers","309349955_Investigation_of_full-sequence_training_of_deep_belief_networks_for_speech_recognition","304109482_Training_products_of_experts_by_minimizing_contrastive_divergence"]}