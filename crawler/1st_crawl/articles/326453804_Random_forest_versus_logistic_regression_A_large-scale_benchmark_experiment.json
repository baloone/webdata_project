{"id":"326453804_Random_forest_versus_logistic_regression_A_large-scale_benchmark_experiment","abstract":"Background and goal: \nThe Random Forest (RF) algorithm for regression and classification has considerably gained popularity since its introduction in 2001. Meanwhile, it has grown to a standard classification approach competing with logistic regression in many innovation-friendly scientific fields.\n\nResults: \nIn this context, we present a large scale benchmarking experiment based on 243 real datasets comparing the prediction performance of the original version of RF with default parameters and LR as binary classification tools. Most importantly, the design of our benchmark experiment is inspired from clinical trial methodology, thus avoiding common pitfalls and major sources of biases.\n\nConclusion: \nRF performed better than LR according to the considered accuracy measured in approximately 69% of the datasets. The mean difference between RF and LR was 0.029 (95%-CI =[0.022,0.038]) for the accuracy, 0.041 (95%-CI =[0.031,0.053]) for the Area Under the Curve, and - 0.027 (95%-CI =[-0.034,-0.021]) for the Brier score, all measures thus suggesting a significantly better performance of RF. As a side-result of our benchmarking experiment, we observed that the results were noticeably dependent on the inclusion criteria used to select the example datasets, thus emphasizing the importance of clear statements regarding this dataset selection process. We also stress that neutral studies similar to ours, based on a high number of datasets and carefully designed, will be necessary in the future to evaluate further variants, implementations or parameters of random forests which may yield improved accuracy compared to the original version with default values.","authors":["Raphael Couronn√©","Philipp Probst","Anne-Laure Boulesteix"],"meta":["July 2018BMC Bioinformatics 19(1)","DOI:10.1186/s12859-018-2264-5"],"references":["324435894_Hyperparameters_and_Tuning_Strategies_for_Random_Forest","319624989_Towards_evidence-based_computational_statistics_Lessons_from_clinical_research_on_the_role_and_design_of_real-data_benchmark_studies","316985543_To_tune_or_not_to_tune_the_number_of_trees_in_random_forest","316817137_IPF-LASSO_Integrative_L_1_-Penalized_Regression_with_Penalty_Factors_for_Prediction_Based_on_Multi-Omics_Data","314255079_batchtools_Tools_for_R_to_work_on_batch_systems","307610832_The_parameter_sensitivity_of_random_forests","283089190_Comparing_Random_Forest_with_Logistic_Regression_for_Predicting_Class-Imbalanced_Civil_War_Onset_Data","326783095_Making_complex_prediction_rules_applicable_for_readers_Current_practice_in_random_forest_literature_and_recommendations","312552836_OpenML_Exploring_Machine_Learning_Better_Together","294088913_Selecting_a_classification_function_for_class_prediction_with_gene_expression_data","275358525_Ten_Simple_Rules_for_Reducing_Overoptimistic_Reporting_in_Methodological_Computational_Research","281313699_Hothorn_T_Bias_in_Random_Forest_Variable_Importance_Measures_Illustrations","281172453_Subsampling_versus_bootstrap_in_resampling-based_model_selection_for_multivariable_regression","280687718_Greedy_Function_Approximation_A_Gradient_Boosting_Machine","276308702_A_Statistical_Framework_for_Hypothesis_Testing_in_Real_Data_Comparison_Studies"]}