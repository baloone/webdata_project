{"id":"262347236_Conditional_Likelihood_Maximisation_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection","abstract":"We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation. This is in response to the question: \"what are the implicit statistical assumptions of feature selection criteria based on mutual information?\". To answer this, we adopt a different strategy than is usual in the feature selection literature--instead of trying to define a criterion, we derive one, directly from a clearly specified objective function: the conditional likelihood of the training labels. While many hand-designed heuristic criteria try to optimize a definition of feature 'relevancy' and 'redundancy', our approach leads to a probabilistic framework which naturally incorporates these concepts. As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem. The primary contribution is to show that common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood. A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms. Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and flexibility with small data samples.","authors":["Gavin Brown","Adam Pocock","Ming-Jie Zhao","Mikel Luj√°n"],"meta":["February 2012Journal of Machine Learning Research 13(1):27-66"],"references":["263441032_Conditional_Mutual_Information-Based_Feature_Selection_Analyzing_for_Synergy_and_Redundancy","255006424_A_Powerful_Feature_Selection_approach_based_on_Mutual_Information","224355092_Gait_Feature_Subset_Selection_by_Mutual_Information","224124950_On_the_Feature_Selection_Criterion_Based_on_an_Approximation_of_Multidimensional_Mutual_Information","223713209_Wrappers_for_Feature_Subset_Selection","221619995_Feature_selection_for_SVMs","221438027_Algorithms_for_Large_Scale_Markov_Blanket_Discovery","221305296_Conditional_Infomax_Learning_An_Integrated_Framework_for_Feature_Extraction_and_Fusion","221173759_A_stability_index_for_feature_selection","220867410_On_the_Use_of_Variable_Complementarity_for_Feature_Selection_in_Cancer_Classification","220702040_On_the_Performance_Assessment_and_Comparison_of_Stochastic_Multiobjective_Optimizers","220699134_Stable_and_Accurate_Feature_Selection","220321053_Eficient_Feature_Selection_Via_Analysis_of_Relevance_and_Redundancy","220319839_Maximum_Likelihood_in_Cost-Sensitive_Learning_Model_Specification_Approximations_and_Upper_Bounds","220284040_Stability_of_feature_selection_algorithms_A_study_on_high-dimensional_spaces","33550082_Machine_Learning_Based_on_Attribute_Interactions","5608615_Choi_C_Input_Feature_Selection_for_Classification_Problems_IEEE_Trans_on_Neural_Networks_13_143-159","4298103_Gait_Feature_Subset_Selection_by_Mutual_Information","4116665_AMIFS_Adaptive_feature_selection_by_using_mutual_information","4038433_Object_recognition_with_informative_features_and_linear_classification","3481589_Information-Theoretic_Feature_Selection_in_Microarray_Data_Using_Variable_Complementarity","3301850_Using_Mutual_Information_for_Selecting_Features_in_Supervised_Neural_Net_Learning","2478803_Towards_Principled_Feature_Selection_Relevancy_Filters_and_Wrappers","2460722_Data_Visualization_and_Feature_Selection_New_Algorithms_for_Nongaussian_Data","2458309_Feature_Selection_and_Feature_Extraction_for_Text_Categorization","324634019_Transmission_of_Information_A_Statistical_Theory_of_Communications","269033767_Data_extraction_as_text_categorization","248594269_Discriminative_models_not_discriminative_training","239501760_Design_of_experiments_for_the_NIPS_2003_variable_selection_benchmark","235418978_A_Mathematical_Theory_of_Communication","233871606_Large-Sample_Learning_of_Bayesian_Networks_is_NP-Hard","230875979_Transmission_of_Information-A_Statistical_Theory_of_Communications","227992567_Elements_of_Information_Theory","224773133_Elements_of_information_theory_2nd_ed","220499803_Estimation_of_Entropy_and_Mutual_Information","220411114_A_Mathematical_Theory_of_Communication","220320537_Fast_Binary_Feature_Selection_with_Conditional_Mutual_Information","220320196_Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets","220320024_A_New_Perspective_for_Information_Theoretic_Feature_Selection","220271965_Stable_feature_selection_via_dense_feature_groups","220049129_Learning_Bayesian_Network_Classifiers_by_Maximizing_Conditional_Likelihood","13988269_The_Mathematical_Theory_of_Communication","7641976_Feature_Selection_Based_On_Mutual_Information_Criteria_of_Max-DependencyMax-Relevance_and_Min-Redundancy","3081680_Probability_of_Error_Equivocation_and_the_Chernoff_Bound","2454353_Toward_Optimal_Feature_Selection"]}