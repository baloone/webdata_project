{"id":"326502265_Content_to_Node_Self-Translation_Network_Embedding","abstract":"This paper concerns the problem of network embedding (NE), whose aim is to learn low-dimensional representations for nodes in networks. Such dense vector representations offer great promises for many network analysis problems. However, existing NE approaches are still faced with challenges posed by the characteristics of complex networks in real-world applications. First, for many real-world networks associated with rich content information, previous NE methods tend to learn separated content and structure representations for each node, which requires a post-processing of combination. The empirical and simple combination strategies often make the final vector suboptimal. Second, the existing NE methods preserve the structure information by considering short and fixed neighborhood scope, such as the first- and/or the second-order proximities. However, it is hard to decide the scope of the neighborhood when facing a complex problem. To this end, we propose a novel sequence-to-sequence model based NE framework which is referred to as Self-Translation Network Embedding (STNE) model. With the sequences generated by random walks on a network, STNE learns the mapping that translates each sequence itself from the content sequence to the node sequence. On the one hand, the bi-directional LSTM encoder of STNE fuses the content and structure information seamlessly from the raw input. On the other hand, high-order proximity can be flexibly learned with the memories of LSTM to capture long-range structural information. By such self-translation from content to node, the learned hidden representations can be adopted as node embeddings. Extensive experimental results based on three real-world datasets demonstrate that the proposed STNE outperforms the state-of-the-art NE approaches. To facilitate reproduction and further study, we provide Internet access to the code and datasets\\footnotehttp://dm.nankai.edu.cn/code/STNE.rar.","authors":["Jie Liu","Zhicheng He","Lai Wei","Yalou Huang"],"meta":["July 2018","DOI:10.1145/3219819.3219988","Conference: the 24th ACM SIGKDD International Conference"],"references":["318737345_CANE_Context-Aware_Network_Embedding_for_Relation_Modeling","313628491_Community_Preserving_Network_Embedding","306093553_End-to-end_Sequence_Labeling_via_Bi-directional_LSTM-CNNs-CRF","322969316_Network_Embedding_as_Matrix_Factorization_Unifying_DeepWalk_LINE_PTE_and_node2vec","319770465_Sequence_to_Sequence_Learning_with_Neural_Networks","319770439_Efficient_Estimation_of_Word_Representations_in_Vector_Space","310823973_Incorporate_Group_Information_to_Enhance_Network_Embedding","308981018_A_General_Framework_for_Content-enhanced_Network_Representation_Learning","308152498_An_overview_of_gradient_descent_optimization_algorithms","306093723_Language_to_Logical_Form_with_Neural_Attention"]}