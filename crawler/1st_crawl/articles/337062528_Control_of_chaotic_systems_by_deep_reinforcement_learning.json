{"id":"337062528_Control_of_chaotic_systems_by_deep_reinforcement_learning","abstract":"Deep reinforcement learning (DRL) is applied to control a nonlinear, chaotic system governed by the one-dimensional Kuramotoâ€“Sivashinsky (KS) equation. DRL uses reinforcement learning principles for the determination of optimal control solutions and deep neural networks for approximating the value function and the control policy. Recent applications have shown that DRL may achieve superhuman performance in complex cognitive tasks. In this work, we show that using restricted localized actuation, partial knowledge of the state based on limited sensor measurements and model-free DRL controllers, it is possible to stabilize the dynamics of the KS system around its unstable fixed solutions, here considered as target states. The robustness of the controllers is tested by considering several trajectories in the phase space emanating from different initial conditions; we show that DRL is always capable of driving and stabilizing the dynamics around target states. The possibility of controlling the KS system in the chaotic regime by using a DRL strategy solely relying on local measurements suggests the extension of the application of RL methods to the control of more complex systems such as drag reduction in bluff-body wakes or the enhancement/diminution of turbulent mixing.","authors":["Michele Alessandro Bucci","Onofrio Semeraro","Alexandre Allauzen","Guillaume Wisniewski"],"meta":["November 2019Proceedings of The Royal Society A Mathematical Physical and Engineering Sciences 475(2231):20190351","DOI:10.1098/rspa.2019.0351"],"references":["331240036_Artificial_neural_networks_trained_through_deep_reinforcement_learning_discover_control_strategies_for_active_flow_control","323026949_Efficient_collective_swimming_by_harnessing_vortices_through_deep_reinforcement_learning","320473480_Mastering_the_game_of_Go_without_human_knowledge","318169652_A_continuous_reinforcement_learning_strategy_for_closed-loop_control_in_fluid_dynamics","310329240_High-Dimensional_Stochastic_Optimal_Control_using_Continuous_Tensor_Decompositions","302893448_Edge_states_as_mediators_of_bypass_transition_in_boundary-layer_flows","301847678_Asynchronous_Methods_for_Deep_Reinforcement_Learning","301573483_Linear_Closed-Loop_Control_of_Fluid_Instabilities_and_Noise-Induced_Perturbations_A_Review_of_Approaches_and_Tools","301200123_A_statistical_learning_strategy_for_closed-loop_control_of_fluid_flows","284219262_Prioritized_Experience_Replay","282462874_Closed-Loop_Turbulence_Control_Progress_and_Challenges","281670459_Continuous_control_with_deep_reinforcement_learning","281275027_Efficient_High-Dimensional_Stochastic_Optimal_Motion_Control_using_Tensor-Train_Decomposition","265538531_Principles_of_Robot_Motion_Theory_Algorithms_and_Implementation_ERRATA_1","262071453_Closed-loop_separation_control_using_machine_learning","260107945_Adaptive_and_Model-Based_Control_Theory_Applied_to_Convectively_Unstable_Flows","258933937_Nonlinear_control_of_unsteady_finite-amplitude_perturbations_in_the_Blasius_boundary-layer_flow","255606462_A_Semi-Implicit_Runge-Kutta_Time-Difference_Scheme_for_the_Two-Dimensional_Shallow-Water_Equations","245426240_Adaptive_Closed-Loop_Separation_Control_on_a_High-Lift_Configuration_Using_Extremum_Seeking","231955051_DNS-based_predictive_control_of_turbulence_An_optimal_benchmark_for_feedback_algorithms","229058308_Multivariable_Feedback_Control_Analysis_and_Design","344486310_Turbulence_Coherent_Structures_Dynamical_Systems_and_Symmetry","333659270_A_Bounded_Actor-Critic_Reinforcement_Learning_Algorithm_Applied_to_Airline_Revenue_Management","329662263_A_Tour_of_Reinforcement_Learning_The_View_from_Continuous_Control","322276906_Adaptive_Control_Processes_A_Guided_Tour","319770330_Prioritized_Experience_Replay","317070551_Nonlinear_optimal_control_of_bypass_transition_in_a_boundary_layer_flow","313135362_Q-learning","308152498_An_overview_of_gradient_descent_optimization_algorithms","280752017_Deterministic_Policy_Gradient_Algorithms","279366255_Subspace_Identification_for_Linear_Systems","278924760_Implicit-explicit_Runge-Kutta_methods_for_time-dependent_partial_differential_equations","272837232_Human-level_control_through_deep_reinforcement_learning","265500547_Dynamical_Systems_Approach_to_Turbulence","263031474_Adjoint_Equations_in_Stability_Analysis","258735890_Turbulence_Coherent_Structures_Dynamical_Systems_and_Symmetry","241251847_Methods_for_the_solution_of_very_large_flow-control_problems_that_bypass_open-loop_model_reduction","239044275_DNS-based_predictive_control_of_turbulence_an_optimal_target_for_feedback_algorithms","235189235_A_Markovian_Decision_Process","234151179_A_Linear_Systems_Approach_to_Flow_Control","230873432_Optimal_Control_And_Estimation","230872615_Dynamic_Programming_Optimal_Control","230641322_Optimal_Control","229287990_The_steady_states_of_the_Kuramoto-Sivashinsky_equation","224773224_Principles_of_Robot_Motion_Theory_Algorithms_and_Implementations","223808102_Real-time_feedback_control_of_flow-induced_cavity_tones-Part_2_Adaptive_control","223164130_Dynamic_programming_and_stochastic_control_processes","222771877_Real-time_feedback_control_of_flow-induced_cavity_tones-Part_1_Fixed-gain_control","222596575_Feedback_control_of_the_Kuramoto-Sivashinsky_equation","221996515_System_Identification_Theory_For_The_User","220344150_Technical_Note_Q-Learning","45861653_Reduced_order_models_for_control_of_fluids_using_the_Eigensystem_Realization_Algorithm","44352605_Optimal_control_theory_an_introduction_Donald_E_Kirk","5596000_Reinforcement_Learning_An_Introduction","4707836_The_Minimal_Flow_Unit_in_Near-Wall_Turbulence"]}