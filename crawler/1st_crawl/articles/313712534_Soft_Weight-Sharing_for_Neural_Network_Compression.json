{"id":"313712534_Soft_Weight-Sharing_for_Neural_Network_Compression","abstract":"The success of deep learning in numerous application domains created the de- sire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression. Recent work by Han et al. (2015a) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates. In this paper, we show that competitive compression rates can be achieved by using a version of soft weight-sharing (Nowlan & Hinton, 1992). Our method achieves both quantization and pruning in one simple (re-)training procedure. This point of view also exposes the relation between compression and the minimum description length (MDL) principle.","authors":["Karen Ullrich","Edward Meeds","Max Welling"],"meta":["February 2017"],"references":["312550572_Variational_Dropout_Sparsifies_Deep_Neural_Networks","308844880_Sparse_Convolutional_Neural_Networks","306227124_Dynamic_Network_Surgery_for_Efficient_DNNs","319770414_Identity_Mappings_in_Deep_Residual_Networks","319770334_Deep_Compression_Compressing_Deep_Neural_Networks_with_Pruning_Trained_Quantization_and_Huffman_Coding","319769906_Training_deep_neural_networks_with_low_precision_multiplications","317194083_Wide_Residual_Networks","308832832_Accelerating_Deep_Convolutional_Networks_using_low-precision_and_sparsity","306187229_Learning_Structured_Sparsity_in_Deep_Neural_Networks","303840259_Deep_neural_networks_are_robust_to_weight_binarization_and_other_non-linear_distortions"]}