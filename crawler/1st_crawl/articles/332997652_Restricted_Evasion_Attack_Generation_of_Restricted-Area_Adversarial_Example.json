{"id":"332997652_Restricted_Evasion_Attack_Generation_of_Restricted-Area_Adversarial_Example","abstract":"Deep neural networks (DNNs) show superior performance in image and speech recognition. However, adversarial examples created by adding a little noise to an original sample can lead to misclassification by a DNN. Conventional studies on adversarial examples have focused on ways of causing misclassification by a DNN by modulating the entire image. However, in some cases, a restricted adversarial example may be required in which only certain parts of the image are modified rather than the entire image and that result in misclassification by the DNN. For example, when the placement of a road sign has already been completed, an attack may be required that will change only a specific part of the sign, such as by placing a sticker on it, to cause misidentification of the entire image. As another example, an attack may be required that causes a DNN to misinterpret images according to a minimal modulation of the outside border of the image. In this paper, we propose a new restricted adversarial example that modifies only a restricted area to cause misclassification by a DNN while minimizing distortion from the original sample. It can also select the size of the restricted area.We used the CIFAR10 and ImageNet datasets to evaluate the performance.We measured the attack success rate and distortion of the restricted adversarial example while adjusting the size, shape, and position of the restricted area. The results show that the proposed scheme generates restricted adversarial examples with a 100% attack success rate in a restricted area of the whole image (approximately 14% for CIFAR10 and 1.07% for ImageNet) while minimizing the distortion distance.","authors":["Hyun Kwon","Hyunsoo Yoon","Daeseon Choi"],"meta":["May 2019IEEE Access PP(99):1-1","DOI:10.1109/ACCESS.2019.2915971"],"references":["327128395_Multi-Targeted_Adversarial_Example_in_Evasion_Attack_on_Deep_Neural_Network","320609325_One_Pixel_Attack_for_Fooling_Deep_Neural_Networks","332790210_APE-GAN_Adversarial_Perturbation_Elimination_with_GAN","326967284_Friend-safe_Evasion_Attack_an_adversarial_example_that_is_correctly_recognized_by_a_friendly_classifier","323248062_Feature_Squeezing_Detecting_Adversarial_Examples_in_Deep_Neural_Networks","322113705_Adversarial_Patch","320671312_MagNet_A_Two-Pronged_Defense_against_Adversarial_Examples","319770378_Explaining_and_harnessing_adversarial_examples","319770355_Generative_Adversarial_Nets","319770131_DeepFool_a_simple_and_accurate_method_to_fool_deep_neural_networks"]}