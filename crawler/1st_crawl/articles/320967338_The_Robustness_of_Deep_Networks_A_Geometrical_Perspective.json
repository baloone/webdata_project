{"id":"320967338_The_Robustness_of_Deep_Networks_A_Geometrical_Perspective","abstract":"Deep neural networks have recently shown impressive classification performance on a diverse set of visual tasks. When deployed in real-world (noise-prone) environments, it is equally important that these classifiers satisfy robustness guarantees: small perturbations applied to the samples should not yield significant loss to the performance of the predictor. The goal of this article is to discuss the robustness of deep networks to a diverse set of perturbations that may affect the samples in practice, including adversarial perturbations, random noise, and geometric transformations. This article further discusses the recent works that build on the robustness analysis to provide geometric insights on the classifier's decision surface, which help in developing a better understanding of deep networks. Finally, we present recent solutions that attempt to increase the robustness of deep networks. We hope this review article will contribute to shed ding light on the open research challenges in the robustness of deep networks and stir interest in the analysis of their fundamental properties.","authors":["Alhussein Fawzi","Seyed-Mohsen Moosavi-Dezfooli","Pascal Frossard"],"meta":["November 2017IEEE Signal Processing Magazine 34(6):50-62","DOI:10.1109/MSP.2017.2740965"],"references":["313857891_A_Survey_on_Deep_Learning_in_Medical_Image_Analysis","313713721_On_Detecting_Adversarial_Perturbations","309797568_Delving_into_Transferable_Adversarial_Examples_and_Black-box_Attacks","309460742_Universal_adversarial_perturbations","309448167_Accessorize_to_a_Crime_Real_and_Stealthy_Attacks_on_State-of-the-Art_Face_Recognition","307560165_Deep_neural_networks_are_easily_fooled_High_confidence_predictions_for_unrecognizable_images","307307101_A_Boundary_Tilting_Persepective_on_the_Phenomenon_of_Adversarial_Examples","306304648_Distillation_as_a_Defense_to_Adversarial_Perturbations_Against_Deep_Neural_Networks","305196650_Going_deeper_with_convolutions","304018355_Exponential_expressivity_in_deep_neural_networks_through_transient_chaos","302376617_Adversarial_Diversity_and_Hard_Positive_Generation","301875216_Practical_Black-Box_Attacks_against_Deep_Learning_Systems_using_Adversarial_Examples","284219659_Understanding_Adversarial_Training_Increasing_Local_Stability_of_Neural_Nets_through_Robust_Optimization","284097112_Distillation_as_a_Defense_to_Adversarial_Perturbations_against_Deep_Neural_Networks","284096769_Adversarial_Manipulation_of_Deep_Representations","272195158_Analysis_of_classifiers'_robustness_to_adversarial_perturbations","269935591_Explaining_and_Harnessing_Adversarial_Examples","269722671_Towards_Deep_Neural_Network_Architectures_Robust_to_Adversarial_Examples","269280482_Deep_Neural_Networks_Are_Easily_Fooled_High_Confidence_Predictions_for_Unrecognizable_Images","265295439_ImageNet_Large_Scale_Visual_Recognition_Challenge","264979485_Caffe_Convolutional_Architecture_for_Fast_Feature_Embedding","259440613_Intriguing_properties_of_neural_networks","221345414_An_empirical_evaluation_of_deep_architectures_on_problems_with_many_factors_of_variation","320968636_Universal_Adversarial_Perturbations","319770183_Imagenet_classification_with_deep_convolutional_neural_networks","319770131_DeepFool_a_simple_and_accurate_method_to_fool_deep_neural_networks","319769909_Distilling_the_Knowledge_in_a_Neural_Network","317919653_Towards_Evaluating_the_Robustness_of_Neural_Networks","317205291_Classification_regions_of_deep_neural_networks","317190993_Measuring_the_effect_of_nuisance_variables_on_classifiers","317187337_Analysis_of_universal_adversarial_perturbations","316598963_Parseval_Networks_Improving_Robustness_to_Adversarial_Examples","315764926_SafetyNet_Detecting_and_Rejecting_Adversarial_Examples_Robustly","315682836_Adversarial_Transformation_Networks_Learning_to_Generate_Adversarial_Examples","315666595_Biologically_inspired_protection_of_deep_networks_from_adversarial_attacks","314153095_Detecting_Adversarial_Samples_from_Artifacts","312881053_Adaptive_data_augmentation_for_image_classification","311757216_Adversarial_Diversity_and_Hard_Positive_Generation","311610675_DeepFool_A_Simple_and_Accurate_Method_to_Fool_Deep_Neural_Networks","311609371_An_Empirical_Evaluation_of_Current_Convolutional_Architectures'_Ability_to_Manage_Nuisance_Location_and_Scale_Variability","311299697_Deep_Variational_Information_Bottleneck","307536105_Robustness_of_classifiers_from_adversarial_to_random_noise","307516149_Adaptive_data_augmentation_for_image_classification","306226844_Towards_Evaluating_the_Robustness_of_Neural_Networks","305388839_Defensive_Distillation_is_Not_Robust_to_Adversarial_Examples","280329479_Manitest_Are_classifiers_really_invariant","277895449_Spatial_Transformer_Networks","273387909_Distilling_the_Knowledge_in_a_Neural_Network","256600830_Evasion_Attacks_against_Machine_Learning_at_Test_Time","232975285_Robust_support_vector_machines_for_classification_and_computational_issues","2834016_A_Robust_Minimax_Approach_to_Classification","4069257_Robust_classification_of_noisy_data_using_second_order_cone_programming_approach","1741729_Robustness_and_Regularization_of_Support_Vector_Machines"]}