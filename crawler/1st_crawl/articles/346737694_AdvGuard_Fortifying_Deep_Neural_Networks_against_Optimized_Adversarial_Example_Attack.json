{"id":"346737694_AdvGuard_Fortifying_Deep_Neural_Networks_against_Optimized_Adversarial_Example_Attack","abstract":"Deep neural networks (DNNs) provide excellent performance in image recognition, speech recognition, video recognition, and pattern analysis. However, they are vulnerable to adversarial example attacks. An adversarial example, which is input to which a little bit of noise has been strategically added, appears normal to the human eye but will be misrecognized by the DNN. In this paper, we propose AdvGuard, a method for resisting adversarial example attacks. This defense method prevents the generation of adversarial examples by constructing a robust DNN that provides random confidence values. This method does not require training of adversarial examples, use of other processing modules, or the ability to perform input data filtering. In addition, a DNN constructed using the proposed scheme can defend against adversarial examples while maintaining its accuracy on the original samples. In the experimental evaluation, MNIST and CIFAR10 were used as datasets, and TensorFlow was used as a machine learning library. The results show that a DNN constructed using the proposed method can correctly classify adversarial examples with 100% and 99.5% accuracy on MNIST and CIFAR10, respectively.","authors":["Hyun Kwon","Jun Lee"],"meta":["December 2020IEEE Access","DOI:10.1109/ACCESS.2020.3042839"],"references":["344054538_Acoustic-Decoy_Detection_of_Adversarial_Examples_through_Audio_Modification_on_Speech_Recognition_System","338508237_Decoupling_Direction_and_Norm_for_Efficient_Gradient-Based_L2_Adversarial_Attacks_and_Defenses","321860123_Adversarial_Examples_Attacks_and_Defenses_for_Deep_Learning","341097356_Understanding_Adversarial_Attacks_on_Deep_Learning_Based_Medical_Image_Analysis_Systems","338940087_Generation_of_Low_Distortion_Adversarial_Attacks_via_Convex_Programming","334994486_Untargeted_Adversarial_Attack_via_Expanding_the_Semantic_Gap","334070470_Selective_Audio_Adversarial_Example_in_Evasion_Attack_on_Speech_Recognition_System","330494219_ADMM_attack_an_enhanced_adversarial_attack_for_deep_neural_networks_with_undetectable_distortions","326854294_Audio_Adversarial_Examples_Targeted_Attacks_on_Speech-to-Text","323248062_Feature_Squeezing_Detecting_Adversarial_Examples_in_Deep_Neural_Networks"]}