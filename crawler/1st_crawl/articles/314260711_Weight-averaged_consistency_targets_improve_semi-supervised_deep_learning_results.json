{"id":"314260711_Weight-averaged_consistency_targets_improve_semi-supervised_deep_learning_results","abstract":"The recently proposed temporal ensembling has achieved state-of-the-art results in several semi-supervised learning benchmarks. It maintains an exponential moving average of label predictions on each training example, and penalizes predictions that are inconsistent with this target. However, because the targets change only once per epoch, temporal ensembling becomes unwieldy when using large datasets. To overcome this problem, we propose a method that averages model weights instead of label predictions. As an additional benefit, the method improves test accuracy and enables training with fewer labels than earlier methods. We report state-of-the-art results on semi-supervised SVHN, reducing the error rate from 5.12% to 4.41% with 500 labels, and achieving 5.39% error rate with 250 labels. By using extra unlabeled data, we reduce the error rate to 2.76% on 500-label SVHN.","authors":["Antti Tarvainen","Harri Valpola"],"meta":["March 2017"],"references":["306304648_Distillation_as_a_Defense_to_Adversarial_Perturbations_Against_Deep_Neural_Networks","305881127_Improved_Techniques_for_Training_GANs","303993074_Regularization_With_Stochastic_Transformations_and_Perturbations_for_Deep_Semi-Supervised_Learning","301879329_Deep_Networks_with_Stochastic_Depth","279968088_Semi-Supervised_Learning_with_Ladder_Network","279632719_Distributional_Smoothing_with_Virtual_Adversarial_Training","269935591_Explaining_and_Harnessing_Adversarial_Examples","263012109_Generative_Adversarial_Networks","236736831_Acceleration_of_Stochastic_Approximation_by_Averaging","319770378_Explaining_and_harnessing_adversarial_examples","319770355_Generative_Adversarial_Nets","319770264_Regularization_of_Neural_Networks_using_DropConnect","319770134_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models","308964680_Temporal_Ensembling_for_Semi-Supervised_Learning","303409493_Swapout_Learning_an_ensemble_of_deep_architectures","301872762_Weight_Normalization_A_Simple_Reparameterization_to_Accelerate_Training_of_Deep_Neural_Networks","286794765_Dropout_A_Simple_Way_to_Prevent_Neural_Networks_from_Overfitting","277959098_Dropout_as_a_Bayesian_Approximation_Representing_Model_Uncertainty_in_Deep_Learning","273387909_Distilling_the_Knowledge_in_a_Neural_Network","272194743_Batch_Normalization_Accelerating_Deep_Network_Training_by_Reducing_Internal_Covariate_Shift","269935079_Adam_A_Method_for_Stochastic_Optimization","266031774_Reading_Digits_in_Natural_Images_with_Unsupervised_Feature_Learning","265748773_Learning_Multiple_Layers_of_Features_from_Tiny_Images","262991675_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models","259400035_Auto-Encoding_Variational_Bayes","2475534_Learning_from_Labeled_and_Unlabeled_Data_with_Label_Propagation"]}