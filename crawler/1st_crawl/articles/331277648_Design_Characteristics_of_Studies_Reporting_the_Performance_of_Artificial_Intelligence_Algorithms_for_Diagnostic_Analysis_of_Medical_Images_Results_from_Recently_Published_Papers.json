{"id":"331277648_Design_Characteristics_of_Studies_Reporting_the_Performance_of_Artificial_Intelligence_Algorithms_for_Diagnostic_Analysis_of_Medical_Images_Results_from_Recently_Published_Papers","abstract":"Objective\nTo evaluate the design characteristics of studies that evaluated the performance of artificial intelligence (AI) algorithms for the diagnostic analysis of medical images.\n\nMaterials and Methods\nPubMed MEDLINE and Embase databases were searched to identify original research articles published between January 1, 2018 and August 17, 2018 that investigated the performance of AI algorithms that analyze medical images to provide diagnostic decisions. Eligible articles were evaluated to determine 1) whether the study used external validation rather than internal validation, and in case of external validation, whether the data for validation were collected, 2) with diagnostic cohort design instead of diagnostic case-control design, 3) from multiple institutions, and 4) in a prospective manner. These are fundamental methodologic features recommended for clinical validation of AI performance in real-world practice. The studies that fulfilled the above criteria were identified. We classified the publishing journals into medical vs. non-medical journal groups. Then, the results were compared between medical and non-medical journals.\n\nResults\nOf 516 eligible published studies, only 6% (31 studies) performed external validation. None of the 31 studies adopted all three design features: diagnostic cohort design, the inclusion of multiple institutions, and prospective data collection for external validation. No significant difference was found between medical and non-medical journals.\n\nConclusion\nNearly all of the studies published in the study period that evaluated the performance of AI algorithms for diagnostic analysis of medical images were designed as proof-of-concept technical feasibility studies and did not have the design features that are recommended for robust validation of the real-world clinical performance of AI algorithms.","authors":["Dong Wook Kim","Hye Young Jang","Kyung Won Kim","Youngbin Shin"],"meta":["March 2019Korean journal of radiology: official journal of the Korean Radiological Society 20(3):405","DOI:10.3348/kjr.2019.0025"],"references":["329777293_Principles_for_evaluating_the_clinical_implementation_of_novel_digital_healthcare_devices","328775195_Clinical_Decision_Support_in_the_Era_of_Artificial_Intelligence","328773481_Variable_generalization_performance_of_a_deep_learning_model_to_detect_pneumonia_in_chest_radiographs_A_cross-sectional_study","328388545_Age_of_Data_in_Contemporary_Research_Articles_Published_in_Representative_General_Radiology_Journals","327958135_Evaluating_Artificial_Intelligence_Applications_in_Clinical_Settings","329870635_Diagnosis_of_thyroid_cancer_using_deep_convolutional_neural_network_models_applied_to_sonographic_images_a_retrospective_multicohort_diagnostic_study","329729368_Artificial_Intelligence_for_Medical_Image_Analysis_A_Guide_for_Authors_and_Reviewers","329550462_Questions_for_Artificial_Intelligence_in_Health_Care","329532198_What_is_an_appropriate_level_of_evidence_for_a_digital_health_intervention","329410885_Diagnostic_Case-Control_versus_Diagnostic_Cohort_Studies_for_Clinical_Validation_of_Artificial_Intelligence_Algorithm_Performance"]}