{"id":"330771284_Item_response_theory_in_AI_Analysing_machine_learning_classifiers_at_the_instance_level","abstract":"AI systems are usually evaluated on a range of problem instances and compared to other AI systems that use different strategies. These instances are rarely independent. Machine learning, and supervised learning in particular, is a very good example of this. Given a machine learning model, its behaviour for a single instance cannot be understood in isolation but rather in relation to the rest of the data distribution or dataset. In a dual way, the results of one machine learning model for an instance can be analysed in comparison to other models. While this analysis is relative to a population or distribution of models, it can give much more insight than an isolated analysis. Item response theory (IRT) combines this duality between items and respondents to extract latent variables of the items (such as discrimination or difficulty) and the respondents (such as ability). IRT can be adapted to the analysis of machine learning experiments (and by extension to any other artificial intelligence experiments). In this paper, we see that IRT suits classification tasks perfectly, where instances correspond to items and classifiers correspond to respondents. We perform a series of experiments with a range of datasets and classification methods to fully understand what the IRT parameters such as discrimination, difficulty and guessing mean for classification instances (and their relation to instance hardness measures) and how the estimated classifier ability can be used to compare classifier performance in a different way through classifier characteristic curves.","authors":["Fernando Martínez Plumed","Ricardo B. C. Prudêncio","Adolfo Martínez-Usó","Jose Hernandez-Orallo"],"meta":["June 2019Artificial Intelligence 271(2)","DOI:10.1016/j.artint.2018.09.004"],"references":["313796595_An_Analysis_of_Machine_Learning_Intelligence","309032785_Making_Sense_of_Item_Response_Theory_in_Machine_Learning","306331182_Evaluation_in_artificial_intelligence_from_task-oriented_to_ability-oriented_measurement","303683267_Beyond_Majority_Voting_Generating_Evaluation_Scales_using_Item_Response_Theory","263890323_OpenML_Networked_science_in_machine_learning","228534363_Mirt_A_Multidimensional_Item_Response_Theory_Package_for_the_R_Environment","228383466_ltm_An_R_Package_for_Latent_Variable_Modeling_and_Item_Response_Theory_Analyses","224384174_Isolation_Forest","220638052_A_Survey_of_Outlier_Detection_Methodologies","220637923_A_review_of_instance_selection_methods","220320437_Learning_Instance-Specific_Predictive_Models","47685417_SATzilla_Portfolio-based_Algorithm_Selection_for_SAT","309272056_Reframing_in_context_A_systematic_approach_for_model_reuse_in_machine_learning","287687232_Modeling_Progress_in_AI","259993539_Towards_UCI_A_mindful_repository_design","258276567_An_Instance_Level_Analysis_of_Data_Complexity","244432323_Instance_Selection_and_Construction_for_Data_Mining","236853551_Some_Latent_Trait_Models_and_Their_Use_in_Inferring_an_Examinee's_Ability","228728929_Feature_subset_selection_using_Thornton's_separability_index_and_its_applicability_to_a_number_of_sparse_proximity-based_classifiers","222831786_Algorithm_portfolios","222343753_An_Experimental_Comparison_of_Performance_Measures_for_Classification","222303175_Selection_of_Relevant_Features_and_Examples_in_Machine_Learning"]}