{"id":"221038227_SiteQ_Engineering_High_Performance_QA_system_Using_Lexico-Semantic_Pattern_Matching_and_Shallow_NLP","abstract":"this paper. 2. QA track: Systems and Experiences In TRECqo, the QA track consisted of three separate tasks: the main task, the list task and the context task. We participated in only the main task. The main task is similar to the task in previous QA tracks (TREC-8, TREC-9). NIST provided 500 questions that seek short, fact-based answers. Some questions may not have a known answer in the document collection. In that case, the response string \"NIL\" is judged correct. This differs from the previous QA tracks and makes the task somewhat more difficult. The answer-string should contain no more than 50 bytes; 25o-byte runs were abandoned this year. Participants must return at least one and no more than five responses per question ranked by preferences","authors":["Gary Geunbae Lee","Jungyun Seo","Seungwoo Lee","Hanmin Jung"],"meta":["January 2001","SourceDBLP"],"references":["221038313_LASSO_A_tool_for_surfing_the_answer_Net","2819226_Building_a_Question_Answering_Test_Collection","288547077_A_taxonomy_for_question_generation","242692115_Wordnet_a_lexical_database","242637644_An_Algorithm_for_Suffix_Stripping","242637407_The_process_of_question_answering_A_computer_simulation_of_cognition","221299246_On_Relevance_Weights_with_Little_Relevance_Information","221038155_FALCON_Boosting_Knowledge_for_Answer_Engines","221037909_Okapi_at_TREC","220017707_An_algoritm_for_suffix_stripping"]}