{"id":"318579097_Does_lp-minimization_outperform_l1-minimization","abstract":"In many application areas ranging from bioinformatics to imaging, we are faced with the following question: can we recover a sparse vector xo ∈ RN from its undersampled set of noisy observations y ∈ Rn, y = Axo + w. The last decade has witnessed a surge of algorithms and theoretical results to address this question. One of the most popular schemes is the ℓp-regularized least squares given by the following formulation: x(γ, p) ∈ argx min ½ ║y − Ax║²₂ + γ║x║pp; where p ∈ [0, 1]. Among these optimization problems, the case p = 1, also known as LASSO, is the best accepted in practice, for the following two reasons: (i) thanks to the extensive studies performed in the fields of high-dimensional statistics and compressed sensing, we have a clear picture of LASSO’s performance. (ii) it is convex and efficient algorithms exist for finding its global minima. Unfortunately, neither of the above two properties hold for 0≤ p < 1. However, they are still appealing because of the following folklores in the high-dimensional statistics: (i) x(γ, p) is closer to xo than x(γ, 1). (ii) If we employ iterative methods that aim to converge to a local minima of arg minx ½ ║y − Ax║²₂+γ║x║pp, then under good initialization, these algorithms converge to a solution that is still closer to xo than x(γ, 1). In spite of the existence of plenty of empirical results that support these folklore theorems, the theoretical progress to establish them has been very limited. This paper aims to study the above folklore theorems and establish their scope of validity. Starting with approximate message passing (AMP) algorithm as a heuristic method for solving ℓp-regularized least squares, we study the following questions: (i) what is the impact of initialization on the performance of the algorithm? (ii) when does the algorithm recover the sparse signal xo under a “good” initialization? (iii) when does the algorithm converge to the sparse signal regardless of the initialization? Studying these questions will not only shed light on the second folklore theorem, but also lead us to the answer of the first one, i.e., the performance of the global optima x(γ, p). For that purpose, we employ the replica analysis1 to show the connection between the solution of AMP and x(γ, p) in the asymptotic settings. This enables us to compare the accuracy of x(γ, p) and x(γ, 1). In particular, we will present an accurate characterization of the phase transition and noise sensitivity of ℓp-regularized least squares for every 0 ≤ p ≤ 1. Our results in the noiseless setting confirm that ℓp-regularized least squares (if γ is tuned optimally) exhibits the same phase transition for every 0 ≤ p < 1 and this phase transition is much better than that of LASSO. Furthermore, we show that in the noisy setting, there is a major difference between the performance of ℓp-regularized least squares with different values of p. For instance, we will show that for very small and very large measurement noises, p = 0 and p = 1 outperform the other values of p, respectively.","authors":["Le Zheng","Arian Maleki","Haolei Weng","Xiaodong Wang"],"meta":["July 2017IEEE Transactions on Information Theory PP(99):1-1","DOI:10.1109/TIT.2017.2717585","Project: lp-based compressed sensing"],"references":["267725837_Stability_and_robustness_of_lq_minimization_using_null_space_property","260542289_Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Applications_to_Compressed_Sensing","259782774_A_typical_reconstruction_limit_of_compressed_sensing_based_on_Lp-norm_minimization","309049473_Estimation_of_the_mean_of_a_multivariate_normal_distribution","278628297_Restricted_Isometry_Constants_where_lp_sparse_recovery_can_fail_for_0_p_1","263201047_From_Denoising_to_Compressed_Sensing","260800154_New_Improved_Algorithms_for_Compressive_Sensing_Based_on_lp_Norm","256981510_Asymptotic_Analysis_of_LASSOs_Solution_Path_with_Implications_for_Approximate_Message_Passing","251641666_Recovery_of_sparsest_signals_via_-minimization","245576839_On_the_Performance_of_Sparse_Recovery_via_L_p-minimization_0"]}