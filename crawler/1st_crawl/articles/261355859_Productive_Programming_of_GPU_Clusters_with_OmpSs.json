{"id":"261355859_Productive_Programming_of_GPU_Clusters_with_OmpSs","abstract":"Clusters of GPUs are emerging as a new computational scenario. Programming them requires the use of hybrid models that increase the complexity of the applications, reducing the productivity of programmers. We present the implementation of OmpSs for clusters of GPUs, which supports asynchrony and heterogeneity for task parallelism. It is based on annotating a serial application with directives that are translated by the compiler. With it, the same program that runs sequentially in a node with a single GPU can run in parallel in multiple GPUs either local (single node) or remote (cluster of GPUs). Besides performing a task-based parallelization, the runtime system moves the data as needed between the different nodes and GPUs minimizing the impact of communication by using affinity scheduling, caching, and by overlapping communication with the computational task. We show several applications programmed with OmpSs and their performance with multiple GPUs in a local node and in remote nodes. The results show good tradeoff between performance and effort from the programmer.","authors":["Javier Bueno","Judit Planas","Alejandro Duran","Rosa M. Badia"],"meta":["May 2012","DOI:10.1109/IPDPS.2012.58","Conference: Parallel & Distributed Processing Symposium (IPDPS), 2012 IEEE 26th International"],"references":["279062908_X10","249665797_Overview_of_the_HPC_Challenge_Benchmark_Suite","240064180_HMPP_A_hybrid_multi-core_parallel_programming_environment","228341641_Performance_analysis_of_a_hybrid_MPICUDA_implementation_of_the_NASLU_benchmark","221643791_Compilation_for_explicitly_managed_memory_hierarchies","221615676_Offload_-_Automating_Code_Migration_to_Heterogeneous_Multicore_Systems","221496783_CUDA-Lite_Reducing_GPU_Programming_Complexity","221321632_X10_An_object-oriented_approach_to_Non-Uniform_Cluster_Computing","221235790_Mint_Realizing_CUDA_performance_in_3D_stencil_methods_with_annotated_C","221201917_Message_passing_for_GPGPU_clusters_CudaMPI","220939027_Accelerating_Linpack_with_CUDA_on_heterogenous_clusters","220811652_Trace-driven_simulation_of_multithreaded_applications","220782774_OpenMPC_extended_OpenMP_programming_and_tuning_for_GPUs","258139318_Parallel_Programmability_and_the_Chapel_Language","242787762_OpenMP_Application_Program_Interface","229101022_The_OpenCL_Specification_version_1029","224130207_hiCUDA_high-level_GPGPU_programming","224124159_Accelerating_High_Performance_Applications_with_CUDA_and_MPI","221643570_Effective_Communication_and_Computation_Overlap_with_Hybrid_MPISMPSs","221497018_Unified_Parallel_C_for_GPU_Clusters_Language_Extensions_and_Compiler_Implementation","221235458_Handling_task_dependencies_under_strided_and_aliased_references","221202235_A_Dependency-Aware_Task-Based_Programming_Environment_for_Multi-Core_Architectures","220875726_Extending_the_OpenMP_Tasking_Model_to_Allow_Dependent_Tasks","220875714_A_Proposal_to_Extend_the_OpenMP_Tasking_Model_for_Heterogeneous_Architectures","220781491_190_TFlops_Astrophysical_N-body_Simulation_on_a_Cluster_of_GPUs","220497766_CellSs_Making_it_easier_to_program_the_Cell_Broadband_Engine_processor","201977140_A_Message_Passing_Interface_Standard","2645115_SUMMA_Scalable_Universal_Matrix_Multiplication_Algorithm","2257721_Cilk_An_Efficient_Multithreaded_Runtime_System"]}