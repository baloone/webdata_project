{"id":"308809538_Reducing_Features_of_KDD_CUP_1999_Dataset_for_Anomaly_Detection_Using_Back_Propagation_Neural_Network","abstract":"To detect and classify the anomaly in computer network, KDD CUP 1999 dataset is extensively used. This KDD CUP 1999 data set was generated by domain expert at MIT Lincon lab. To reduced number of features of this KDD CUP data set, various feature reduction techniques has been already used. These techniques reduce features from 41 into range of 10 to 22. Usage of such reduced dataset in machine learning algorithm leads to lower complexity, less processing time and high accuracy. Out of the various feature reduction technique available, one of them is Information Gain (IG) which has been already applied for the random forests classifier by Tesfahun et al. Tesfahun's approach reduces time and complexity of model and improves the detection rate for the minority classes in a considerable amount. This work investigates the effectiveness and the feasibility of Tesfahun et al.'s feature reduction technique on Back Propagation Neural Network classifier. We had performed various experiments on KDD CUP 1999 dataset and recorded Accuracy, Precision, Recall and Fscore values. In this work, we had done Basic, N-Fold Validation and Testing comparisons on reduced dataset with full feature dataset. Basic comparison clearly shows that the reduced dataset outer performs on size, time and complexity parameters. Experiments of N-Fold validation show that classifier that uses reduced dataset, have better generalization capacity. During the testing comparison, we found both the datasets are equally compatible. All the three comparisons clearly show that reduced dataset is better or is equally compatible, and does not have any drawback as compared to full dataset. Our experiments shows that usage of such reduced dataset in BPNN can lead to better model in terms of dataset size, complexity, processing time and generalization ability.","authors":["Bhavin Manharlal Shah","Bhushan H Trivedi"],"meta":["February 2015","DOI:10.1109/ACCT.2015.131","Conference: 2015 Fifth International Conference on Advanced Computing & Communication Technologies (ACCT)","Project: Intrusion Detection System"],"references":["224155468_Comparison_of_Machine_Learning_algorithms_performance_in_detecting_network_intrusion","220695151_Advanced_Data_Mining_Techniques","220321053_Eficient_Feature_Selection_Via_Analysis_of_Relevance_and_Redundancy","3619899_Chi2_Feature_Selection_and_Discretization_of_Numeric_Attributes","312621023_Filters_Wrappers_and_a_boosting-based_hybrid_for_feature_selection","266310934_Motoda_H_Feature_Selection_for_Knowledge_Discovery_and_Data_Mining_Kluwer_Academic_USA","264907682_Feature_extraction_Foundations_and_applications_Papers_from_NIPS_2003_workshop_on_feature_extraction_Whistler_BC_Canada_December_11-13_2003_With_CD-ROM","262388699_A_Comparative_Study_of_Classification_Techniques_for_Intrusion_Detection","261259249_Intrusion_Detection_Using_Random_Forests_Classifier_with_SMOTE_and_Feature_Reduction","261053758_An_Effective_Feature_Selection_Approach_for_Network_Intrusion_Detection","251834015_Intrusion_Detection_System_Based_on_Feature_Selection_and_Support_Vector_Machine","232618826_Feature_Selection_and_Design_of_Intrusion_Detection_System_Based_on_k-Means_and_Triangle_Area_Support_Vector_Machine","225615983_An_Introduction_to_Feature_Extraction","221438563_Feature_Selection_for_Machine_Learning_Comparing_a_Correlation-Based_Filter_Approach_to_the_Wrapper","221345775_A_Practical_Approach_to_Feature_Selection","221079877_Intrusion_Detection_in_Computer_Networks_with_Neural_and_Fuzzy_Classifiers","220688794_C45_Programs_for_Machine_Learning","2749444_Estimating_attributes_Analysis_and_extensions_of_RELIEF"]}