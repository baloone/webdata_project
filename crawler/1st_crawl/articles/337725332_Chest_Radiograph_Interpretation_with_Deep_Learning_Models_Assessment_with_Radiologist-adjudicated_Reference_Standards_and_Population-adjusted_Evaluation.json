{"id":"337725332_Chest_Radiograph_Interpretation_with_Deep_Learning_Models_Assessment_with_Radiologist-adjudicated_Reference_Standards_and_Population-adjusted_Evaluation","abstract":"Background\nDeep learning has the potential to augment the use of chest radiography in clinical radiology, but challenges include poor generalizability, spectrum bias, and difficulty comparing across studies.PurposeTo develop and evaluate deep learning models for chest radiograph interpretation by using radiologist-adjudicated reference standards.Materials and Methods\nDeep learning models were developed to detect four findings (pneumothorax, opacity, nodule or mass, and fracture) on frontal chest radiographs. This retrospective study used two data sets. Data set 1 (DS1) consisted of 759 611 images from a multicity hospital network and ChestX-ray14 is a publicly available data set with 112 120 images. Natural language processing and expert review of a subset of images provided labels for 657 954 training images. Test sets consisted of 1818 and 1962 images from DS1 and ChestX-ray14, respectively. Reference standards were defined by radiologist-adjudicated image review. Performance was evaluated by area under the receiver operating characteristic curve analysis, sensitivity, specificity, and positive predictive value. Four radiologists reviewed test set images for performance comparison. Inverse probability weighting was applied to DS1 to account for positive radiograph enrichment and estimate population-level performance.ResultsIn DS1, population-adjusted areas under the receiver operating characteristic curve for pneumothorax, nodule or mass, airspace opacity, and fracture were, respectively, 0.95 (95% confidence interval [CI]: 0.91, 0.99), 0.72 (95% CI: 0.66, 0.77), 0.91 (95% CI: 0.88, 0.93), and 0.86 (95% CI: 0.79, 0.92). With ChestX-ray14, areas under the receiver operating characteristic curve were 0.94 (95% CI: 0.93, 0.96), 0.91 (95% CI: 0.89, 0.93), 0.94 (95% CI: 0.93, 0.95), and 0.81 (95% CI: 0.75, 0.86), respectively.Conclusion\nExpert-level models for detecting clinically relevant chest radiograph findings were developed for this study by using adjudicated reference standards and with population-level performance estimation. Radiologist-adjudicated labels for 2412 ChestX-ray14 validation set images and 1962 test set images are provided.Â© RSNA, 2019Online supplemental material is available for this article.See also the editorial by Chang in this issue.","authors":["Anna Majkowska","Sid Mittal","David F. Steiner","Joshua J. Reicher"],"meta":["December 2019Radiology 294(2):191293","DOI:10.1148/radiol.2019191293"],"references":["332212711_Development_and_Validation_of_a_Deep_Learning-Based_Automated_Detection_Algorithm_for_Major_Thoracic_Diseases_on_Chest_Radiographs","331456618_Comparative_Accuracy_of_Diagnosis_by_Collective_Intelligence_of_Multiple_Physicians_vs_Individual_Physicians","330544991_Automated_Triaging_of_Adult_Chest_Radiographs_with_Deep_Artificial_Neural_Networks","329080328_Automated_detection_of_moderate_and_large_pneumothorax_on_frontal_chest_X-rays_using_deep_convolutional_neural_networks_A_retrospective_study","329078287_Deep_learning_for_chest_radiograph_diagnosis_A_retrospective_comparison_of_the_CheXNeXt_algorithm_to_practicing_radiologists","323411509_Deep_learning_in_radiology_an_overview_of_the_concepts_and_a_survey_of_the_state_of_the_art","316736470_ChestX-ray8_Hospital-scale_Chest_X-ray_Database_and_Benchmarks_on_Weakly-Supervised_Classification_and_Localization_of_Common_Thorax_Diseases","279989984_Multireader_multicase_reader_studies_with_binary_agreement_data_Simulation_analysis_validation_and_sizing","234960985_The_Lung_Image_Database_Consortium_LIDC_and_Image_Database_Resource_Initiative_IDRI_A_completed_reference_database_of_lung_nodules_on_CT_scans","221345647_Supervised_Learning_from_Multiple_Experts_Whom_to_trust_when_everyone_lies_a_bit","45092805_Current_perspectives_in_medical_image_perception","13447200_Variability_and_accuracy_in_mammographic_interpretation_using_the_American_College_of_Radiology_Breast_Imaging_Reporting_and_Dana_System_BI-RADS","346851078_Observer_Performance_Methods_for_Diagnostic_Imaging_Foundations_Modeling_and_Applications_with_R-Based_Examples","331217346_Artificial_Intelligence_in_Cardiothoracic_Radiology","329410885_Diagnostic_Case-Control_versus_Diagnostic_Cohort_Studies_for_Clinical_Validation_of_Artificial_Intelligence_Algorithm_Performance","327875259_Development_and_Validation_of_Deep_Learning-based_Automatic_Detection_Algorithm_for_Malignant_Pulmonary_Nodules_on_Chest_Radiographs","326511158_Discrepancy_Rates_and_Clinical_Impact_of_Imaging_Secondary_Interpretations_A_Systematic_Review_and_Meta-Analysis","324108781_Deep_Learning_in_Radiology","323986205_Double_reading_in_breast_cancer_screening_cohort_evaluation_in_the_CO-OPS_trial","322319200_Methodologic_Guide_for_Evaluating_Clinical_Performance_and_Effect_of_Artificial_Intelligence_Technology_for_Medical_Diagnosis_and_Prediction","322058545_Revisiting_Unreasonable_Effectiveness_of_Data_in_Deep_Learning_Era","320223300_Grader_Variability_and_the_Importance_of_Reference_Standards_for_Evaluating_Machine_Learning_Models_for_Diabetic_Retinopathy","318742463_Very_Deep_Convolutional_Networks_for_Text_Classification","317579200_SmoothGrad_removing_noise_by_adding_noise","290789847_Inverse_probability_weighting","233003154_Hypothesis_testing_of_diagnostic_accuracy_for_multiple_readers_and_multiple_tests_An_ANOVA_approach_with_dependent_observations","229551897_Learning_From_Errors_in_Radiology_A_Comprehensive_Review","227708963_Hypothesis_Testing_in_Noninferiority_and_Equivalence_MRMC_ROC_Studies","224004787_Common_patterns_in_558_diagnostic_radiology_errors","12267548_Measuring_Performance_in_Chest_Radiography1","7239350_A_comparison_of_denominator_degrees_of_freedom_for_multiple_observer_ROC_analysis","5656939_Fleischner_Society_Glossary_of_Terms_for_Thoracic_Imaging"]}