{"id":"270878020_Factored_Language_Model_based_on_Recurrent_Neural_Network","abstract":"Among various neural network language models (NNLMs), recurrent neural network-based language models (RNNLMs) are very competitive in many cases. Most current RNNLMs only use one single feature stream, i.e., surface words. However, previous studies proved that language models with additional linguistic information achieve better performance. In this study, we extend RNNLM by explicitly integrating additional linguistic information, including morphological, syntactic, or semantic factors. Our proposed RNNLM is called a factored RNNLM that is expected to enhance RNNLMs. A number of experiments are carried out that show the factored RNNLM improves the performance for all considered tasks: consistent perplexity and word error rate (WER) reductions. In the Penn Treebank corpus, the relative improvements over n-gram LM and RNNLM are 29.0% and 13.0%, respectively. In the IWSLT-2011 TED ASR test set, absolute WER reductions over RNNLM and n-gram LM reach 0.63 and 0.73 points.","authors":["Youzheng Wu","Xugang lu","Hitoshi Yamamoto","Shigeki Matsuda"],"meta":["December 2012","Conference: Proceedings of COLING 2012"],"references":["281555625_Towards_Recurrent_Neural_Networks_Language_Models_with_Linguistic_and_Contextual_Features","262272503_Large_pruned_or_continuous_space_language_models_on_a_GPU_for_statistical_machine_translation","239765773_Feature_engineering_in_Context-Dependent_Deep_Neural_Networks_for_conversational_speech_transcription","228828379_The_Kaldi_speech_recognition_toolkit","228567121_Overview_of_the_IWSLT_2010_evaluation_campaign","224246503_Extensions_of_recurrent_neural_network_language_model","224097026_Syntactic_Features_for_Arabic_Speech_Recognition","221489926_Recurrent_neural_network_based_language_model","221013253_Factored_Translation_Models","221013220_Style_topic_language_model_adaptation_using_HMM-LDA","220816701_Factored_Neural_Language_Models","220355244_Class-Based_n-gram_Models_of_Natural_Language","2903062_A_Guide_to_Recurrent_Neural_Networks_and_Backpropagation","2572499_Exploiting_Syntactic_Structure_for_Language_Modeling","2539093_A_Study_on_Richer_Syntactic_Dependencies_for_Structured_Language_Modeling","289699881_Large_scale_hierarchical_neural_network_language_models","260321000_Fast_Syntactic_Analysis_for_Statistical_Language_Modeling_via_Substructure_Sharing_and_Uptraining","242821088_A_Maximum_Entropy_approach_to_adaptive_statistical_language_modeling","229100910_Topic-based_language_models_using_EM","225818196_Neural_Probabilistic_Language_Models","222800591_Structured_language_modeling","222663060_Maximum_entropy_techniques_for_exploiting_syntactic_semantic_and_collocational_dependencies_in_language_modeling","222650733_Continuous_space_language_models","221480736_Empirical_Evaluation_and_Combination_of_Advanced_Language_Modeling_Techniques","221012724_Random_Forests_in_Language_Modelin","220875255_An_Empirical_Study_of_Smoothing_Techniques_for_Language_Modeling","220873718_Discriminative_Syntactic_Language_Modeling_for_Speech_Recognition","4087374_Exact_training_of_a_neural_syntactic_language_model","3908397_Classes_for_Fast_Maximum_Entropy_Training","2952252_Automatic_Learning_of_Language_Model_Structure","2602251_Srilm_---_An_Extensible_Language_Modeling_Toolkit"]}