{"id":"327295866_Human_Knowledge_in_Constructing_AI_Systems_-_Neural_Logic_Networks_Approach_towards_an_Explainable_AI","abstract":"To build an easy-to-use AI and ML, it is crucial to gain user’s trust. Trust comes from understanding the reasoning behind an AI system’s conclusions and results. The recent research efforts on Explainable AI (XAI) reflect the importance of explainability in responding to the criticism of “black box” type of AI. Neural Logic Networks (NLN) is a research to embed logic reasoning (being binary or fuzzy) to connectionist models having humans’ domain knowledge taken into consideration. The reasoning carried out on such network structures allows possible interpretation beyond binary logic. This article intends to discuss the potential contribution of NLN approach in making reasoning more explainable.","authors":["Liya Ding"],"meta":["January 2018Procedia Computer Science 126:1561-1570","DOI:10.1016/j.procs.2018.08.129"],"references":["225734295_The_Elements_of_Statistical_Learning_Data_Mining_Inference_and_Prediction","286764893_Human-level_concept_learning_through_probabilistic_program_induction","283478271_Interpretable_classifiers_using_rules_and_Bayesian_analysis_Building_a_better_stroke_prediction_model","250920463_Learning_AND-OR_Templates_for_Object_Recognition_and_Detection","244454769_Artificial_Intelligence_A_Guide_to_Intelligent_Systems","216722124_Neural_Fuzzy_Systems_--_A_Neuro-Fuzzy_Synergism_to_Intelligent_Systems"]}