{"id":"330690895_Training_deep_neural_networks_for_binary_communication_with_the_Whetstone_method","abstract":"The computational cost of deep neural networks presents challenges to broadly deploying these algorithms. Low-power and embedded neuromorphic processors offer potentially dramatic performance-per-watt improvements over traditional processors. However, programming these brain-inspired platforms generally requires platform-specific expertise. It is therefore difficult to achieve state-of-the-art performance on these platforms, limiting their applicability. Here we present Whetstone, a method to bridge this gap by converting deep neural networks to have discrete, binary communication. During the training process, the activation function at each layer is progressively sharpened towards a threshold activation, with limited loss in performance. Whetstone sharpened networks do not require a rate code or other spike-based coding scheme, thus producing networks comparable in timing and size to conventional artificial neural networks. We demonstrate Whetstone on a number of architectures and tasks such as image classification, autoencoders and semantic segmentation. Whetstone is currently implemented within the Keras wrapper for TensorFlow and is widely extendable.","authors":["William Severa","Craig M. Vineyard","Ryan Dellana","Stephen J. Verzi"],"meta":["February 2019Nature Machine Intelligence 1(2)","DOI:10.1038/s42256-018-0015-y"],"references":["321654016_Conversion_of_Continuous-Valued_Deep_Networks_to_Efficient_Event-Driven_Networks_for_Image_Classification","319856813_In-Datacenter_Performance_Analysis_of_a_Tensor_Processing_Unit","319312259_Fashion-MNIST_a_Novel_Image_Dataset_for_Benchmarking_Machine_Learning_Algorithms","317613363_In-Datacenter_Performance_Analysis_of_a_Tensor_Processing_Unit","339440322_Hyperopt_A_Python_Library_for_Optimizing_the_Hyperparameters_of_Machine_Learning_Algorithms","326855225_NVIDIA_Tensor_Core_Programmability_Performance_Precision","326109067_Hyperband_A_novel_bandit-based_approach_to_hyperparameter_optimization","323722776_NVIDIA_Tensor_Core_Programmability_Performance_Precision","320968331_Designing_Energy-Efficient_Convolutional_Neural_Networks_Using_Energy-Aware_Pruning","317614308_Gradient_Descent_for_Spiking_Neural_Networks","317040195_A_Survey_of_Neuromorphic_Computing_and_Neural_Networks_in_Hardware","315667264_Efficient_Processing_of_Deep_Neural_Networks_A_Tutorial_and_Survey","310440966_Designing_Energy-Efficient_Convolutional_Neural_Networks_using_Energy-Aware_Pruning","312103385_A_historical_survey_of_algorithms_and_hardware_architectures_for_neural-inspired_and_neuromorphic_computing_applications","311609041_Deep_Residual_Learning_for_Image_Recognition"]}