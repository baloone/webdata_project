{"id":"314115916_Towards_Deeper_Understanding_of_Variational_Autoencoding_Models","abstract":"We propose a new family of optimization criteria for variational auto-encoding models, generalizing the standard evidence lower bound. We provide conditions under which they recover the data distribution and learn latent features, and formally show that common issues such as blurry samples and uninformative latent features arise when these conditions are not met. Based on these new insights, we propose a new sequential VAE model that can generate sharp samples on the LSUN image dataset based on pixel-wise reconstruction loss, and propose an optimization criterion that encourages unsupervised learning of informative latent features.","authors":["Shengjia Zhao","Jiaming Song","Stefano Ermon"],"meta":["February 2017"],"references":["315489474_Learning_to_Generate_Samples_from_Noise_through_Infusion_Training","311648103_An_Architecture_for_Deep_Hierarchical_Generative_Models","310329534_PixelVAE_A_Latent_Variable_Model_for_Natural_Images","319770355_Generative_Adversarial_Nets","319770221_Ladder_Variational_Autoencoders","319770144_Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative_Adversarial_Networks","319770134_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models","303993361_Improving_Variational_Inference_with_Inverse_Autoregressive_Flow","303755314_f-GAN_Training_Generative_Neural_Samplers_using_Variational_Divergence_Minimization","301874314_Pixel_Recurrent_Neural_Networks"]}