{"id":"256981584_Asymptotic_Normality_and_Optimalities_in_Estimation_of_Large_Gaussian_Graphical_Model","abstract":"The Gaussian graphical model has attracted great attention in recent years.\nThis paper considers a fundamental question: When is it possible to estimate\nlow-dimensional parameters at parametric square-root rate in a large Gaussian\ngraphical model? A novel regression approach is proposed to obtain\nasymptotically efficient estimation of each entry of a precision matrix under a\nsparseness condition relative to the sample size. When the precision matrix is\nnot sufficiently sparse, a lower bound is established to show that it is no\nlonger possible to achieve the parametric rate in the estimation of each entry.\nThis lower bound result, which provides an answer to the delicate sample size\nquestion, is established with a novel construction of a subset of sparse\nprecision matrices in an application of Le Cam's Lemma. Moreover, the proposed\nestimator is proven to have optimal convergence rate when the parametric rate\ncannot be achieved, under a minimal sample requirement.\nThe proposed estimator is applied to test the presence of an edge in the\nGaussian graphical model or to recover the support of the entire model, to\nobtain adaptive rate-optimal estimation of the entire precision matrix as\nmeasured by the matrix $l_{q}$ operator norm, and to make inference in latent\nvariables in the graphical model. All these are achieved under a sparsity\ncondition on the precision matrix and a side condition on the range of its\nspectrum. This significantly relaxes the commonly imposed uniform signal\nstrength condition on the precision matrix, irrepresentable condition on the\nHessian tensor operator of the covariance matrix or the $\\ell_{1}$ constraint\non the precision matrix. Numerical results confirm our theoretical findings.\nThe ROC curve of the proposed algorithm, Asymptotic Normal Thresholding (ANT),\nfor support recovery significantly outperforms that of the popular GLasso\nalgorithm.","authors":["Zhao Ren","Tingni Sun","Cun-Hui Zhang","Harrison H. Zhou"],"meta":["September 2013The Annals of Statistics 43(3)","DOI:10.1214/14-AOS1286","SourcearXiv"],"references":["254211576_High-dimensional_covariance_estimation_by_minimizing_-penalized_log-determinant_divergence","304045069_High-dimensional_Statistical_Inference_from_Vector_to_Matrix","301853907_Latent_variable_graphical_model_selection_via_convex_optimization","284803034_Convergence_of_estimates_under_dimensionality_restrictions","278252560_MINIMAX_ESTIMATION_OF_LARGE_COVARIANCE_MATRICES_UNDER_l1-NORM_COMMENT","271512973_Limit_Theorems_of_Probability_Theory","271512802_Extensions_of_results_of_Komlos_Major_and_Tusnady_to_the_multivariate_case","266720094_Festschrift_for_Lucien_Le_Cam","247934886_Regularization","245462345_Approximate_Distributions_of_Order_Statistics_With_Applications_to_Nonparameteric_Statistics"]}