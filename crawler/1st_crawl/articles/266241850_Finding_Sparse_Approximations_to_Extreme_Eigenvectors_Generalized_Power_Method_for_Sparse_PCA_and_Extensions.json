{"id":"266241850_Finding_Sparse_Approximations_to_Extreme_Eigenvectors_Generalized_Power_Method_for_Sparse_PCA_and_Extensions","abstract":"In the first part of this work, based on [2], we develop a new approach to sparse principal component analysis (sparse PCA). We propose four optimization formulations of the problem, aimed at extracting one or several sparse dominant components. While the initial formulations involve nonconvex functions, we rewrite them into the form of an optimization program involving maximization of a convex function on a compact set and propose and analyze a simple gradient method for solving it (generalized power method). We demonstrate numerically on a set of random and gene expression test problems that our approach outperforms existing algorithms both in quality of the obtained solution and in speed. A natural extension of the ideas above allows us to construct a method for finding, simultaneously, jointly sparse approximations to the eigenvectors associated with the largest and smallest eigenvalues of a symmetric psd matrix. This problem is equivalent to the Compressed Sensing problem of finding bounds on the asymmetric Restricted Isometry constants with the additional new requirement for the respective sparse eigenvectors to be supported on the same set. We prove a result on the emergence of joint sparsity in the iterates of the method and show that in the non-penalized case, the iterates are identical to the normalized gradients of the iterates of the Cauchy steepest descent method applied to minimizing a convex quadratic function [1].","authors":["Peter Richt√°rik"],"meta":[],"references":["46466551_Generalized_Power_Method_for_Sparse_Principal_Component_Analysis","226246361_On_a_successive_transformation_of_probability_distribution_and_its_application_to_the_analysis_of_the_optimum_gradient_method"]}