{"id":"280590083_Document_Embedding_with_Paragraph_Vectors","abstract":"Paragraph Vectors has been recently proposed as an unsupervised method for\nlearning distributed representations for pieces of texts. In their work, the\nauthors showed that the method can learn an embedding of movie review texts\nwhich can be leveraged for sentiment analysis. That proof of concept, while\nencouraging, was rather narrow. Here we consider tasks other than sentiment\nanalysis, provide a more thorough comparison of Paragraph Vectors to other\ndocument modelling algorithms such as Latent Dirichlet Allocation, and evaluate\nperformance of the method as we vary the dimensionality of the learned\nrepresentation. We benchmarked the models on two document similarity data sets,\none from Wikipedia, one from arXiv. We observe that the Paragraph Vector method\nperforms significantly better than other methods, and propose a simple\nimprovement to enhance embedding quality. Somewhat surprisingly, we also show\nthat much like word embeddings, vector operations on Paragraph Vectors can\nperform useful semantic results.","authors":["Andrew M. Dai","Christopher Olah","Quoc V. Le"],"meta":["July 2015","SourcearXiv"],"references":["234131319_Efficient_Estimation_of_Word_Representations_in_Vector_Space","319770439_Efficient_Estimation_of_Word_Representations_in_Vector_Space","303157903_Visualizing_High-Dimensional_Data_using_t-SNE","262416109_Distributed_Representations_of_Sentences_and_Documents","262157385_Latent_Dirichlet_Allocation","241858646_Visualizing_High-Dimensional_Data_Using_t-SNE","220319974_Latent_Dirichlet_Allocation"]}