{"id":"225984362_On_Early_Stopping_in_Gradient_Descent_Learning","abstract":"In this paper we study a family of gradient descent algorithms to approximate the regression function from reproducing kernel\nHilbert spaces (RKHSs), the family being characterized by a polynomial decreasing rate of step sizes (or learning rate). By\nsolving a bias-variance trade-off we obtain an early stopping rule and some\nprobabilistic upper bounds for the convergence of the algorithms. We also discuss the implication of these results in the\ncontext of classification where some fast convergence rates can be achieved for plug-in classifiers. Some connections are\naddressed with Boosting, Landweber iterations, and the online learning algorithms as stochastic approximations of the gradient\ndescent method.","authors":["Yuan Yao","Lorenzo Rosasco","Andrea Caponnetto"],"meta":["August 2007Constructive Approximation 26(2):289-315","DOI:10.1007/s00365-006-0663-2"],"references":["277034934_Reproducing_kernel_Hilbert_spaces_and_Mercer_theorem","280687718_Greedy_Function_Approximation_A_Gradient_Boosting_Machine","268316893_A_Distribution-Free_Theory_of_Non-Parametric_Regression","248728006_Spline_Models_for_Observational_Data","248086333_Three_topics_in_ill-posed_problems","247062440_Operator-theoretical_treatment_of_Markoff's_process_and_mean_ergodic_theorem","246999853_On_Complexity_Issue_of_Online_Learning_Algorithms","246861705_Moduli_of_continuity_for_operator_valued_functions","246258275_From_Uniform_Laws_of_Large_Numbers_to_Uniform_Ergodic_Theorems","243772934_Boosting_with_the_L2Loss_Regression_Classification"]}