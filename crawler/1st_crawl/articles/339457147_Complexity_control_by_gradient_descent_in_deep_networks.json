{"id":"339457147_Complexity_control_by_gradient_descent_in_deep_networks","abstract":"Overparametrized deep networks predict well, despite the lack of an explicit complexity control during training, such as an explicit regularization term. For exponential-type loss functions, we solve this puzzle by showing an effective regularization effect of gradient descent in terms of the normalized weights that are relevant for classification. Understanding the underlying mechanisms behind the successes of deep networks remains a challenge. Here, the author demonstrates an implicit regularization in training deep networks, showing that the control of complexity in the training is hidden within the optimization technique of gradient descent.","authors":["Tomaso A. Poggio","Qianli Liao","Andrzej Banburski"],"meta":["February 2020Nature Communications 11(1)","DOI:10.1038/s41467-020-14663-9"],"references":["343009795_Just_interpolate_Kernel_Ridgeless_regression_can_generalize","320726685_The_Implicit_Bias_of_Gradient_Descent_on_Separable_Data","319770104_Deep_Learning_Requires_Rethinking_Generalization","316821263_Geometry_of_Optimization_and_Implicit_Regularization_in_Deep_Learning","310122390_Understanding_deep_learning_requires_rethinking_generalization","301872762_Weight_Normalization_A_Simple_Reparameterization_to_Accelerate_Training_of_Deep_Neural_Networks","300356791_Regularization_Operators","3317656_On_gradient_adaptation_with_unit-norm_constraints"]}