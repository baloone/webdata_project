{"id":"320836912_Reliable_benchmarking_requirements_and_solutions","abstract":"Benchmarking is a widely used method in experimental computer science, in particular, for the comparative evaluation of tools and algorithms. As a consequence, a number of questions need to be answered in order to ensure proper benchmarking, resource measurement, and presentation of results, all of which is essential for researchers, tool developers, and users, as well as for tool competitions. We identify a set of requirements that are indispensable for reliable benchmarking and resource measurement of time and memory usage of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework can (on Linux systems) currently only be done by using the cgroup and namespace features of the kernel. We developed BenchExec, a ready-to-use, tool-independent, and open-source implementation of a benchmarking framework that fulfills all presented requirements, making reliable benchmarking and resource measurement easy. Our framework is able to work with a wide range of different tools, has proven its reliability and usefulness in the International Competition on Software Verification, and is used by several research groups worldwide to ensure reliable benchmarking. Finally, we present guidelines on how to present measurement results in a scientifically valid and comprehensible way.","authors":["Dirk Beyer","Stefan LÃ¶we","Philipp Wendler"],"meta":["February 2019International Journal on Software Tools for Technology Transfer 21(3)","DOI:10.1007/s10009-017-0469-y"],"references":["337642198_The_2014_SMT_Competition","326393913_Experiments_for_On_the_Techniques_We_Create_the_Tools_We_Build_and_Their_Misalignments","315857258_Software_Verification_with_Validation_of_Results","314258385_EMP_Execution_time_measurement_protocol_for_compute-bound_programs","311474956_Producing_wrong_data_without_doing_anything_obviously_wrong","308731375_BenchKit_a_Tool_for_Massive_Concurrent_Benchmarking","303099230_On_the_techniques_we_create_the_tools_we_build_and_their_misalignments_a_study_of_KLEE","300541540_Benchmarking_and_Resource_Measurement","300078493_Reliable_and_Reproducible_Competition_Results_with_BenchExec_and_Witnesses_Report_on_SV-COMP_2016","298787690_Repeatability_in_Computer_Systems_Research"]}