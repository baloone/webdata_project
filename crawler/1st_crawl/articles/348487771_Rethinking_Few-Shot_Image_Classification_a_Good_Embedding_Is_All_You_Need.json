{"id":"348487771_Rethinking_Few-Shot_Image_Classification_a_Good_Embedding_Is_All_You_Need","abstract":"The focus of recent meta-learning research has been onthe development of learning algorithms that can quicklyadapt to test time tasks with limited data and low compu-tational cost. Few-shot learning is widely used as one ofthe standard benchmarks in meta-learning. In this work, weshow that a simple baseline: learning a supervised or self-supervised representation on the meta-training set, followedby training a linear classifier on top of this representation,outperforms state-of-the-art few-shot learning methods. Anadditional boost can be achieved through the use of self-distillation. This demonstrates that using a good learnedembedding model can be more effective than sophisticatedmeta-learning algorithms. We believe that our findings mo-tivate a rethinking of few-shot image classification bench-marks and the associated role of meta-learning algorithms.Code is available at:http://github.com/WangYueFt/rfs/.","authors":["Yonglong Tian","Yue Wang","Joshua B Tenenbaum","Phillip John Isola"],"meta":["August 2020"],"references":[]}