{"id":"332366904_Meta_Inverse_Reinforcement_Learning_via_Maximum_Reward_Sharing_for_Human_Motion_Analysis","abstract":"This work handles the inverse reinforcement learning (IRL) problem where only a small number of demonstrations are available from a demonstrator for each high-dimensional task, insufficient to estimate an accurate reward function. Observing that each demonstrator has an inherent reward for each state and the task-specific behaviors mainly depend on a small number of key states, we propose a meta IRL algorithm that first models the reward function for each task as a distribution conditioned on a baseline reward function shared by all tasks and dependent only on the demonstrator, and then finds the most likely reward function in the distribution that explains the task-specific behaviors. We test the method in a simulated environment on path planning tasks with limited demonstrations, and show that the accuracy of the learned reward function is significantly improved.","authors":["Kun Li","Joel W. Burdick"],"meta":["December 2017","Conference: Workshop on Meta-Learning (MetaLearn 2017)"],"references":["326273717_Inverse_Reinforcement_Learning_via_Function_Approximation_for_Clinical_Motion_Analysis","318699603_Bellman_Gradient_Iteration_for_Inverse_Reinforcement_Learning","221606352_Maximum_Entropy_Inverse_Reinforcement_Learning","220815343_Bayesian_Inverse_Reinforcement_Learning","272161307_Reinforcement_Learning_An_Introduction","2622278_Algorithms_for_Inverse_Reinforcement_Learning"]}