{"id":"315105670_The_coverage_of_Microsoft_Academic_Analyzing_the_publication_output_of_a_university","abstract":"This is the first detailed study on the coverage of Microsoft Academic (MA). Based on the complete and verified publication list of a university, the coverage of MA was assessed and compared with two benchmark databases, Scopus and Web of Science (WoS), on the level of individual publications. Citation counts were analyzed, and issues related to data retrieval and data quality were examined. A Perl script was written to retrieve metadata from MA based on publication titles. The script is freely available on GitHub. We find that MA covers journal articles, working papers, and conference items to a substantial extent and indexes more document types than the benchmark databases (e.g., working papers, dissertations). MA clearly surpasses Scopus and WoS in covering book-related document types and conference items but falls slightly behind Scopus in journal articles. The coverage of MA is favorable for evaluative bibliometrics in most research fields, including economics/business, computer/information sciences, and mathematics. However, MA shows biases similar to Scopus and WoS with regard to the coverage of the humanities, non-English publications, and open-access publications. Rank correlations of citation counts are high between MA and the benchmark databases. We find that the publication year is correct for 89.5% of all publications and the number of authors is correct for 95.1% of the journal articles. Given the fast and ongoing development of MA, we conclude that MA is on the verge of becoming a bibliometric superpower. However, comprehensive studies on the quality of MA metadata are still lacking.","authors":["Sven E. Hug","Martin Paul Br√§ndle"],"meta":["September 2017Scientometrics 113(3)","DOI:10.1007/s11192-017-2535-3","Project: Bibliometrics with Microsoft Academic"],"references":["318162758_Microsoft_Academic_is_one_year_old_the_Phoenix_is_ready_to_leave_the_nest","318034275_An_Evidence-Based_Review_of_Academic_Web_Search_Engines_2014-2016_Implications_for_Librarians'_Practice_and_Research_Agenda","315570602_Quantifying_and_suppressing_ranking_bias_in_a_large_citation_network","311491690_WSDM_Cup_2016_Entity_Ranking_Challenge","345710943_Visualizing_Scholarly_Publications_and_Citations_to_Enhance_Author_Profiles","345695379_Analysing_Trends_in_Computer_Science_Research_A_Preliminary_Study_Using_The_Microsoft_Academic_Graph","321877961_Citation_Analysis_in_Research_Evaluation","319730867_Implications_for_the_Use_of_Citation_Analysis_in_Research_Evaluation","312639147_Investigations_on_Rating_Computer_Sciences_Conferences_An_Experiment_with_the_Microsoft_Academic_Graph_Dataset","312635222_AceMap_A_Novel_Approach_towards_Displaying_Relationship_among_Academic_Literatures"]}