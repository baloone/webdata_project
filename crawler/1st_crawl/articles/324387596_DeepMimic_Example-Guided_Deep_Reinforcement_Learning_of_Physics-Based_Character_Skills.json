{"id":"324387596_DeepMimic_Example-Guided_Deep_Reinforcement_Learning_of_Physics-Based_Character_Skills","abstract":"A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.","authors":["Xue Bin Peng","Pieter Abbeel","Sergey Levine","Michiel van de Panne"],"meta":["April 2018ACM Transactions on Graphics 37(4)","DOI:10.1145/3197517.3201311"],"references":["320098321_Overcoming_Exploration_in_Reinforcement_Learning_with_Demonstrations","318418554_Distral_Robust_Multitask_Reinforcement_Learning","318316125_Learning_human_behaviors_from_motion_capture_by_adversarial_imitation","318316001_Emergence_of_Locomotion_Behaviours_in_Rich_Environments","314237647_EX2_Exploration_with_Exemplar_Models_for_Deep_Reinforcement_Learning","309207004_Learning_and_Transfer_of_Modulated_Locomotor_Controllers","308896398_EPOpt_Learning_Robust_Neural_Network_Policies_Using_Model_Ensembles","305881121_Generative_Adversarial_Imitation_Learning","303822096_Unifying_Count-Based_Exploration_and_Intrinsic_Motivation","277958927_High-Dimensional_Continuous_Control_Using_Generalized_Advantage_Estimation","277953869_Dynamic_Terrain_Traversal_Skills_Using_Reinforcement_Learning","261353847_Synthesis_and_stabilization_of_complex_behaviors_through_online_trajectory_optimization","256663259_Continuous_Character_Control_with_Low-Dimensional_Embeddings","256663171_Optimizing_Locomotion_Controllers_Using_Biologically-Based_Actuators_and_Objectives","224221676_Stable_Proportional-Derivative_Controllers","324640731_Learning_to_Schedule_Control_Fragments_for_Physics-Based_Characters_Using_Deep_Q-Learning","321234717_How_to_train_your_dragon_example-guided_control_of_flapping_flight","320486736_Sim-to-Real_Transfer_of_Robotic_Control_with_Dynamics_Randomization","320098469_Learning_Complex_Dexterous_Manipulation_with_Deep_Reinforcement_Learning_and_Demonstrations","319769991_Trust_Region_Policy_Optimization","318780065_Robust_Task-based_Control_Policies_for_Physics-based_Characters","318715042_Learning_locomotion_skills_using_DeepRL_does_the_choice_of_action_space_matter","318613602_Learning_to_schedule_control_fragments_for_physics-based_characters_using_deep_Q-learning","318612822_DeepLoco_dynamic_locomotion_skills_using_hierarchical_deep_reinforcement_learning","318610298_Phase-functioned_neural_networks_for_character_control","318584439_Proximal_Policy_Optimization_Algorithms","317989385_Learning_to_Schedule_Control_Fragments_for_Physics-Based_Characters_Using_Deep_Q-Learning","314779250_Discovery_of_complex_behaviors_through_contact-invariant_optimization","314727158_Continuous_character_control_with_low-dimensional_embeddings","310235383_CAD2RL_Real_Single-Image_Flight_without_a_Single_Real_Image","305218345_A_deep_learning_framework_for_character_motion_synthesis_and_editing","305218120_Terrain-adaptive_locomotion_skills_using_deep_reinforcement_learning","305217928_Task-based_locomotion","303357202_Guided_Learning_of_Control_Graphs_for_Physics-Based_Characters","301648324_Benchmarking_Deep_Reinforcement_Learning_for_Continuous_Control","286197079_Locomotion_Control_for_Many-Muscle_Humanoids","283889537_Online_Control_of_Simulated_Humanoids_Using_Particle_Belief_Propagation","279964320_Simple_statistical_gradient-following_algorithms_for_connectionist_reinforcement_learning","273188545_Iterative_Training_of_Dynamic_Skills_Inspired_by_Human_Coaching_Techniques","272837232_Human-level_control_through_deep_reinforcement_learning","270697694_Generalizing_Locomotion_Style_to_New_Animals_With_Inverse_Optimal_Regression","266654220_Diverse_motion_variations_for_physics-based_character_animation","258670815_Flexible_Muscle-Based_Locomotion_for_Bipedal_Creatures","254461837_Discovery_of_Complex_Behaviors_through_Contact-Invariant_Optimization","227535912_Simulation_of_Human_Motion_Data_using_Short-Horizon_Model-Predictive_Control","224625877_Synthesis_of_Controllers_for_Stylized_Planar_Bipedal_Walking","220505886_Synthesis_of_Responsive_Motion_Using_a_Dynamic_Model","220184581_Motion_Fields_for_Interactive_Character_Locomotion","220184380_Simulating_Biped_Behaviors_from_Human_Motion_Data","220184242_Data-Driven_Biped_Control","220183956_SIMBICON_simple_biped_locomotion_control","220183871_Sampling-based_Contact-rich_Motion_Control","220183702_Robust_Task-based_Control_Policies_for_Physics-based_Characters","220184080_Generalized_Biped_Walking_Control","220183965_Contact-aware_Nonlinear_Control_of_Dynamic_Characters"]}