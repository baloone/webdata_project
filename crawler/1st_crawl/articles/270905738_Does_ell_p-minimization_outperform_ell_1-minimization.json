{"id":"270905738_Does_ell_p-minimization_outperform_ell_1-minimization","abstract":"In many application areas we are faced with the following question: Can we\nrecover a sparse vector $x_o \\in \\mathbb{R}^N$ from its undersampled set of\nnoisy observations $y \\in \\mathbb{R}^n$, $y=A x_o+w$. The last decade has\nwitnessed a surge of algorithms and theoretical results addressing this\nquestion. One of the most popular algorithms is the $\\ell_p$-penalized least\nsquares (LPLS) given by the following formulation: \\[ \\hat{x}(\\lambda,p )\\in\n\\arg\\min_x \\frac{1}{2}\\|y - Ax\\|_2^2+\\lambda\\|x\\|_p^p, \\] where $p \\in [0,1]$.\nDespite the non-convexity of these problems for $p<1$, they are still appealing\nbecause of the following folklores in compressed sensing: (i)\n$\\hat{x}(\\lambda,p )$ is closer to $x_o$ than $\\hat{x}(\\lambda,1)$. (ii) If we\nemploy iterative methods that aim to converge to a local minima of LPLS, then\nunder good initialization these algorithms converge to a solution that is\ncloser to $x_o$ than $\\hat{x}(\\lambda,1)$. In spite of the existence of plenty\nof empirical results that support these folklore theorems, the theoretical\nprogress to establish them has been very limited.\nThis paper aims to study the above folklore theorems and establish their\nscope of validity. Starting with approximate message passing algorithm as a\nheuristic method for solving LPLS, we study the impact of initialization on the\nperformance of AMP. Then, we employ the replica analysis to show the connection\nbetween the solution of $AMP$ and $\\hat{x}(\\lambda, p)$ in the asymptotic\nsettings. This enables us to compare the accuracy of $\\hat{x}(\\lambda,p)$ for\n$p \\in [0,1]$. In particular, we will characterize the phase transition and\nnoise sensitivity of LPLS for every $0\\leq p\\leq 1$ accurately. Our results in\nthe noiseless setting confirm that LPLS exhibits the same phase transition for\nevery $0\\leq p <1$ and this phase transition is much higher than that of LASSO.","authors":["Le Zheng","Arian Maleki","Xiaodong Wang","Teng Long"],"meta":["January 2015","SourcearXiv"],"references":["267725837_Stability_and_robustness_of_lq_minimization_using_null_space_property","260542289_Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Applications_to_Compressed_Sensing","259782774_A_typical_reconstruction_limit_of_compressed_sensing_based_on_Lp-norm_minimization","313508668_Compressive_sensing","309049473_Estimation_of_the_mean_of_a_multivariate_normal_distribution","278628297_Restricted_Isometry_Constants_where_lp_sparse_recovery_can_fail_for_0_p_1","263201047_From_Denoising_to_Compressed_Sensing","260800154_New_Improved_Algorithms_for_Compressive_Sensing_Based_on_lp_Norm","256981510_Asymptotic_Analysis_of_LASSOs_Solution_Path_with_Implications_for_Approximate_Message_Passing","251641666_Recovery_of_sparsest_signals_via_-minimization","239814804_Erratum_A_typical_reconstruction_limit_of_compressed_sensing_based_on_Lp-norm_minimization","234841064_Stein_COnsistent_Risk_Estimator_SCORE_for_hard_thresholding","245576839_On_the_Performance_of_Sparse_Recovery_via_L_p-minimization_0","242358078_The_Elements_Of_Statistical_Learning","239524362_Lifting_ell_q-optimization_thresholds"]}