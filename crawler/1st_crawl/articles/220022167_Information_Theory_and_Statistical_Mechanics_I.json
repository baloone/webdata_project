{"id":"220022167_Information_Theory_and_Statistical_Mechanics_I","abstract":"Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting \"subjective statistical mechanics,\" the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that could have been made on the basis of the information available.","authors":["Edwin T. Jaynes"],"meta":["May 1957Physical Review 106:620-630","DOI:10.1103/PhysRev.106.620"],"references":["258098337_Foundations_of_Statistical_Mechanics","252109114_Extension_of_the_Condensation_Theory_of_Yang_and_Lee_to_the_Pressure_Ensemble","235418978_A_Mathematical_Theory_of_Communication","226982374_The_general_statistical_problem_in_physics_and_the_theory_of_probability"]}