{"id":"305886295_Randomized_Prediction_Games_for_Adversarial_Machine_Learning","abstract":"In spam and malware detection, attackers exploit randomization to obfuscate malicious data and increase their chances of evading detection at test time, e.g., malware code is typically obfuscated using random strings or byte sequences to hide known exploits. Interestingly, randomization has also been proposed to improve security of learning algorithms against evasion attacks, as it results in hiding information about the classifier to the attacker. Recent work has proposed game-theoretical formulations to learn secure classifiers, by simulating different evasion attacks and modifying the classification function accordingly. However, both the classification function and the simulated data manipulations have been modeled in a deterministic manner, without accounting for any form of randomization. In this paper, we overcome this limitation by proposing a randomized prediction game, namely, a noncooperative game-theoretic formulation in which the classifier and the attacker make randomized strategy selections according to some probability distribution defined over the respective strategy set. We show that our approach allows one to improve the tradeoff between attack detection and false alarms with respect to the state-of-the-art secure classifiers, even against attacks that are different from those hypothesized during design, on application examples including handwritten digit recognition, spam, and malware detection.","authors":["Samuel Rota Bul√≤","Battista Biggio","Ignazio Pillai","Marcello Pelillo"],"meta":["August 2016IEEE Transactions on Neural Networks and Learning Systems 28(11):1-13","DOI:10.1109/TNNLS.2016.2593488","Project: Adversarial Machine Learning"],"references":["281736749_Cyberspace_Security_Using_Adversarial_Learning_and_Conformal_Prediction","271197324_Poisoning_Complete-Linkage_Hierarchical_Clustering","228982724_Feature_weighting_for_improved_classifier_robustness","228934695_Evaluation_of_classifiers_practical_considerations_for_security_applications","221654486_Adversarial_learning","221617993_Convex_Learning_with_Invariances","221609706_A_framework_for_quantitative_security_analysis_of_machine_learning","221022685_Secure_or_Insure_A_Game-Theoretic_Analysis_of_Information_Security_Games","220832101_Exploiting_Machine_Learning_to_Subvert_Your_Spam_Filter","220588703_Finite-Dimensional_Variational_Inequality_and_Nonlinear_Complementarity_Problems_A_Survey_of_Theory_Algorithms_and_Applications","220279290_Input_Space_Versus_Feature_Space_in_Kernel-Based_Methods","216792818_Comparison_of_learning_algorithms_for_handwritten_digit_recognition","324717611_Is_feature_selection_secure_against_training_data_poisoning","312538118_Support-vector_networks","307881957_LIBSVM_A_library_for_support_vector_machines","304069424_Student_research_highlight_Secure_and_resilient_distributed_machine_learning_under_adversarial_environments","287397306_Bayesian_games_for_adversarial_regression_problems","286370353_Practical_Evasion_of_a_Learning-Based_Classifier_A_Case_Study","273396404_Detection_in_Adversarial_Environments","271513141_Support_Vector_Networks","267092812_Existence_and_Uniqueness_of_Equilibrium_Points_for_Concave_N-Person_Games","266994455_Security_and_Game_Theory_Algorithms_Deployed_Systems_Lessons_Learned","266226977_A_Further_Generalization_of_the_Kakutani_Fixed_Point_Theorem_with_Application_to_Nash_Equilibrium_Points","263808304_Network_Security_-_A_Decision_and_Game-Theoretic_Approach","262410247_Static_Prediction_Games_for_Adversarial_Learning_Problems","262403920_Malicious_PDF_detection_using_metadata_and_structural_features","262347236_Conditional_Likelihood_Maximisation_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection","262173234_Adversarial_machine_learning","260635270_Security_Analytics_and_Measurements","259983458_Security_Evaluation_of_Support_Vector_Machines_in_Adversarial_Environments","256600830_Evasion_Attacks_against_Machine_Learning_at_Test_Time","256600633_Is_Data_Clustering_in_Adversarial_Settings_Secure","242370124_Polymorphic_Blending_Attacks","240383291_Security_Evaluation_of_Pattern_Classifiers_Under_Attack","239938882_Looking_at_the_bag_is_not_enough_to_find_the_bomb_An_evasion_of_structural_methods_for_malicious_PDF_files_detection","239938880_A_Pattern_Recognition_System_for_Malicious_PDF_Files_Detection","230872893_Theorie_und_Numerik_Restringierter_Optimierungsaufgaben","228715647_LIBSVM_A_library_for_support_vector_machines","228095591_Poisoning_Attacks_against_Support_Vector_Machines","226749770_Multiple_classifier_systems_for_robust_classifier_design_in_adversarial_environments","222727356_Modified_Descent_Methods_for_Solving_the_Monotone_Variational_Inequality_Problem","221654070_Stackelberg_games_for_adversarial_prediction_problems","221650864_On_Attacking_Statistical_Spam_Filters","221619407_Nash_Equilibria_of_Static_Prediction_Games","221611962_ANTIDOTE_understanding_and_defending_against_poisoning_of_anomaly_detectors","221609372_Can_machine_learning_be_secure","221345874_Nightmare_at_test_time_robust_learning_by_feature_deletion","221276067_Adversarial_Pattern_Classification_Using_Multiple_Classifiers_and_Randomisation","221038522_TREC_2007_spam_track_overview","220543428_Stackelberg_vs_Nash_in_Security_Games_An_Extended_Investigation_of_Interchangeability_Equivalence_and_Uniqueness","216792737_Scaling_learning_algorithms_towards_AI","41781474_Trading_Convexity_for_Scalability","4141044_Semantics-Aware_Malware_Detection","210303561_Learning_to_Classify_with_Missing_and_Corrupted_Features","3966042_Analysis_of_support_vector_machines"]}