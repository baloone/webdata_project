{"id":"323956667_Deep_Learning_using_Rectified_Linear_Units_ReLU","abstract":"We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $\\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \\max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\\hat{y}$ through argmax function, i.e. argmax $f(x)$.","authors":["Abien Fred Agarap"],"meta":["March 2018"],"references":["319312259_Fashion-MNIST_a_Novel_Image_Dataset_for_Benchmarking_Machine_Learning_Algorithms","305334401_Hierarchical_Attention_Networks_for_Document_Classification","279310175_Attention-Based_Models_for_Speech_Recognition","261094071_A_novel_approach_combining_recurrent_neural_network_and_support_vector_machines_for_time_series_classification","322672293_Parametric_Exponential_Linear_Unit_for_Deep_Convolutional_Neural_Networks","317016806_A_Neural_Network_Architecture_Combining_Gated_Recurrent_Unit_GRU_and_Support_Vector_Machine_SVM_for_Intrusion_Detection_in_Network_Traffic_Data","269935079_Adam_A_Method_for_Stochastic_Optimization","267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks","247931959_The_mnist_database_of_handwritten_digits","237000269_Deep_Learning_using_Linear_Support_Vector_Machines"]}