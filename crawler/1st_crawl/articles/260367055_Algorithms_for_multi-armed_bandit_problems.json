{"id":"260367055_Algorithms_for_multi-armed_bandit_problems","abstract":"Although many algorithms for the multi-armed bandit problem are\nwell-understood theoretically, empirical confirmation of their effectiveness is\ngenerally scarce. This paper presents a thorough empirical study of the most\npopular multi-armed bandit algorithms. Three important observations can be made\nfrom our results. Firstly, simple heuristics such as epsilon-greedy and\nBoltzmann exploration outperform theoretically sound algorithms on most\nsettings by a significant margin. Secondly, the performance of most algorithms\nvaries dramatically with the parameters of the bandit problem. Our study\nidentifies for each algorithm the settings where it performs well, and the\nsettings where it performs poorly. Thirdly, the algorithms' performance\nrelative each to other is affected only by the number of bandit arms and the\nvariance of the rewards. This finding may guide the design of subsequent\nempirical evaluations. In the second part of the paper, we turn our attention\nto an important area of application of bandit algorithms: clinical trials.\nAlthough the design of clinical trials has been one of the principal practical\nproblems motivating research on multi-armed bandits, bandit algorithms have\nnever been evaluated as potential treatment allocation strategies. Using data\nfrom a real study, we simulate the outcome that a 2001-2002 clinical trial\nwould have had if bandit algorithms had been used to allocate patients to\ntreatments. We find that an adaptive trial would have successfully treated at\nleast 50% more patients, while significantly reducing the number of adverse\neffects and increasing patient retention. At the end of the trial, the best\ntreatment could have still been identified with a high level of statistical\nconfidence. Our findings demonstrate that bandit algorithms are attractive\nalternatives to current adaptive treatment allocation strategies.","authors":["Volodymyr Kuleshov","Doina Precup"],"meta":["February 2014Journal of Machine Learning Research 1","SourcearXiv"],"references":["221112399_Bandit_Based_Monte-Carlo_Planning","220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem","220151284_Exploration-exploitation_tradeoff_using_variance_estimates_in_multi-armed_bandits","265467613_Contingency_Tables_Involving_Small_Numbers_and_The_Chi_Squared_Test","245264342_Allocation_Indices_for_Multi-Armed_Bandits","243765564_A_Class_of_Rapidly_Converging_Algorithms_for_Learning_Automata","239292007_Asymptotically_efficient_adaptive_allocation_rules1","235709752_A_Dynamic_Allocation_Index_for_the_Discounted_Multiarmed_Bandit_Problem","221678022_Individual_Choice_Behavior_A_Theoretical_Analysis","221345714_Finite-Time_Regret_Bounds_for_the_Multiarmed_Bandit_Problem"]}