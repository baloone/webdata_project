{"id":"343301801_BART_Denoising_Sequence-to-Sequence_Pre-training_for_Natural_Language_Generation_Translation_and_Comprehension","authors":["Mike Lewis","Yinhan Liu","Naman Goyal","Marjan Ghazvininejad"],"meta":["January 2020","DOI:10.18653/v1/2020.acl-main.703","Conference: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"],"references":["342257016_Leveraging_Pre-trained_Checkpoints_for_Sequence_Generation_Tasks","339887954_SpanBERT_Improving_Pre-training_by_Representing_and_Predicting_Spans","336868525_ALBERT_A_LITE_BERT_FOR_SELF-SUPERVISED_LEARNING_OF_LANGUAGE_REPRESENTATIONS","337653450_The_Second_Conversational_Intelligence_Challenge_ConvAI2","336998952_Text_Summarization_with_Pretrained_Encoders","334602160_Pre-trained_language_model_representations_for_language_generation","334115830_Don't_Give_Me_the_Details_Just_the_Summary_Topic-Aware_Convolutional_Neural_Networks_for_Extreme_Summarization","325447291_A_Broad-Coverage_Challenge_Corpus_for_Sentence_Understanding_through_Inference","319770439_Efficient_Estimation_of_Word_Representations_in_Vector_Space","318740838_Get_To_The_Point_Summarization_with_Pointer-Generator_Networks","317558625_Attention_Is_All_You_Need","312250707_SQuAD_100000_Questions_for_Machine_Comprehension_of_Text","306093962_Edinburgh_Neural_Machine_Translation_Systems_for_WMT_16","278048272_Teaching_Machines_to_Read_and_Comprehend","221251113_The_Winograd_Schema_Challenge"]}