{"id":"347811904_Mastering_Atari_Go_chess_and_shogi_by_planning_with_a_learned_model","abstract":"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess¹ and Go², where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games³—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled⁴—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm⁵ that was supplied with the rules of the game.","authors":["Julian Schrittwieser","Ioannis Antonoglou","Thomas Hubert","Karen Simonyan"],"meta":["December 2020Nature 588(7839):604-609","DOI:10.1038/s41586-020-03051-4"],"references":["336911787_Grandmaster_level_in_StarCraft_II_using_multi-agent_reinforcement_learning","324066687_Planning_chemical_syntheses_with_deep_neural_networks_and_symbolic_AI","323027085_Learning_and_Querying_Fast_Generative_Models_for_Reinforcement_Learning","340285012_Recurrent_Experience_Replay_in_Distributed_Reinforcement_Learning","337393821_TreeqN_and_ATreEC_Differentiable_tree-structured_models_for_deep_reinforcement_learning","335624643_Multi-Task_Deep_Reinforcement_Learning_with_PopArt","329473176_A_general_reinforcement_learning_algorithm_that_masters_chess_shogi_and_Go_through_self-play","323550003_Distributed_Prioritized_Experience_Replay","322950202_IMPALA_Scalable_Distributed_Deep-RL_with_Importance_Weighted_Actor-Learner_Architectures","321878591_Superhuman_AI_for_heads-up_no-limit_poker_Libratus_beats_top_professionals","320754952_TreeQN_and_ATreeC_Differentiable_Tree_Planning_for_Deep_Reinforcement_Learning","320473480_Mastering_the_game_of_Go_without_human_knowledge","320280165_Rainbow_Combining_Improvements_in_Deep_Reinforcement_Learning","319895937_Revisiting_the_Arcade_Learning_Environment_Evaluation_Protocols_and_Open_Problems_for_General_Agents","319770414_Identity_Mappings_in_Deep_Residual_Networks"]}