{"id":"318560334_Imagination-Augmented_Agents_for_Deep_Reinforcement_Learning","abstract":"We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.","authors":["Theophane Weber","Sébastien Racanière","David P. Reichert","Lars Buesing"],"meta":["July 2017"],"references":["318560285_Learning_model-based_planning_from_scratch","310673522_A_Deep_Learning_Approach_for_Joint_Video_Frame_and_Reward_Prediction_in_Atari_Games","310440792_Reinforcement_Learning_with_Unsupervised_Auxiliary_Tasks","319769991_Trust_Region_Policy_Optimization","318829926_Value_Iteration_Networks","318694645_Deep_visual_foresight_for_planning_robot_motion","318392190_Value_Prediction_Network","318360327_Imitation_from_Observation_Learning_to_Imitate_Behaviors_from_Raw_Video_via_Context_Translation","317062142_Model-Based_Planning_in_Discrete_Action_Spaces","314237506_Virtual_vs_Real_Trading_Off_Simulations_and_Physical_Experiments_in_Reinforcement_Learning_with_Bayesian_Optimization"]}