{"id":"351566605_Information_Structures_for_Causally_Explainable_Decisions","abstract":"For an AI agent to make trustworthy decision recommendations under uncertainty on behalf of human principals, it should be able to explain why its recommended decisions make preferred outcomes more likely and what risks they entail. Such rationales use causal models to link potential courses of action to resulting outcome probabilities. They reflect an understanding of possible actions, preferred outcomes, the effects of action on outcome probabilities, and acceptable risks and trade-offs—the standard ingredients of normative theories of decision-making under uncertainty, such as expected utility theory. Competent AI advisory systems should also notice changes that might affect a user’s plans and goals. In response, they should apply both learned patterns for quick response (analogous to fast, intuitive “System 1” decision-making in human psychology) and also slower causal inference and simulation, decision optimization, and planning algorithms (analogous to deliberative “System 2” decision-making in human psychology) to decide how best to respond to changing conditions. Concepts of conditional independence, conditional probability tables (CPTs) or models, causality, heuristic search for optimal plans, uncertainty reduction, and value of information (VoI) provide a rich, principled framework for recognizing and responding to relevant changes and features of decision problems via both learned and calculated responses. This paper reviews how these and related concepts can be used to identify probabilistic causal dependencies among variables, detect changes that matter for achieving goals, represent them efficiently to support responses on multiple time scales, and evaluate and update causal models and plans in light of new data. The resulting causally explainable decisions make efficient use of available information to achieve goals in uncertain environments.","authors":["Louis Anthony Tony Cox"],"meta":["May 2021Entropy 23(5):601","DOI:10.3390/e23050601"],"references":["348664012_Constrained_Risk-Averse_Markov_Decision_Processes","339427775_Safe_Exploration_for_Optimization_with_Gaussian_Processes","336307894_Information_Theoretic_Causal_Effect_Quantification","335941254_An_estimation_of_causal_structure_based_on_Latent_LiNGAM_for_mixed_data","328734051_Explaining_Explanations_in_AI","326660433_A_Deep_Hierarchical_Reinforcement_Learning_Algorithm_in_Partially_Observable_Markov_Decision_Processes","343247753_Risk-Averse_Planning_Under_Uncertainty","342099478_Visualizing_the_effects_of_predictor_variables_in_black_box_supervised_learning_models","341085015_Linear_Thompson_Sampling_Under_Unknown_Linear_Constraints","333597288_Causal_Interpretations_of_Black-Box_Models"]}