{"id":"343027526_Automated_Classification_of_Blood_Loss_from_Transurethral_Resection_of_the_Prostate_Surgery_Videos_Using_Deep_Learning_Technique","abstract":"Transurethral resection of the prostate (TURP) is a surgical removal of obstructing prostate tissue. The total bleeding area is used to determine the performance of the TURP surgery. Although the traditional method for the detection of bleeding areas provides accurate results, it cannot detect them in time for surgery diagnosis. Moreover, it is easily disturbed to judge bleeding areas for experienced physicians because a red light pattern arising from the surgical cutting loop often appears on the images. Recently, the automatic computer-aided technique and artificial intelligence deep learning are broadly used in medical image recognition, which can effectively extract the desired features to reduce the burden of physicians and increase the accuracy of diagnosis. In this study, we integrated two state-of-the-art deep learning techniques for recognizing and extracting the red light areas arising from the cutting loop in the TURP surgery. First, the ResNet-50 model was used to recognize the red light pattern appearing in the chipped frames of the surgery videos. Then, the proposed Res-Unet model was used to segment the areas with the red light pattern and remove these areas. Finally, the hue, saturation, and value color space were used to classify the four levels of the blood loss under the circumstances of non-red light pattern images. The experiments have shown that the proposed Res-Unet model achieves higher accuracy than other segmentation algorithms in classifying the images with the red and non-red lights, and is able to extract the red light patterns and effectively remove them in the TURP surgery images. The proposed approaches presented here are capable of obtaining the level classifications of blood loss, which are helpful for physicians in diagnosis.","authors":["Jian-Wen Chen","Wan-Ju Lin","Chun-Yuan Lin","Che-Lun Hung"],"meta":["July 2020Applied Sciences 10(14):4908","DOI:10.3390/app10144908"],"references":["338417713_Breast_Cancer_Image_Classification_via_Multi-Network_Features_and_Dual-Network_Orthogonal_Low-Rank_Learning","336571637_A_Deep_Learning_Ensemble_Approach_for_Diabetic_Retinopathy_Detection","334776073_Moving_Object_Detection_Method_via_ResNet-18_With_Encoder-Decoder_Structure_in_Complex_Scenes","334751402_Data_Driven_Intelligent_Diagnostics_for_Parkinson's_Disease","334287825_A_Stacked_Multi-Connection_Simple_Reducing_Net_for_Brain_Tumor_Segmentation","333131201_Fig_Plant_Segmentation_from_Aerial_Images_Using_a_Deep_Convolutional_Encoder-Decoder_Network","331385680_Crop_and_Weeds_Classification_for_Precision_Agriculture_Using_Context-Independent_Pixel-Wise_Segmentation","334407912_Automatic_Cataract_Classification_Using_Deep_Neural_Network_With_Discrete_State_Transition","333408310_Fine-Grained_Classification_of_Cervical_Cells_Using_Morphological_and_Appearance_Based_Convolutional_Neural_Networks","332374997_Automatic_Classification_of_Chinese_Herbal_Based_on_Deep_Learning_Method"]}