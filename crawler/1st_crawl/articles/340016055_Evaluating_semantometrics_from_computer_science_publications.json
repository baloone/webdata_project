{"id":"340016055_Evaluating_semantometrics_from_computer_science_publications","abstract":"Identification of important works and assessment of importance of publications in vast scientific corpora are challenging yet common tasks subjected by many research projects. While the influence of citations in finding seminal papers has been analysed thoroughly, citation-based approaches come with several problems. Their impracticality when confronted with new publications which did not yet receive any citations, area-dependent citation practices and different reasons for citing are only a few drawbacks of them. Methods relying on more than citations, for example semantic features such as words or topics contained in publications of citation networks, are regarded with less vigour while providing promising preliminary results. In this work we tackle the issue of classifying publications with their respective referenced and citing papers as either seminal, survey or uninfluential by utilising semantometrics. We use distance measures over words, semantics, topics and publication years of papers in their citation network to engineer features on which we predict the class of a publication. We present the SUSdblp dataset consisting of 1980 labelled entries to provide a means of evaluating this approach. A classification accuracy of up to .9247 was achieved when combining multiple types of features using semantometrics. This is +.1232 compared to the current state of the art (SOTA) which uses binary classification to identify papers from classes seminal and survey. The utilisation of one-vector representations for the ternary classification task resulted in an accuracy of .949 which is +.1475 compared to the binary SOTA. Classification based on information available at publication time derived with semantometrics resulted in an accuracy of .8152 while an accuracy of .9323 could be achieved when using one-vector representations.","authors":["Christin Katharina Kreutz","Premtim Sahitaj","Ralf Schenkel"],"meta":["March 2020Scientometrics 125(3)","DOI:10.1007/s11192-020-03409-5"],"references":["318741781_Pulling_Out_the_Stops_Rethinking_Stopword_Removal_for_Topic_Models","318418494_Incidental_or_influential_-_Challenges_in_automatically_detecting_citation_importance_using_publication_full_texts","311491690_WSDM_Cup_2016_Entity_Ranking_Challenge","330169734_ha_An_index_to_quantify_an_individual's_scientific_leadership","323373287_Some_Common_Mistakes_In_IR_Evaluation_And_How_They_Can_Be_Avoided","322609308_Audience_Based_View_of_Publication_Impact","319421450_Incidental_or_Influential_-_Challenges_in_Automatically_Detecting_Citation_Importance_Using_Publication_Full_Texts","317145684_Citations_and_Readership_are_Poor_Indicators_of_Research_Excellence_Introducing_TrueImpactDataset_a_New_Dataset_for_Validating_Research_Evaluation_Metrics","313383979_Dynamic_topic_models","313005015_The_Kolmogorov-Smirnov_test_of_goodness_of_fit"]}