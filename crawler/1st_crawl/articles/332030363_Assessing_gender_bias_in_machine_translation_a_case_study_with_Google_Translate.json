{"id":"332030363_Assessing_gender_bias_in_machine_translation_a_case_study_with_Google_Translate","abstract":"Recently there has been a growing concern in academia, industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias, where trained statistical models—unbeknownst to their creators—grow to reflect controversial societal asymmetries, such as gender or racial bias. A significant number of Artificial Intelligence tools have recently been suggested to be harmfully biased toward some minority, with reports of racist criminal behavior predictors, Apple’s Iphone X failing to differentiate between two distinct Asian people and the now infamous case of Google photos’ mistakenly classifying black people as gorillas. Although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in AI. In this paper, we start with a comprehensive list of job positions from the U.S. Bureau of Labor Statistics (BLS) and used it in order to build sentences in constructions like “He/She is an Engineer” (where “Engineer” is replaced by the job position of interest) in 12 different gender neutral languages such as Hungarian, Chinese, Yoruba, and several others. We translate these sentences into English using the Google Translate API, and collect statistics about the frequency of female, male and gender neutral pronouns in the translated output. We then show that Google Translate exhibits a strong tendency toward male defaults, in particular for fields typically associated to unbalanced gender distribution or stereotypes such as STEM (Science, Technology, Engineering and Mathematics) jobs. We ran these statistics against BLS’ data for the frequency of female participation in each job position, in which we show that Google Translate fails to reproduce a real-world distribution of female workers. In summary, we provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, Google Translate yields male defaults much more frequently than what would be expected from demographic data alone. We believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques—which can already be found in the scientific literature.","authors":["Marcelo O. R. Prates","Pedro Henrique da Costa Avelar","Luís C. Lamb"],"meta":["May 2020Neural Computing and Applications 32(1)","DOI:10.1007/s00521-019-04144-6","Projects: Human and Social-Problem Solving in Complex NetworksMachine Learning and Reasoning"],"references":["321602721_Recent_Advances_in_Example-Based_Machine_Translation","322871591_Deep_neural_networks_are_more_accurate_than_humans_at_detecting_sexual_orientation_from_facial_images","318015716_On_Chomsky_and_the_Two_Cultures_of_Statistical_Learning","312609380_Sex_syntax_and_semantics","312080801_Racist_in_the_Machine_The_Disturbing_Implications_of_Algorithmic_Bias","310235569_Google's_Multilingual_Neural_Machine_Translation_System_Enabling_Zero-Shot_Translation","309879022_Multi-Way_Multilingual_Neural_Machine_Translation","308571510_Battling_algorithmic_bias_how_do_we_ensure_algorithms_treat_us_fairly","307174794_Statistical_Machine_Translation","305997939_Algorithmic_Bias_From_Discrimination_Discovery_to_Fairness-aware_Data_Mining"]}