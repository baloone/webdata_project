{"id":"220695762_InformationTheory_Inference_and_Learning_Algorithms","abstract":"Best known in our circles for his key role in the renaissance of low- density parity-check (LDPC) codes, David MacKay has written an am- bitious and original textbook. Almost every area within the purview of these TRANSACTIONS can be found in this book: data compression al- gorithms, error-correcting codes, Shannon theory, statistical inference, constrained codes, classification, and neural networks. The required mathematical level is rather minimal beyond a modicum of familiarity with probability. The author favors exposition by example, there are few formal proofs, and chapters come in mostly self-contained morsels richly illustrated with all sorts of carefully executed graphics. With its breadth, accessibility, and handsome design, this book should prove to be quite popular. Highly recommended as a primer for students with no background in coding theory, the set of chapters on error-correcting codes are an excellent brief introduction to the elements of modern sparse-graph codes: LDPC, turbo, repeat-accumulate, and fountain codes are de- scribed clearly and succinctly. As a result of the author's research on the field, the nine chapters on neural networks receive the deepest and most cohesive treatment in the book. Under the umbrella title of Probability and Inference we find a medley of chapters encompassing topics as varied as the Viterbi algorithm and the forward-backward algorithm, Monte Carlo simu- lation, independent component analysis, clustering, Ising models, the saddle-point approximation, and a sampling of decision theory topics. The chapters on data compression offer a good coverage of Huffman and arithmetic codes, and we are rewarded with material not usually encountered in information theory textbooks such as hash codes and efficient representation of integers. The expositions of the memoryless source coding theorem and of the achievability part of the memoryless channel coding theorem stick closely to the standard treatment in (1), with a certain tendency to over- simplify. For example, the source coding theorem is verbalized as: \" i.i.d. random variables each with entropy can be compressed into more than bits with negligible risk of information loss, as ; conversely if they are compressed into fewer than bits it is virtually certain that informa- tion will be lost.\" Although no treatment of rate-distortion theory is offered, the author gives a brief sketch of the achievability of rate with bit- error rate , and the details of the converse proof of that limit are left as an exercise. Neither Fano's inequality nor an operational definition of capacity put in an appearance. Perhaps his quest for originality is what accounts for MacKay's pro- clivity to fail to call a spade a spade. Almost-lossless data compres- sion is called \"lossy compression;\" a vanilla-flavored binary hypoth-","authors":["David J. C. MacKay"],"meta":["January 2003IEEE Transactions on Information Theory 50(10)","DOI:10.1109/TIT.2004.834752","SourceDBLP","Publisher: Cambridge University PressISBN: 978-0-521-64298-9"],"references":["220685038_Improved_low-density_parity-check_codes_using_irregular_graphs","49459301_Fast_Forward_Selection_to_Speed_Up_Sparse_Gaussian_Process_Regression","3080331_Efficient_erasure_correcting_codes_IEEE_Trans_Inf_Theory","3080328_The_Capacity_of_Low-Density_Parity_Check_Codes_under_Message-Passing_Decoding","3080325_Design_of_capacity-approaching_irregular_low-density_parity-check_codes","3080314_Analysis_of_Sum-Product_Decoding_of_Low-Density_Parity-Check_Codes_Using_a_Gaussian_Approximation","2621351_An_Algebraic_Description_Of_Iterative_Decoding_Schemes","2367988_Bayesian_Parameter_Estimation_Via_Variational_Methods","2337096_Non-linear_Bayesian_Image_Modelling","265369049_Slice_sampling_With_discussions_and_rejoinder","261662232_Elements_of_Information_Theory","224773133_Elements_of_information_theory_2nd_ed","221620391_Generalized_Belief_Propagation","221617820_Bayesian_Monte_Carlo","220876997_Dasher---a_data_entry_interface_using_continuous_gestures_and_language_models","220681289_Codes_on_graphs_Normal_realizations","51993442_A_Mathematical_Theory_of_Communication","40498233_Gaussian_Processes_for_Classification_Mean-Field_Algorithms","11294743_CCCP_Algorithms_to_Minimize_the_Bethe_and_Kikuchi_Free_Energies_Convergent_Alternatives_to_Belief_Propagation","11207765_ARTICLE_Training_Products_of_Experts_by_Minimizing_Contrastive_Divergence","3302847_Variational_Gaussian_process_classifiers","3084716_Tree-based_reparameterization_framework_for_analysis_of_sum-product_and_related_algorithms","3080334_Factor_Graphs_and_the_Sum-Product_Algorithm","2554295_A_New_View_of_ICA","2538401_Infinite_Mixtures_of_Gaussian_Process_Experts","2418858_BSC_Thresholds_For_Code_Ensembles_Based_On_Typical_Pairs_Decoding","2306327_Annealed_Importance_Sampling"]}