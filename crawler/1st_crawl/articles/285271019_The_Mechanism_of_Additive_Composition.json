{"id":"285271019_The_Mechanism_of_Additive_Composition","abstract":"We prove an upper bound for the bias of additive composition (Foltz et al.,\n1998; Landauer and Dutnais, 1997; Mitchell and Lapata, 2010), a widely used\nmethod for computing meanings of phrases by averaging the vector\nrepresentations of their constituent words. The result endorses additive\ncomposition as a reasonable operation for calculating meanings of phrases,\nwhich is the first theoretical analysis on compositional frameworks from a\nmachine learning point of view. The theory also suggests ways to improve\nadditive compositionality, including: transforming entries of distributional\nword vectors by a function that meets a specific condition, constructing a\nnovel type of vector representations to make additive composition sensitive to\nword order, and utilizing singular value decomposition to train word vectors.","authors":["Ran Tian","Naoaki Okazaki","Kentaro Inui"],"meta":["July 2017Machine Learning 106(4)","DOI:10.1007/s10994-017-5634-8","SourcearXiv"],"references":["313170906_Word_association_norms_mutual_information_and_lexicography","306093885_Learning_Semantically_and_Additively_Compositional_Distributional_Representations","329977655_A_Latent_Variable_Model_Approach_to_PMI-based_Word_Embeddings","329974677_Word_Embeddings_as_Metric_Recovery_in_Semantic_Spaces","319770369_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality","311207397_Intensionality_was_only_alleged_On_adjective-noun_composition_in_distributional_semantics","308797129_Distributional_structure","307955489_Distributed_representations_of_words_and_phrases_and_their_compositionality","306093956_Composing_Distributed_Representations_of_Relational_Patterns","306093526_context2vec_Learning_Generic_Context_Embedding_with_Bidirectional_LSTM"]}