{"id":"301419711_Model_Inversion_Attacks_that_Exploit_Confidence_Information_and_Basic_Countermeasures","abstract":"Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.","authors":["Matt Fredrikson","Somesh Jha","Thomas Ristenpart"],"meta":["October 2015","DOI:10.1145/2810103.2813677","Conference: the 22nd ACM SIGSAC Conference"],"references":["269326213_Open_source_biometric_recognition","261336897_Learning_hierarchical_representations_for_face_verification_with_convolutional_deep_belief_networks","255983764_Pylearn2_A_machine_learning_research_library","221654486_Adversarial_learning","221609194_Learning_your_identity_and_disease_from_research_papers_Information_leaks_in_genome_wide_association_study","221559640_Revealing_Information_while_Preserving_Privacy","221344904_Convolutional_deep_belief_networks_for_scalable_unsupervised_learning_of_hierarchical_representations","44572873_The_disclosure_of_diagnosis_codes_can_breach_research_participants'_privacy","26761503_Genomic_privacy_and_limits_of_individual_detection_in_a_pool","24026511_Estimation_of_the_Warfarin_Dose_with_Clinical_and_Pharmacogenetic_Data","23236143_Resolving_Individuals_Contributing_Trace_Amounts_of_DNA_to_Highly_Complex_Mixtures_Using_High-Density_SNP_Genotyping_Microarrays","4250679_Secure_Authentication_for_Face_Recognition","322276713_The_openCV_library","301534715_Privacy_in_Pharmacogenetics_An_End-to-End_Case_Study_of_Personalized_Warfarin_Dosing","272149731_Pylearn2_a_machine_learning_research_library","267716853_Simple_Demographics_Often_Identify_People_Uniquely","262356297_Membership_privacy_A_unifying_framework_for_privacy_definitions","262209585_Learning_hierarchical_representations_for_face_verification_with_convolutional_deep_belief_networks","232063366_The_Power_of_Linear_Reconstruction_Attacks","230697972_Individualized_patient-centered_lifestyle_recommendations_An_expert_system_for_communicating_patient_specific_cardiovascular_risk_information_and_prioritizing_lifestyle_options","225451636_Differential_Privacy","221654433_Personal_privacy_vs_population_privacy_Learning_to_attack_anonymization","221609372_Can_machine_learning_be_secure","221591329_The_price_of_privately_releasing_contingency_tables_and_the_spectra_of_random_matrices_with_correlated_rows","221590176_The_price_of_privacy_and_the_limits_of_LP_decoding","6097065_Social_Science_Research","4339941_Robust_De-anonymization_of_Large_Sparse_Datasets"]}