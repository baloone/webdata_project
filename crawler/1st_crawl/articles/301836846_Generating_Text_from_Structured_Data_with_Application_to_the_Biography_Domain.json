{"id":"301836846_Generating_Text_from_Structured_Data_with_Application_to_the_Biography_Domain","abstract":"This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magni- tude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocab- ulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text genera- tion. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that trans- fer sample-specific words from the in- put database to the generated output sen- tence. Our neural model significantly out- performs a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.","authors":["Remi Lebret","David Grangier","Michael Auli"],"meta":["March 2016"],"references":["319770160_Show_and_Tell_A_Neural_Image_Caption_Generator","308809117_From_captions_to_visual_concepts_and_back","307747289_Show_and_tell_A_neural_image_caption_generator","305334305_What_to_talk_about_and_how_Selective_Generation_using_LSTMs_with_Coarse-to-Fine_Alignment","281487096_What_to_talk_about_and_how_Selective_Generation_using_LSTMs_with_Coarse-to-Fine_Alignment","273388012_Neural_Responding_Machine_for_Short-Text_Conversation","272194766_Show_Attend_and_Tell_Neural_Image_Caption_Generation_with_Visual_Attention","308813785_Deep_visual-semantic_alignments_for_generating_image_descriptions","289176734_Word_Embeddings_through_Hellinger_PCA","283334712_Attention_with_Intention_for_a_Neural_Network_Conversation_Model"]}