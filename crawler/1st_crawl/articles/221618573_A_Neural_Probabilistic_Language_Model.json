{"id":"221618573_A_Neural_Probabilistic_Language_Model","abstract":"A goal of statistical language modeling is to learn the joint probabilit y function of sequences of words. This is intrinsically difficult because o f the curse of dimensionality: we propose to fight it with its own weap ons. In the proposed approach one learns simultaneously (1) a distributed r ep- resentation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these repr e- sentations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, sh owing on two text corpora that the proposed approach very significantly im- proves on a state-of-the-art trigram model.","authors":["Y. Bengio","RÃ©jean Ducharme","Pascal Vincent"],"meta":["January 2000Journal of Machine Learning Research 3(6):932-938","DOI:10.1162/153244303322533223","SourceDBLP","Conference: Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS) 2000, Denver, CO, USA"],"references":["244436420_New_distributed_probabilistic_language_models","312536675_Self-organizing_letter_code-book_for_text-to-phoneme_neural_network_model","307174896_An_Empirical_Study_of_Smoothing_Techniques_for_Language_Modeling","303802749_Indexing_by_Latent_Semantic_Analysis","260480187_MPI_A_Message-Passing_Interface_Standard","230876151_Interpolated_Estimation_of_Markov_Source_Parameters_from_Sparse_Data","230854795_WordNet_-_An_Electronical_Lexical_Database","228057706_Indexing_By_Latent_Semantic_Analysis","222452871_Natural_Language_Processing_With_Modular_Pdp_Networks_and_Distributed_Lexicon","222449846_Finding_Structure_in_Time"]}