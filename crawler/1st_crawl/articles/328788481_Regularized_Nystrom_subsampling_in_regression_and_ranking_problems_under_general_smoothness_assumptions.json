{"id":"328788481_Regularized_Nystrom_subsampling_in_regression_and_ranking_problems_under_general_smoothness_assumptions","abstract":"In the supervised learning, the Nyström type subsampling is considered as a tool for reducing the computational complexity of regularized kernel methods in the big data setting. Up to now, the theoretical analysis of this approach has been done almost exclusively in the context of the regression learning and in the case where the smoothness of the target functions is restricted to the Hölder type source conditions. Such conditions do not cover the case of target functions with high and low smoothness, which are also of practical interest. Moreover, in the case of the Hölder source conditions, there is no need to consider a regularization with high enough qualification because order-optimal learning rates are achieved by the simple Tikhonov regularization known also as the kernel ridge regression. At the same time, this learning method does not improve its performance for any smoothness higher than Hölder ones. Therefore, in this paper, our goal is to extend previous analysis of the Nyström type subsampling to the case of the general source conditions, and to the regularization schemes with high enough qualification. We also show that under rather natural assumption, our results can be easily reformulated in the ranking learning setting.","authors":["G. L. Myleiko","S. Pereverzyev","S. G. Solodky"],"meta":["November 2018Analysis and Applications 17(03)","DOI:10.1142/S021953051850029X"],"references":["317796796_Nystrom_type_subsampling_analyzed_as_a_regularized_projection","316902965_Learning_Theory_of_Distributed_Spectral_Algorithms","275072617_Convergence_Analysis_of_an_Empirical_Eigenfunction-Based_Ranking_Algorithm_with_Truncated_Sparsity","272845294_Online_Pairwise_Learning_Algorithms_with_Kernels","267557636_Cross-validation_based_Adaptation_for_Regularization_Operators_in_Learning","231142756_How_general_are_general_source_conditions","226880815_Learning_Theory_Estimates_via_Integral_Operators_and_Their_Approximations","225480031_Optimal_Rates_for_the_Regularized_Least-Squares_Algorithm","38003131_Adaptive_Kernel_Methods_Using_the_Balancing_Principle","316052365_Learning_Rates_for_Regularized_Least_Squares_Ranking_Algorithm","315660316_Optimal_Rates_for_the_Regularized_Learning_Algorithms_under_General_Source_Condition","283947518_On_the_convergence_rate_and_some_applications_of_regularized_ranking_algorithms","283444256_A_linear_functional_strategy_for_regularized_ranking","278794994_Aggregation_of_regularized_solutions_from_multiple_observation_models","266194992_Regularization_Theory_for_Ill-posed_Problems_Selected_Topics","262368982_Full_length_article_The_convergence_rate_of_a_regularized_ranking_algorithm","243648706_A_general_method_of_constructing_regularizing_algorithms_for_a_linear_ill-posed_equation_in_Hilbert_space","230950743_Geometry_of_linear_ill-posed_problems_in_variable_Hilbert_scales","228942160_Discretization_strategy_for_linear_ill-posed_problems_in_variable_Hilbert_scales","225215908_On_the_regularization_of_projection_methods_for_solving_III-posed_problems","224040099_Learning_Sets_with_Separating_Kernels","222839718_On_regularization_algorithms_in_learning_theory","221995160_Theory_of_reproducing_kernels","221345831_Magnitude-preserving_ranking_algorithms","51893594_Learning_to_Order_Things"]}