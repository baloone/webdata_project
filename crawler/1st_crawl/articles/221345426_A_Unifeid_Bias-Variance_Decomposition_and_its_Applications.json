{"id":"221345426_A_Unifeid_Bias-Variance_Decomposition_and_its_Applications","abstract":"This paper presents a unied bias-variance decomposition that is applicable to squared loss, zero-one loss, variable misclassication costs, and other loss functions. The unied decomposition sheds light on a number of sig- nican t issues: the relation between some of the previously-proposed decompositions for zero-one loss and the original one for squared loss, the relation between bias, variance and Schapire et al.'s (1997) notion of margin, and the nature of the trade-o between bias and variance in classication. While the bias- variance behavior of zero-one loss and vari- able misclassication costs is quite dieren t from that of squared loss, this dierence de- rives directly from the dieren t denitions of loss. We have applied the proposed decom- position to decision tree learning, instance- based learning and boosting on a large suite of benchmark data sets, and made several sig- nican t observations.","authors":["Pedro Domingos"],"meta":["January 2000","SourceDBLP","Conference: Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000), Stanford University, Stanford, CA, USA, June 29 - July 2, 2000"],"references":["221344849_Boosting_the_margin_A_new_explanation_for_the_effectiveness_of_voting_methods","220499843_Neural_Networks_and_the_BiasVariance_Dilemma","220451897_On_Bias_Variance_01-Loss_and_the_Curse-of-Dimensionality","282707871_Nearest_Neighbor_Pattern_Classification","248192679_UCI_Repository_of_Machine_Learning_Databases","226270700_Bagging_Predictors","220688794_C45_Programs_for_Machine_Learning","220049076_An_Introduction_To_The_Bootstrap","2846430_A_Unified_Bias-Variance_Decomposition_and_its_Applications","2826687_Bias_Variance_and_Prediction_Error_for_Classification_Rules"]}