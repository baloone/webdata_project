{"id":"267665795_Understanding_machine_learning_From_theory_to_algorithms","abstract":"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.","authors":["Shai Shalev-Shwartz","Shai Ben-David"],"meta":["January 2013","DOI:10.1017/CBO9781107298019"],"references":["237092232_Efficient_Classification_for_Metric_Data","228574972_Stability_results_in_learning_theory","225680384_Greedy_adaptive_approximation","220390884_Learning_theory_Stability_is_sufficient_for_generalization_and_necessary_and_sufficient_for_consistency_of_empirical_risk_minimization","220343561_The_Robustness_of_the_p-Norm_Algorithms","38358309_A_Finite_Sample_Distribution-Free_Performance_Bound_for_Local_Discrimination_Rules","3593599_Scale-sensitive_dimensions_uniform_convergence_and_learnability","313605606_Aggregating_strategies","242613831_The_Relaxation_Method_for_Linear_Inequalities","238806442_A_note_on_resolving_infeasibility_in_linear_programs_by_constraint_relaxation","234778795_Learnability_Stability_and_Uniform_Convergence","226852172_Uniform_and_universal_Glivenko-Cantelli_classes","226319248_On_Learning_Sets_and_Functions","225171987_Rademacher_and_Gaussian_Complexities_Risk_Bounds_and_Structural_Results","223598227_Combinatorial_variability_of_Vapnik-Chervonenkis_classes_with_applications_to_sample_compression_schemes","222745231_Hardness_Results_for_Neural_Network_Approximation_Problems","222505227_A_generalization_of_Sauer's_lemma","222441166_Characterizations_of_Learnability_for_Classes_of_0_n-Valued_Functions","222400076_On_the_Difficulty_of_Approximately_Maximizing_Agreements","221497619_Learnability_of_Bipartite_Ranking_Functions","221346105_SVM_optimization_Inverse_dependence_on_training_set_size","38362268_Distribution_Inequalities_for_the_Binomial_Law","38361216_Universal_Donsker_Classes_and_Metric_Entropy","2834006_PAC-Bayes_margins","2573396_PAC-Bayesian_model_averaging","2525851_PAC-Bayesian_Generalization_Error_Bounds_for_Gaussian_Process_Classification","2373554_Rademacher_Processes_And_Bounding_The_Risk_Of_Function_Learning"]}