{"id":"349575047_First_return_then_explore","abstract":"Reinforcement learning promises to solve complex sequential-decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse1 and deceptive2 feedback. Avoiding these pitfalls requires a thorough exploration of the environment, but creating algorithms that can do so remains one of the central challenges of the field. Here we hypothesize that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (detachment) and failing to first return to a state before exploring from it (derailment). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly ‘remembering’ promising states and returning to such states before intentionally exploring. Go-Explore solves all previously unsolved Atari games and surpasses the state of the art on all hard-exploration games1, with orders-of-magnitude improvements on the grand challenges of Montezuma’s Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore’s exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration—an insight that may prove critical to the creation of truly intelligent learning agents. A reinforcement learning algorithm that explicitly remembers promising states and returns to them as a basis for further exploration solves all as-yet-unsolved Atari games and out-performs previous algorithms on Montezuma’s Revenge and Pitfall.","authors":["Adrien Ecoffet","Joost Huizinga","Joel Lehman","Kenneth O. Stanley"],"meta":["February 2021Nature 590(7847):580-586","DOI:10.1038/s41586-020-03157-9"],"references":["342800521_Exploration_Based_Language_Learning_for_Text-Based_Games","337345997_Learning_dexterous_in-hand_manipulation","336911787_Grandmaster_level_in_StarCraft_II_using_multi-agent_reinforcement_learning","326741033_Sim-to-Real_Learning_Agile_Locomotion_For_Quadruped_Robots","323694489_The_Surprising_Creativity_of_Digital_Evolution_A_Collection_of_Anecdotes_from_the_Evolutionary_Computation_and_Artificial_Life_Research_Communities","330708842_Object_Detection_With_Deep_Learning_A_Review","327808270_Sim-to-Real_Transfer_of_Robotic_Control_with_Dynamics_Randomization","323550003_Distributed_Prioritized_Experience_Replay","323410413_Multi-Goal_Reinforcement_Learning_Challenging_Robotics_Environments_and_Request_for_Research","322950202_IMPALA_Scalable_Distributed_Deep-RL_with_Importance_Weighted_Actor-Learner_Architectures","321902735_Improving_Exploration_in_Evolution_Strategies_for_Deep_Reinforcement_Learning_via_a_Population_of_Novelty-Seeking_Agents","321374980_Deep_Reinforcement_Learning_for_De-Novo_Drug_Design","320473480_Mastering_the_game_of_Go_without_human_knowledge","320743072_Autoencoder-augmented_neuroevolution_for_visual_doom_playing","319898160_The_Uncertainty_Bellman_Equation_and_Exploration"]}