{"id":"327812188_Linguistic_Unit_Discovery_from_Multi-Modal_Inputs_in_Unwritten_Languages_Summary_of_the_Speaking_Rosetta_JSALT_2017_Workshop","authors":["Odette Scharenborg","Laurent Besacier","Alan Black","Mark Hasegawa-Johnson"],"meta":["April 2018","DOI:10.1109/ICASSP.2018.8461761","Conference: ICASSP 2018 - 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"],"references":["321768375_The_Zero_Resource_Speech_Challenge_2017","320322031_A_Very_Low_Resource_Language_Speech_Corpus_for_Computational_Language_Documentation_Experiments","320030446_SPEECH-COCO_600k_Visually_Grounded_Spoken_Captions_Aligned_to_MSCOCO_Data_Set","319896363_Unwritten_Languages_Demand_Attention_Too_Word_Discovery_with_Encoder-Decoder_Models","322710213_The_zero_resource_speech_challenge_2017","322706871_Unwritten_languages_demand_attention_too_Word_discovery_with_encoder-decoder_models","322465846_Evaluating_speech_features_with_the_Minimal-Pair_ABX_task_II_Resistance_to_noise","318740990_Representations_of_language_in_a_model_of_visually_grounded_speech_signal","315667124_Sequence-to-Sequence_Models_Can_Directly_Transcribe_Foreign_Speech","313434663_Representations_of_language_in_a_model_of_visually_grounded_speech_signal"]}