{"id":"336906206_Grad-CAM_Visual_Explanations_from_Deep_Networks_via_Gradient-Based_Localization","abstract":"We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach—Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog’ in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.VGG), (2) CNNs used for structured outputs (e.g.captioning), (3) CNNs used in tasks with multi-modal inputs (e.g.visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names (Bau et al. in Computer vision and pattern recognition, 2017) to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV (Agrawal et al., in: Mobile cloud visual media computing, pp 265–290. Springer, 2015) (http://gradcam.cloudcv.org) and a video at http://youtu.be/COjUB9Izk6E.","authors":["Ramprasaath Rs","Michael Cogswell","Abhishek Das","Ramakrishna Vedantam"],"meta":["February 2020International Journal of Computer Vision 128(8)","DOI:10.1007/s11263-019-01228-7"],"references":["322023613_Top-Down_Neural_Attention_by_Excitation_Backprop","339561963_Taking_a_HINT_Leveraging_Explanations_to_Make_Vision_and_Language_Models_More_Grounded","329747290_Embodied_Question_Answering","329740378_Embodied_Question_Answering","328156137_Choose_Your_Neuron_Incorporating_Domain_Knowledge_Through_Neuron-Importance_15th_European_Conference_Munich_Germany_September_8-14_2018_Proceedings_Part_XIII","322060438_Learning_Cooperative_Visual_Dialog_Agents_with_Deep_Reinforcement_Learning","321745029_IQA_Visual_Question_Answering_in_Interactive_Environments","321417765_Embodied_Question_Answering","320971289_GuessWhat_Visual_Object_Discovery_through_Multi-modal_Dialogue","320971142_Network_Dissection_Quantifying_Interpretability_of_Deep_Visual_Representations","320965140_Visual_Dialog","319770430_Rich_feature_hierarchies_for_accurate_object_detection_and_semantic_segmentation","319770378_Explaining_and_harnessing_adversarial_examples","319770345_Exploring_Models_and_Data_for_Image_Question_Answering","319770249_Deep_Visual-Semantic_Alignments_for_Generating_Image_Descriptions"]}