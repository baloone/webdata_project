{"id":"328072916_Optimal_Rates_for_Spectral_Algorithms_with_Least-Squares_Regression_over_Hilbert_Spaces","abstract":"In this paper, we study regression problems over a separable Hilbert space with the square loss, covering non-parametric regression over a reproducing kernel Hilbert space. We investigate a class of spectral/regularized algorithms, including ridge regression, principal component regression, and gradient methods. We prove optimal, high-probability convergence results in terms of variants of norms for the studied algorithms, considering a capacity assumption on the hypothesis space and a general source condition on the target function. Consequently, we obtain almost sure convergence results with optimal rates. Our results improve and generalize previous results, filling a theoretical gap for the non-attainable cases.","authors":["Junhong Lin","Alessandro Rudi","Lorenzo Rosasco","Volkan Cevher"],"meta":["October 2018Applied and Computational Harmonic Analysis 48(3)","DOI:10.1016/j.acha.2018.09.009"],"references":["316902965_Learning_Theory_of_Distributed_Spectral_Algorithms","306186572_Distributed_Learning_with_Regularized_Least_Squares","294871659_Generalization_Properties_of_Learning_with_Random_Features","328788481_Regularized_Nystrom_subsampling_in_regression_and_ranking_problems_under_general_smoothness_assumptions","322652570_Optimal_Convergence_for_Distributed_Learning_with_Stochastic_Gradient_Methods_and_Spectral_Algorithms","321147454_Optimal_rates_for_multi-pass_stochastic_gradient_methods","315660316_Optimal_Rates_for_the_Regularized_Learning_Algorithms_under_General_Source_Condition","303681196_Kernel_ridge_vs_principal_component_regression_minimax_bounds_and_adaptability_of_regularization_operators","301835259_Optimal_Rates_For_Regularization_Of_Statistical_Inverse_Learning_Problems","289861661_Random_design_analysis_of_ridge_regression"]}