{"id":"240383291_Security_Evaluation_of_Pattern_Classifiers_Under_Attack","abstract":"Pattern classification systems are commonly used in adversarial applications, like biometric authentication, network intrusion detection, and spam filtering, in which data can be purposely manipulated by humans to undermine their operation. As this adversarial scenario is not taken into account by classical design methods, pattern classification systems may exhibit vulnerabilities, whose exploitation may severely affect their performance, and consequently limit their practical utility. Extending pattern classification theory and design methods to adversarial settings is thus a novel and very relevant research direction, which has not yet been pursued in a systematic way. In this paper, we address one of the main open issues: evaluating at design phase the security of pattern classifiers, namely, the performance degradation under potential attacks they may incur during operation. We propose a framework for empirical evaluation of classifier security that formalizes and generalizes the main ideas proposed in the literature, and give examples of its use in three real applications. Reported results show that security evaluation can provide a more complete understanding of the classifierâ€™s behavior in adversarial environments, and lead to better design choices.","authors":["Battista Biggio","Giorgio Fumera","Fabio Roli"],"meta":["January 2013IEEE Transactions on Knowledge and Data Engineering 99(4):1","DOI:10.1109/TKDE.2013.57","Project: Adversarial Machine Learning"],"references":["240383284_Security_evaluation_of_biometric_authentication_systems_under_real_spoofing_attacks","228982724_Feature_weighting_for_improved_classifier_robustness","228934695_Evaluation_of_classifiers_practical_considerations_for_security_applications","224952248_Machine_Learning_in_Automated_Text_Categorization","221654486_Adversarial_learning","221650859_Good_Word_Attacks_on_Statistical_Spam_Filters","221609706_A_framework_for_quantitative_security_analysis_of_machine_learning","221427504_Comparing_Anomaly_Detection_Techniques_for_HTTP","221427503_Anomalous_Payload-Based_Network_Intrusion_Detection","221427473_Advanced_Allergy_Attacks_Does_a_Corpus_Really_Help","221173723_A_framework_for_generating_data_to_simulate_changing_environments","221037747_Spam_Filtering_Using_Inexact_String_Matching_in_Explicit_Feature_Space_with_On-Line_Linear_Classifiers","220832101_Exploiting_Machine_Learning_to_Subvert_Your_Spam_Filter","220765814_Using_an_Ensemble_of_One-Class_SVM_Classifiers_to_Harden_Payload-based_Anomaly_Detection_Systems","220713708_Casting_out_Demons_Sanitizing_Training_Data_for_Anomaly_Sensors","220320948_Online_Anomaly_Detection_under_Adversarial_Impact","4238326_A_framework_for_the_evaluation_of_intrusion_detection_systems","309761040_Likelihood_ratio-based_biometric_score_fusion","307881957_LIBSVM_A_library_for_support_vector_machines","262410247_Static_Prediction_Games_for_Adversarial_Learning_Problems","262173234_Adversarial_machine_learning","259864416_Pattern_Classification","242376908_An_Introduction_to_The_Bootstrap","242370124_Polymorphic_Blending_Attacks","240383273_Robustness_of_multi-modal_biometric_verification_systems_under_realistic_spoofing_attacks","240383216_Poisoning_Adaptive_Biometric_Systems","240383025_Bagging_Classifiers_for_Fighting_Poisoning_Attacks_in_Adversarial_Environments","228095591_Poisoning_Attacks_against_Support_Vector_Machines","226749770_Multiple_classifier_systems_for_robust_classifier_design_in_adversarial_environments","225112116_Paragraph_Thwarting_Signature_Learning_by_Training_Maliciously","224574262_Adversarial_Knowledge_Discovery","224218290_Multimodal_fusion_vulnerability_to_non-zero_effort_spoof_imposters","222691332_Robustness_of_multimodal_biometric_fusion_methods_against_spoof_attacks","221650864_On_Attacking_Statistical_Spam_Filters","221611962_ANTIDOTE_understanding_and_defending_against_poisoning_of_anomaly_detectors","221609372_Can_machine_learning_be_secure","221597756_Vulnerabilities_in_Biometric_Encryption_Systems","221345874_Nightmare_at_test_time_robust_learning_by_feature_deletion","221276067_Adversarial_Pattern_Classification_Using_Multiple_Classifiers_and_Randomisation","221094150_Bagging_Classifiers_for_Fighting_Poisoning_Attacks_in_Adversarial_Classification_Tasks","221094149_Multiple_Classifier_Systems_for_Adversarial_Classification_Tasks","220754470_Design_of_robust_classifiers_for_adversarial_environments","220688792_Advanced_Database_Systems","220343885_The_security_of_machine_learning","220343865_Machine_learning_in_adversarial_environments","220320686_A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters","220049076_An_Introduction_To_The_Bootstrap","210303561_Learning_to_Classify_with_Missing_and_Corrupted_Features","200110599_Adversarial_Information_Retrieval_The_Manipulation_of_Web_Content","12413257_New_Support_Vector_Algorithms","3302671_Support_Vector_Machines_for_Spam_Categorization","3194491_Likelihood_Ratio-Based_Biometric_Score_Fusion","2925496_Learning_in_the_Presence_of_Malicious_Errors","2886081_Adversarial_Classification"]}