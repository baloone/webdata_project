{"id":"314115623_Stabilising_Experience_Replay_for_Deep_Multi-Agent_Reinforcement_Learning","abstract":"Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A key stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep RL relies. This paper proposes two methods that address this problem: 1) conditioning each agent's value function on a footprint that disambiguates the age of the data sampled from the replay memory and 2) using a multi-agent variant of importance sampling to naturally decay obsolete data. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.","authors":["Jakob Foerster","Nantas Nardelli","Gregory Farquhar","Philip Hilaire Torr"],"meta":["February 2017"],"references":["309854648_Learning_to_Play_Guess_Who_and_Inventing_a_Grounded_Language_as_a_Consequence","309729802_Sample_Efficient_Actor-Critic_with_Experience_Replay","309631686_TorchCraft_a_Library_for_Machine_Learning_Research_on_Real-Time_Strategy_Games","308027016_Episodic_Exploration_for_Deep_Deterministic_Policies_An_Application_to_StarCraft_Micromanagement_Tasks","303448501_Learning_to_Communicate_with_Deep_Multi-Agent_Reinforcement_Learning","284219262_Prioritized_Experience_Replay","319770411_Torch7_A_Matlab-like_Environment_for_Machine_Learning","313650896_Multi-agent_Reinforcement_Learning_in_Sequential_Social_Dilemmas","313642261_Multi-agent_Reinforcement_Learning_in_Sequential_Social_Dilemmas","280329735_Deep_Recurrent_Q-Learning_for_Partially_Observable_MDPs"]}