{"id":"349438420_Robust_ecological_analysis_of_camera_trap_data_labelled_by_a_machine_learning_model","abstract":"1. Ecological data are collected over vast geographic areas using digital sensors such as camera traps and bioacoustic recorders. Camera traps have become the standard method for surveying many terrestrial mammals and birds, but camera trap arrays often generate millions of images that are time‐consuming to label. This causes significant latency between data collection and subsequent inference, which impedes conservation at a time of ecological crisis. Machine learning algorithms have been developed to improve the speed of labeling camera trap data, but it is uncertain how the outputs of these models can be used in ecological analyses without secondary validation by a human. 2. Here, we present our approach to developing, testing and applying a machine learning model to camera trap data for the purpose of achieving fully automated ecological analyses. As a case‐study, we built a model to classify 26 Central African forest mammal and bird species (or groups). The model generalizes to new spatially and temporally independent data (n = 227 camera stations, n = 23868 images), and outperforms humans in several respects (e.g. detecting ‘invisible’ animals). We demonstrate how ecologists can evaluate a machine learning model’s precision and accuracy in an ecological context by comparing species richness, activity patterns (n = 4 species tested) and occupancy (n = 4 species tested) derived from machine learning labels with the same estimates derived from expert labels. 3. Results show that fully automated species labels can be equivalent to expert labels when calculating species richness, activity patterns (n = 4 species tested) and estimating occupancy (n = 3 of 4 species tested) in a large, completely out‐of‐sample test dataset. Simple thresholding using the Softmax values (i.e. excluding ‘uncertain’ labels) improved the model's performance when calculating activity patterns and estimating occupancy but did not improve estimates of species richness. 4. We conclude that, with adequate testing and evaluation in an ecological context, a machine learning model can generate labels for direct use in ecological analyses without the need for manual validation. We provide the user‐community with a multi‐platform, multi‐language graphical user interface that can be used to run our model offline.","authors":["Robin Whytock","Jędrzej Świeżewski","Joeri Zwerts","Tadeusz Bara‐Słupski"],"meta":["February 2021Methods in Ecology and Evolution 12(1)","DOI:10.1111/2041-210X.13576","Projects: African Pangolin Conservation planningDeveloping strategies for sustainable forest resource management in Gabon"],"references":["344703608_A_deep_active_learning_system_for_species_identification_and_counting_in_camera_trap_images","340297755_Drawn_out_of_the_shadows_Surveying_secretive_forest_species_with_camera_trap_distance_sampling","339773700_Three_critical_factors_affecting_automated_image_species_recognition_performance_for_camera_traps","339348397_Fastai_A_Layered_API_for_Deep_Learning","337190312_Camera_trapping_reveals_trends_in_forest_duiker_populations_in_African_National_Parks","336080706_Wildlife_Insights_A_Platform_to_Maximize_the_Potential_of_Camera_Trap_and_Other_Passive_Sensor_Wildlife_Data_for_the_Planet","334816830_The_Role_of_Forest_Elephants_in_Shaping_Tropical_Forest-Savanna_Coexistence","333879675_Efficient_Pipeline_for_Automating_Species_ID_in_new_Camera_Trap_Projects","336999177_Zilong_A_tool_to_identify_empty_images_in_camera-trap_data","332032622_Recognition_in_Terra_Incognita","330688543_Camera-trapping_version_30_current_constraints_and_future_priorities_for_development","329750896_Deep_Learning_Object_Detection_Methods_for_Ecological_Camera_Trap_Data","329647674_Past_Present_and_Future_Approaches_Using_Computer_Vision_for_Animal_Re-Identification_from_Camera_Trap_Data","329210510_Machine_learning_to_classify_animal_species_in_camera_trap_images_Applications_in_ecology","329290645_Camera_trapping_in_Africa_Paving_the_way_for_ease_of_use_and_consistency"]}