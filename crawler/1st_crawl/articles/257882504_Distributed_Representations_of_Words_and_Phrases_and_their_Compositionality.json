{"id":"257882504_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality","abstract":"The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large\nnumber of precise syntactic and semantic word relationships. In this paper we\npresent several extensions that improve both the quality of the vectors and the\ntraining speed. By subsampling of the frequent words we obtain significant\nspeedup and also learn more regular word representations. We also describe a\nsimple alternative to the hierarchical softmax called negative sampling. An\ninherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\nMotivated by this example, we present a simple method for finding phrases in\ntext, and show that learning good vector representations for millions of\nphrases is possible.","authors":["Tomas Mikolov","Ilya Sutskever","Kai Chen","G.s. Corrado"],"meta":["October 2013Advances in Neural Information Processing Systems 26","SourcearXiv"],"references":["258082321_Distributional_Semantics_Beyond_Words_Supervised_Learning_of_Analogy_and_Paraphrase","241637478_Strategies_for_training_large_scale_neural_network_language_models","234131319_Efficient_Estimation_of_Word_Representations_in_Vector_Space","228348202_Hierarchical_probabilistic_neural_network_language_model","285895924_Linguistic_regularities_in_continuous_space_word_representations","266458936_Statistical_Language_Models_Based_on_Neural_Networks","262367926_Semantic_Compositionality_through_Recursive_Matrix-Vector_Spaces","229091480_Learning_Representations_by_Back_Propagating_Errors","228095628_A_Fast_and_Simple_Algorithm_for_Training_Neural_Probabilistic_LanguageModels","225818196_Neural_Probabilistic_Language_Models"]}