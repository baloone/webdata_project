{"id":"321417458_A_Semantic_Loss_Function_for_Deep_Learning_with_Symbolic_Knowledge","abstract":"This paper develops a novel methodology for using symbolic knowledge in deep learning. From first principles, we derive a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that our semantic loss function effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classification. Moreover, it significantly increases the ability of the neural network to predict structured objects, such as rankings and paths. These discrete concepts are tremendously difficult to learn, and benefit from a tight integration of deep learning and symbolic reasoning methods.","authors":["Jingyi Xu","Zilu Zhang","Tal Friedman","Yitao Liang"],"meta":["November 2017","Project: Applying Constraints on Neural Network"],"references":["329467135_Adversarial_Sets_for_Regularising_Neural_Link_Predictors","319312259_Fashion-MNIST_a_Novel_Image_Dataset_for_Benchmarking_Machine_Learning_Algorithms","318671287_Adversarial_Sets_for_Regularising_Neural_Link_Predictors","317399356_Imposing_Hard_Constraints_on_Deep_Networks_Promises_and_Limitations","317163971_Logic_Tensor_Networks_for_Semantic_Image_Interpretation","316098571_Virtual_Adversarial_Training_A_Regularization_Method_for_Supervised_and_Semi-Supervised_Learning","326682126_Programming_with_a_differentiable_forth_interpreter","320098526_Premise_Selection_for_Theorem_Proving_by_Deep_Graph_Embedding","319770183_Imagenet_classification_with_deep_convolutional_neural_networks","319770163_Semi-Supervised_Learning_with_Deep_Generative_Models"]}