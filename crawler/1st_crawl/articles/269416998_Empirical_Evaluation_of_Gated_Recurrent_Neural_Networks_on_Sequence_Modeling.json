{"id":"269416998_Empirical_Evaluation_of_Gated_Recurrent_Neural_Networks_on_Sequence_Modeling","abstract":"In this paper we compare different types of recurrent units in recurrent\nneural networks (RNNs). Especially, we focus on more sophisticated units that\nimplement a gating mechanism, such as a long short-term memory (LSTM) unit and\na recently proposed gated recurrent unit (GRU). We evaluate these recurrent\nunits on the tasks of polyphonic music modeling and speech signal modeling. Our\nexperiments revealed that these advanced recurrent units are indeed better than\nmore traditional recurrent units such as tanh units. Also, we found GRU to be\ncomparable to LSTM.","authors":["Junyoung Chung","Caglar Gulcehre","KyungHyun Cho","Y. Bengio"],"meta":["December 2014","SourcearXiv"],"references":["265385879_On_the_Properties_of_Neural_Machine_Translation_Encoder-Decoder_Approaches","265252627_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate","255983764_Pylearn2_A_machine_learning_research_library","243781690_Untersuchungen_zu_dynamischen_neuronalen_Netzen","279394722_Learned-Norm_Pooling_for_Deep_Feedforward_and_Recurrent_Neural_Networks","267706055_Practical_Variational_Inference_for_Neural_Networks","265554383_Sequence_to_Sequence_Learning_with_Neural_Networks","262395872_Random_Search_for_Hyper-Parameter_Optimization","258818168_Speech_Recognition_with_Deep_Recurrent_Neural_Networks","255173850_Generating_Sequences_With_Recurrent_Neural_Networks"]}