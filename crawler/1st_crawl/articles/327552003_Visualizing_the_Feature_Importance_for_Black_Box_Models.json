{"id":"327552003_Visualizing_the_Feature_Importance_for_Black_Box_Models","abstract":"In recent years, a large amount of model-agnostic methods to improve the transparency, trustability and interpretability of machine learning models have been developed. We introduce local feature importance as a local version of a recent model-agnostic global feature importance method. Based on local feature importance, we propose two visual tools: partial importance (PI) and individual conditional importance (ICI) plots which visualize how changes in a feature affect the model performance on average, as well as for individual observations. Our proposed methods are related to partial dependence (PD) and individual conditional expectation (ICE) plots, but visualize the expected (conditional) feature importance instead of the expected (conditional) prediction. Furthermore, we show that averaging ICI curves across observations yields a PI curve, and integrating the PI curve with respect to the distribution of the considered feature results in the global feature importance. Paper: https://arxiv.org/abs/1804.06620","authors":["Giuseppe Casalicchio","Christoph Molnar","Bernd Bischl"],"meta":["September 2018","Conference: ECML 2018"],"references":["326023812_iml_An_R_package_for_Interpretable_Machine_Learning"]}