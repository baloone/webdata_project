{"id":"318107236_Towards_Understanding_Generalization_of_Deep_Learning_Perspective_of_Loss_Landscapes","abstract":"It is widely observed that deep learning models with learned parameters generalize well, even with much more model parameters than the number of training samples. We systematically investigate the underlying reasons why deep neural networks often generalize well, and reveal the difference between the minima (with the same training error) that generalize well and those they don't. We show that it is the characteristics the landscape of the loss function that explains the good generalization capability. For the landscape of loss function for deep networks, the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima. We theoretically justify our findings through analyzing 2-layer neural networks; and show that the low-complexity solutions have a small norm of Hessian matrix with respect to model parameters. For deeper networks, extensive numerical evidence helps to support our arguments.","authors":["Lei Wu","Zhanxing Zhu","Weinan Ee"],"meta":["June 2017"],"references":["303409641_Unreasonable_Effectiveness_of_Learning_Neural_Networks_From_Accessible_States_and_Robust_Ensembles_to_Basic_Algorithmic_Schemes","282000248_Subdominant_Dense_Clusters_Allow_for_Simple_Learning_and_High_Computational_Performance_in_Neural_Networks_with_Discrete_Synapses","277959140_Path-SGD_Path-Normalized_Optimization_in_Deep_Neural_Networks","277411157_Deep_Learning","321595511_Information_and_Complexity_in_Statistical_Modeling","319770272_Delving_Deep_into_Rectifiers_Surpassing_Human-Level_Performance_on_ImageNet_Classification","319770104_Deep_Learning_Requires_Rethinking_Generalization","315096447_Sharp_Minima_Can_Generalize_For_Deep_Nets","314951892_Langevin_Dynamics_with_Continuous_Tempering_for_High-dimensional_Non-convex_Optimization","310671527_Singularity_of_the_Hessian_in_Deep_Learning"]}