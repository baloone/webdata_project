{"id":"324686168_Continuation_of_Nesterov's_Smoothing_for_Regression_With_Structured_Sparsity_in_High-Dimensional_Neuroimaging","abstract":"Predictive models can be used on high-dimensional brain images to decode cognitive states or diagnosis/prognosis of a clinical condition/evolution. Spatial regularization through structured sparsity offers new perspectives in this context and reduces the risk of overfitting the model while providing in-terpretable neuroimaging signatures by forcing the solution to adhere to domain-specific constraints. Total Variation (TV) is a promising candidate for structured penalization: it enforces spatial smoothness of the solution while segmenting predictive regions from the background. We consider the problem of minimizing the sum of a smooth convex loss, a non-smooth convex penalty (whose proximal operator is known) and a wide range of possible complex, non-smooth convex structured penalties such as TV or overlapping group Lasso. Existing solvers are either limited in the functions they can minimize or in their practical capacity to scale to high-dimensional imaging data. Nesterov's smoothing technique can be used to minimize a large number of non-smooth convex structured penalties. However, reasonable precision requires a small smoothing parameter, which slows down the convergence speed to unacceptable levels. To benefit from the versatility of Nesterov's smoothing technique, we propose a first order continuation algorithm, CONESTA, which automatically generates a sequence of decreasing smoothing parameters. The generated sequence maintains the optimal convergence speed towards any globally desired precision. Our main contributions are: To propose an expression of the duality gap to probe the current distance to the global optimum in order to adapt the smoothing parameter and the convergence speed. This expression is applicable to many penalties and can be used with other solvers than CONESTA. We also propose an expression for the particular smoothing parameter that minimizes the number of iterations required to reach a given precision. Further, we provide a convergence proof and its rate, which is an improvement over classical proximal gradient smoothing methods. We demonstrate on both simulated and high-dimensional structural neuroimaging data that CONESTA significantly outperforms many state-of-the-art solvers in regard to convergence speed and precision.","authors":["Fouad Hadj Selem","Tommy LÃ¶fstedt","Elvis Dohmatob","Vincent Frouin"],"meta":["April 2018IEEE Transactions on Medical Imaging PP(99)","DOI:10.1109/TMI.2018.2829802","Project: Machine Learning for neuroimaging"],"references":["288060441_FAASTA_A_fast_solver_for_total-variation_regularization_of_ill-conditioned_problems_with_application_to_brain_imaging","281696962_Sparse_coding_for_machine_learning_image_processing_and_computer_vision","265013411_Benchmarking_solvers_for_TV-l1_least-squares_and_logistic_regression_in_brain_imaging","261488500_Identifying_Predictive_Regions_from_fMRI_with_TV-L1_Prior","234088415_Interpretable_Whole-Brain_Prediction_Analysis_with_GraphNet","308823558_Speeding-Up_Model-Selection_in_Graphnet_via_Early-Stopping_and_Univariate_Feature-Screening","304824205_Regression_Shrinkage_and_Selection_via_the_LASSO","299342860_Single_Subject_Prediction_of_Brain_Disorders_in_Neuroimaging_Promises_and_Pitfalls","276459455_On_the_Convergence_of_the_Iterates_of_the_Fast_Iterative_ShrinkageThresholding_Algorithm","267179039_Smoothing_and_First_Order_Methods_A_Unified_Framework"]}