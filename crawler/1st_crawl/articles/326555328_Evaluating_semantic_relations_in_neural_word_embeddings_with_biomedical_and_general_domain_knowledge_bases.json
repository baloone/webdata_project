{"id":"326555328_Evaluating_semantic_relations_in_neural_word_embeddings_with_biomedical_and_general_domain_knowledge_bases","abstract":"Background: In the past few years, neural word embeddings have been widely used in text mining. However, the vector representations of word embeddings mostly act as a black box in downstream applications using them, thereby limiting their interpretability. Even though word embeddings are able to capture semantic regularities in free text documents, it is not clear how different kinds of semantic relations are represented by word embeddings and how semantically-related terms can be retrieved from word embeddings.\n\nMethods: To improve the transparency of word embeddings and the interpretability of the applications using them, in this study, we propose a novel approach for evaluating the semantic relations in word embeddings using external knowledge bases: Wikipedia, WordNet and Unified Medical Language System (UMLS). We trained multiple word embeddings using health-related articles in Wikipedia and then evaluated their performance in the analogy and semantic relation term retrieval tasks. We also assessed if the evaluation results depend on the domain of the textual corpora by comparing the embeddings of health-related Wikipedia articles with those of general Wikipedia articles.\n\nResults: Regarding the retrieval of semantic relations, we were able to retrieve various semantic relations. Meanwhile, the two popular word embedding approaches, Word2vec and GloVe, obtained comparable results on both the analogy retrieval task and the semantic relation retrieval task, while dependency-based word embeddings had much worse performance in both tasks. We also found that the word embeddings trained with health-related Wikipedia articles obtained better performance in the health-related relation retrieval tasks than those trained with general Wikipedia articles.\n\nConclusion: It is evident from this study that word embeddings can group terms with diverse semantic relations together. The domain of the training corpus does have impact on the semantic relations represented by word embeddings. We thus recommend using domain-specific corpus to train word embeddings for domain-specific text mining tasks.","authors":["Zhiwei Chen","Zhe He","Xiuwen Liu","Jiang Bian"],"meta":["July 2018BMC Medical Informatics and Decision Making 18(S2)","DOI:10.1186/s12911-018-0630-x","Projects: Bridging the Vocabulary Gap between Health Professionals and ConsumersStructural and Semantic Methodologies for Enhancing Biomedical Terminologies"],"references":["319370400_Visual_Exploration_of_Semantic_Relationships_in_Neural_Word_Embeddings","318189009_Semantic_relatedness_and_similarity_of_biomedical_terms_Examining_the_effects_of_recency_size_and_section_of_biomedical_publications_on_the_performance_of_word2vec","329977655_A_Latent_Variable_Model_Approach_to_PMI-based_Word_Embeddings","323885950_The_Unified_Medical_Language_System","321990241_An_exploration_of_semantic_relations_in_neural_word_embeddings_using_extrinsic_knowledge","319770439_Efficient_Estimation_of_Word_Representations_in_Vector_Space","315731226_Enriching_consumer_health_vocabulary_through_mining_a_social_QA_site_A_similarity-based_approach","312451745_Document_modeling_with_gated_recurrent_neural_network_for_sentiment_classification_In_EMNLP","308797129_Distributional_structure","305388986_Enriching_Word_Vectors_with_Subword_Information"]}