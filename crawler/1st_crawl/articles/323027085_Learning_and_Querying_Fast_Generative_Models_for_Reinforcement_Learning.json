{"id":"323027085_Learning_and_Querying_Fast_Generative_Models_for_Reinforcement_Learning","abstract":"A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed generative models that learn and operate on compact state representations, so-called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment from raw pixels. The computational speed-up of state-space models while maintaining high accuracy makes their application in RL feasible: We demonstrate that agents which query these models for decision making outperform strong model-free baselines on the game MSPACMAN, demonstrating the potential of using learned environment models for planning.","authors":["Lars Buesing","Theophane Weber","Sébastien Racanière","S. M. Ali Eslami"],"meta":["February 2018"],"references":["320754817_Stochastic_Variational_Video_Prediction","317591001_Hybrid_Reward_Architecture_for_Reinforcement_Learning","315835265_Recurrent_Environment_Simulators","320280165_Rainbow_Combining_Improvements_in_Deep_Reinforcement_Learning","319770134_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models","319769991_Trust_Region_Policy_Optimization","318829926_Value_Iteration_Networks","318694645_Deep_visual_foresight_for_planning_robot_motion","318392190_Value_Prediction_Network","317062142_Model-Based_Planning_in_Discrete_Action_Spaces"]}