{"id":"284280060_Can_High-Order_Dependencies_Improve_Mutual_Information_based_Feature_Selection","abstract":"Mutual information (MI) based approaches are a popular paradigm for feature selection. Most previous methods have made use of low-dimensional MI quantities that are only effective at detecting low-order dependencies between variables. Several works have considered the use of higher dimensional mutual information, but the theoretical underpinning of these approaches is not yet comprehensive. To fill this gap, in this paper, we systematically investigate the issues of employing high-order dependencies for mutual information based feature selection. We first identify a set of assumptions under which the original high-dimensional mutual information based criterion can be decomposed into a set of low-dimensional MI quantities. By relaxing these assumptions, we arrive at a principled approach for constructing higher dimensional MI based feature selection methods that takes into account higher order feature interactions. Our extensive experimental evaluation on real data sets provides concrete evidence that methodological inclusion of high-order dependencies improve MI based feature selection.","authors":["Nguyen the vinh","Shuo Zhou","Jeffrey Chan","James Cameron Bailey"],"meta":["November 2015Pattern Recognition 53","DOI:10.1016/j.patcog.2015.11.007"],"references":["228979623_A_Feature_Subset_Selection_Method_Based_On_High-Dimensional_Mutual_Information","227100298_Overcoming_the_Myopia_of_Inductive_Learning_Algorithms_with_RELIEFF","224124950_On_the_Feature_Selection_Criterion_Based_on_an_Approximation_of_Multidimensional_Mutual_Information","221996079_An_Introduction_of_Variable_and_Feature_Selection","221305296_Conditional_Infomax_Learning_An_Integrated_Framework_for_Feature_Extraction_and_Fusion","220343798_Not_So_Naive_Bayes_Aggregating_One-Dependence_Estimators","220343395_Bayesian_Network_Classifiers","220319871_Quadratic_Programming_Feature_Selection","3301850_Using_Mutual_Information_for_Selecting_Features_in_Supervised_Neural_Net_Learning","312852468_Dror_G_2004_Result_analysis_of_the_NIPS","266660352_Effective_global_approaches_for_mutual_information_based_feature_selection","262347236_Conditional_Likelihood_Maximisation_A_Unifying_Framework_for_Information_Theoretic_Feature_Selection","262239374_Mutual_information-based_method_for_selecting_informative_feature_sets","224365012_Conditional_Mutual_Information_Based_Feature_Selection","221346409_Spectral_feature_selection_for_supervised_and_unsupervised_learning","220320537_Fast_Binary_Feature_Selection_with_Conditional_Mutual_Information","8002088_Estimating_Optimal_Feature_Subsets_Using_Efficient_Estimation_of_High-Dimensional_Mutual_Information","7641976_Feature_Selection_Based_On_Mutual_Information_Criteria_of_Max-DependencyMax-Relevance_and_Min-Redundancy"]}