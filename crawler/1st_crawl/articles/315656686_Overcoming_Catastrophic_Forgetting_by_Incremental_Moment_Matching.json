{"id":"315656686_Overcoming_Catastrophic_Forgetting_by_Incremental_Moment_Matching","abstract":"Catastrophic forgetting is a problem which refers to losing the information of the first task after training from the second task in continual learning of neural networks. To resolve this problem, we propose the incremental moment matching (IMM), which uses the Bayesian neural network framework. IMM assumes that the posterior distribution of parameters of neural networks is approximated with Gaussian distribution and incrementally matches the moment of the posteriors, which are trained for the first and second task, respectively. To make our Gaussian assumption reasonable, the IMM procedure utilizes various transfer learning techniques including weight transfer, L2-norm of old and new parameters, and a newly proposed variant of dropout using old parameters. We analyze our methods on the MNIST and CIFAR-10 datasets, and then evaluate them on a real-world life-log dataset collected using Google Glass. Experimental results show that IMM produces state-of-the-art performance in a variety of datasets.","authors":["Sang-Woo Lee","Jin-Hwa Kim","Jung-Woo Ha","Byoung-Tak Zhang"],"meta":["October 2017","Conference: Neural Information Processing Systems (NIPS 2017)Volume: (to appear, spotlight)"],"references":["313798242_Maximum_Number_of_Modes_of_Gaussian_Mixtures","313096253_PathNet_Evolution_Channels_Gradient_Descent_in_Super_Neural_Networks","286966779_Compete_to_compute","277022910_Weight_Uncertainty_in_Neural_Networks","269935498_Qualitatively_characterizing_neural_network_optimization_problems","268079628_How_transferable_are_features_in_deep_neural_networks","308277168_Learning_Without_Forgetting","301878549_Structured_and_Efficient_Variational_Deep_Learning_with_Matrix_Gaussian_Posteriors","286794765_Dropout_A_Simple_Way_to_Prevent_Neural_Networks_from_Overfitting","286761751_Understanding_Dropout"]}