{"id":"350998362_A_Selective_Overview_of_Deep_Learning","abstract":"Deep learning has achieved tremendous success in recent years. In simple words, deep learning uses the composition of many nonlinear functions to model the complex dependency between input features and labels. While neural networks have a long history, recent advances have greatly improved their performance in computer vision, natural language processing, etc. From the statistical and scientific perspective, it is natural to ask: What is deep learning? What are the new characteristics of deep learning, compared with classical methods? What are the theoretical foundations of deep learning? To answer these questions, we introduce common neural network models (e.g., convolutional neural nets, recurrent neural nets, generative adversarial nets) and training techniques (e.g., stochastic gradient descent, dropout, batch normalization) from a statistical point of view. Along the way, we highlight new characteristics of deep learning (including depth and over-parametrization) and explain their practical and theoretical benefits. We also sample recent results on theories of deep learning, many of which are only suggestive. While a complete understanding of deep learning remains elusive, we hope that our perspectives and discussions serve as a stimulus for new statistical research.","authors":["Jianqing Fan","Cong Ma","Yiqiao Zhong"],"meta":["April 2021Statistical Science 36(2)","DOI:10.1214/20-STS783"],"references":["337804525_A_priori_estimates_of_the_population_risk_for_two-layer_neural_networks","326996103_Clinically_applicable_deep_learning_for_diagnosis_and_referral_in_retinal_disease","324600286_A_Mean_Field_View_of_the_Landscape_of_Two-Layers_Neural_Networks","323932330_Gradient_Descent_with_Random_Initialization_Fast_Global_Convergence_for_Nonconvex_Phase_Retrieval","323612444_Deep_Learning_and_Its_Applications_in_Biomedicine","320473480_Mastering_the_game_of_Go_without_human_knowledge","317614269_Deep_Learning-Based_Numerical_Methods_for_High-Dimensional_Parabolic_Partial_Differential_Equations_and_Backward_Stochastic_Differential_Equations","315006561_Why_and_when_can_deep-but_not_shallow-networks_avoid_the_curse_of_dimensionality_A_review","307473101_Why_Does_Deep_and_Cheap_Learning_Work_So_Well","306885833_Densely_Connected_Convolutional_Networks","305881127_Improved_Techniques_for_Training_GANs","305196650_Going_deeper_with_convolutions","268079628_How_transferable_are_features_in_deep_neural_networks","267439735_Adaptive_estimation_algorithms_Convergence_optimality_stability","265295439_ImageNet_Large_Scale_Visual_Recognition_Challenge","246546737_Dropout_Training_as_Adaptive_Regularization","243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods","243075889_On_the_Structure_of_Continuous_Functions_of_Several_Variables","236736831_Acceleration_of_Stochastic_Approximation_by_Averaging","227114386_On_the_near_optimality_of_the_stochastic_approximation_of_smooth_functions_by_neural_networks","334840350_On_deep_learning_as_a_remedy_for_the_curse_of_dimensionality_in_nonparametric_regression","329651461_Circuit_Complexity_and_Neural_Networks","328723323_On_the_convergence_of_Adam_Beyond","328525446_On_the_Insufficiency_of_Existing_Momentum_Schemes_for_Stochastic_Optimization","325889978_Generalization_and_equilibrium_in_generative_adversarial_nets_GANs_invited_talk","322113581_Visualizing_the_Loss_Landscape_of_Neural_Nets","320726685_The_Implicit_Bias_of_Gradient_Descent_on_Separable_Data","319770355_Generative_Adversarial_Nets","319770183_Imagenet_classification_with_deep_convolutional_neural_networks","319770104_Deep_Learning_Requires_Rethinking_Generalization","316921534_Learning_ReLUs_via_Gradient_Descent","316349627_On_the_uniform_convergence_of_relative_frequencies_of_events_to_their_probabilities","312451443_Efficient_learning_of_sparse_representations_with_an_energy-based_model","311609041_Deep_Residual_Learning_for_Image_Recognition","311570081_Sliced_Inverse_Regression_for_Dimension_Reduction_With_Discussion","308277201_Identity_Mappings_in_Deep_Residual_Networks","304824205_Regression_Shrinkage_and_Selection_via_the_LASSO","303755314_f-GAN_Training_Generative_Neural_Samplers_using_Variational_Divergence_Minimization","285058764_Receptive_fields_binocular_interaction_and_functional_architecture_in_the_cat's_visual_cortex","272837232_Human-level_control_through_deep_reinforcement_learning","267665795_Understanding_machine_learning_From_theory_to_algorithms","260406235_Projection_Pursuit_Regression","247329569_Random_Approximants_and_Neural_Networks","244474892_Online_Learning_and_Stochastic_Approximations","243743707_Learning_Internal_Representations_by_Error_Propagation","239030103_Heuristics_of_instability_in_model_selection","236736808_Stochastic_Approximation_Algorithms_and_Recursive_Algorithms_and_Applications","236736791_A_Stochastic_Approximation_Method","234778795_Learnability_Stability_and_Uniform_Convergence","232472619_Neocognitron_A_Self-Organizing_Neural_Network_Model_for_a_Mechanism_of_Visual_Pattern_Recognition","221346269_Extracting_and_composing_robust_features_with_denoising_autoencoders","220320677_Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization","226254336_On_Functions_of_Three_Variables","220695511_Neural_Network_Learning_Theoretical_Foundations","220693968_Computational_Complexity_A_Modern_Approach"]}