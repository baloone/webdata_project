{"id":"263582586_SAGA_A_Fast_Incremental_Gradient_Method_With_Support_for_Non-Strongly_Convex_Composite_Objectives","abstract":"In this work we introduce a new optimisation method called SAGA in the spirit\nof SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient\nalgorithms with fast linear convergence rates. SAGA improves on the theory\nbehind SAG and SVRG, with better theoretical convergence rates, and has support\nfor composite objectives where a proximal operator is used on the regulariser.\nUnlike SDCA, SAGA supports non-strongly convex problems directly, and is\nadaptive to any inherent strong convexity of the problem. We give experimental\nresults showing the effectiveness of our method.","authors":["Aaron Defazio","Francis Bach","Simon Lacoste-Julien"],"meta":["July 2014Advances in Neural Information Processing Systems 2","SourcearXiv"],"references":["260250951_Incremental_Majorization-Minimization_Optimization_with_Application_to_Large-Scale_Machine_Learning","259151625_Semi-Stochastic_Gradient_Descent_Methods","313501588_Accelerating_stochastic_gradient_descent_using_predictive_variance_reduction","282790276_New_Optimisation_Methods_for_Machine_Learning","265271826_Introductory_Lectures_on_Convex_Programming_Volume_I_Basic_course","263856131_Finito_A_Faster_Permutable_Incremental_Gradient_Method_for_Big_Data_Problems","263725092_Incrementally_Updated_Gradient_Methods_for_Constrained_and_Regularized_Optimization","260946764_A_Proximal_Stochastic_Gradient_Method_with_Progressive_Variance_Reduction","258247422_Stochastic_Dual_Coordinate_Ascent_with_Alternating_Direction_Multiplier_Method","258222850_Introductory_Lectures_on_Convex_Optimization_A_Basic_Course"]}