{"id":"341268229_Diverse_deep_neural_networks_all_predict_human_IT_well_after_training_and_fitting","abstract":"Deep neural networks (DNNs) trained on object recognition provide the best current models of high-level visual areas in the brain. What remains unclear is how strongly network design choices, such as architecture, task training, and subsequent fitting to brain data contribute to the observed similarities. Here we compare a diverse set of nine DNN architectures on their ability to explain the representational geometry of 62 isolated object images in human inferior temporal (hIT) cortex, as measured with functional magnetic resonance imaging. We compare untrained networks to their task-trained counterparts, and assess the effect of fitting them to hIT using a cross-validation procedure. To best explain hIT, we fit a weighted combination of the principal components of the features within each layer, and subsequently a weighted combination of layers. We test all models across all stages of training and fitting for their correlation with the hIT representational dissimilarity matrix (RDM) using an independent set of images and subjects. We find that trained models significantly outperform untrained models (accounting for 57% more of the explainable variance), suggesting that features representing natural images are important for explaining hIT. Model fitting further improves the alignment of DNN and hIT representations (by 124%), suggesting that the relative prevalence of different features in hIT does not readily emerge from the particular ImageNet object-recognition task used to train the networks. Finally, all DNN architectures tested achieved equivalent high performance once trained and fitted. Similar ability to explain hIT representations appears to be shared among deep feedforward hierarchies of nonlinear features with spatially restricted receptive fields.","authors":["Katherine R Storrs","Tim C Kietzmann","Alexander Walther","Johannes Mehrer"],"meta":["May 2020","DOI:10.1101/2020.05.07.082743"],"references":["349061048_End-to-end_neural_system_identification_with_neural_information_flow","337659963_Learning_to_see_stuff","326381669_Large-Scale_High-Resolution_Comparison_of_the_Core_Visual_Object_Recognition_Behavior_of_Humans_Monkeys_and_State-of-the-Art_Deep_Artificial_Neural_Networks","326379215_Integrated_deep_visual_and_semantic_attractor_neural_networks_predict_fMRI_pattern-information_along_the_ventral_object_processing_pathway","320283410_Deep_Convolutional_Neural_Networks_Outperform_Feature-Based_But_Not_Categorical_Models_in_Explaining_Object_Similarity_Judgments","339083231_Convolutional_Neural_Networks_as_a_Model_of_the_Visual_System_Past_Present_and_Future","336860572_A_deep_learning_framework_for_neuroscience","332829763_Evolving_Images_for_Visual_Neurons_Using_a_Deep_Generative_Network_Reveals_Coding_Principles_and_Neuronal_Preferences","332824689_Neural_population_control_via_deep_image_synthesis","331311082_Deep_Neural_Networks_in_Computational_Neuroscience"]}