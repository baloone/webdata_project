{"id":"323355664_Learning_to_Explain_An_Information-Theoretic_Perspective_on_Model_Interpretation","abstract":"We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show that the resulting method compares favorably to other model explanation methods on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.","authors":["Jianbo Chen","Le Song","Martin J. Wainwright","Michael Jordan"],"meta":["February 2018"],"references":["317062430_A_Unified_Approach_to_Interpreting_Model_Predictions","315892529_Learning_Important_Features_Through_Propagating_Activation_Differences","309663606_Categorical_Reparameterization_with_Gumbel-Softmax","303879493_Variational_Information_Maximization_for_Feature_Selection","319770369_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality","319770118_The_Concrete_Distribution_A_Continuous_Relaxation_of_Discrete_Random_Variables","315782001_Online_and_Linear-Time_Attention_by_Enforcing_Monotonic_Alignments","305342147_Why_Should_I_Trust_You_Explaining_the_Predictions_of_Any_Classifier","286794765_Dropout_A_Simple_Way_to_Prevent_Neural_Networks_from_Overfitting","282906526_A_Sensitivity_Analysis_of_and_Practitioners'_Guide_to_Convolutional_Neural_Networks_for_Sentence_Classification"]}