{"id":"344006532_Layer-specific_Optimization_for_Mixed_Data_Flow_with_Mixed_Precision_in_FPGA_Design_for_CNN-based_Object_Detectors","abstract":"Convolutional neural networks (CNNs) require both intensive computation and frequent memory access, which lead to a low processing speed and large power dissipation. Although the characteristics of the different layers in a CNN are frequently quite different, previous hardware designs have employed common optimization schemes for them. This paper proposes a layer-specific design that employs different organizations that are optimized for the different layers. The proposed design employs two layer-specific optimizations: layer-specific mixed data flow and layer-specific mixed precision. The mixed data flow aims to minimize the off-chip access while demanding a minimal on-chip memory (BRAM) resource of an FPGA device. The mixed precision quantization is to achieve both a lossless accuracy and an aggressive model compression, thereby further reducing the off-chip access. A Bayesian optimization approach is used to select the best sparsity for each layer, achieving the best trade-off between the accuracy and compression. This mixing scheme allows the entire network model to be stored in BRAMs of the FPGA to aggressively reduce the off-chip access, and thereby achieves a significant performance enhancement. The model size is reduced by 22.66-28.93 times compared to that in a full-precision network with a negligible degradation of accuracy on VOC, COCO, and ImageNet datasets. Furthermore, the combination of mixed dataflow and mixed precision significantly outperforms the previous works in terms of both throughput, off-chip access, and on-chip memory requirement.","authors":["Duy Thanh Nguyen","Hyun been Kim","Hyuk-Jae Lee"],"meta":["August 2020IEEE Transactions on Circuits and Systems for Video Technology PP(99):1-1","DOI:10.1109/TCSVT.2020.3020569"],"references":["340445934_A_High-Throughput_and_Power-Efficient_FPGA_Implementation_of_YOLO_CNN_for_Object_Detection","339558933_HAWQ_Hessian_AWare_Quantization_of_Neural_Networks_With_Mixed-Precision","338590207_An_Approximate_Memory_Architecture_for_Energy_Saving_in_Deep_Learning_Applications","332074076_Shortcut_Mining_Exploiting_Cross-Layer_Shortcut_Reuse_in_DCNN_Accelerators","347268082_RaQu_An_automatic_high-utilization_CNN_quantization_and_mapping_framework_for_general-purpose_RRAM_Accelerator","339554747_Gaussian_YOLOv3_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving","338503878_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision","333331683_A_Configurable_Multi-Precision_CNN_Computing_Framework_Based_on_Single_Bit_RRAM","332475825_Accelerator-Aware_Pruning_for_Convolutional_Neural_Networks","331295573_REQ-YOLO_A_Resource-Aware_Efficient_Quantization_Framework_for_Object_Detection_on_FPGAs","331295301_Compressed_CNN_Training_with_FPGA-based_Accelerator","329977399_Promoting_the_Harmony_between_Sparsity_and_Regularity_A_Relaxed_Synchronous_Architecture_for_Convolutional_Neural_Networks","329740282_MobileNetV2_Inverted_Residuals_and_Linear_Bottlenecks","329733296_FINN-_R_An_End-to-End_Deep-Learning_Framework_for_Fast_Exploration_of_Quantized_Neural_Networks","329159622_A_68-mw_22_Topsw_Low_Bit_Width_and_Multiplierless_DCNN_Object_Detection_Processor_for_Visually_Impaired_People"]}