{"id":"335243909_Human_Uncertainty_Makes_Classification_More_Robust","abstract":"The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper , we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks.","authors":["Joshua C. Peterson","Ruairidh McLennan Battleday","Thomas L Griffiths","Olga Russakovsky"],"meta":["October 2019","DOI:10.1109/ICCV.2019.00971","Conference: International Conference on Computer Vision (ICCV)At: Seoul, Korea","Project: Leveraging machine learning to study human cognition"],"references":["328491510_The_Moral_Machine_Experiment","319770123_Densely_Connected_Convolutional_Networks","321718936_Wild_Patterns_Ten_Years_After_the_Rise_of_Adversarial_Machine_Learning","320971540_Aggregated_Residual_Transformations_for_Deep_Neural_Networks","320968593_Deep_Pyramidal_Residual_Networks","319770377_Visual_Concept_Learning_Combining_Machine_Vision_and_Bayesian_Generalization_on_Concept_Hierarchies","318200394_Places_A_10_Million_Image_Database_for_Scene_Recognition","317300001_Toward_Robustness_against_Label_Noise_in_Training_Deep_Discriminative_Neural_Networks","317194083_Wide_Residual_Networks","312457795_Semantic_hierarchies_for_visual_object_recognition"]}