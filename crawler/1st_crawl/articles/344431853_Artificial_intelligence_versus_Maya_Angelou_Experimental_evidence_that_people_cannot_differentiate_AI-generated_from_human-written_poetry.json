{"id":"344431853_Artificial_intelligence_versus_Maya_Angelou_Experimental_evidence_that_people_cannot_differentiate_AI-generated_from_human-written_poetry","abstract":"The release of openly available, robust natural language generation algorithms (NLG) has spurred much public attention and debate. One reason lies in the algorithms' purported ability to generate humanlike text across various domains. Empirical evidence using incentivized tasks to assess whether people (a) can distinguish and (b) prefer algorithm-generated versus human-written text is lacking. We conducted two experiments assessing behavioral reactions to the state-of-the-art Natural Language Generation algorithm GPT-2 (Ntotal = 830). Using the identical starting lines of human poems, GPT-2 produced samples of poems. From these samples, either a random poem was chosen (Human-out-of-theloop) or the best one was selected (Human-in-the-loop) and in turn matched with a human-written poem. In a new incentivized version of the Turing Test, participants failed to reliably detect the algorithmicallygenerated poems in the Human-in-the-loop treatment, yet succeeded in the Human-out-of-the-loop treatment. Further, people reveal a slight aversion to algorithm-generated poetry, independent on whether participants were informed about the algorithmic origin of the poem (Transparency) or not (Opacity). We discuss what these results convey about the performance of NLG algorithms to produce human-like text and propose methodologies to study such learning algorithms in human-agent experimental settings. Artificial intelligence (AI), \"the development of machines capable of sophisticated (intelligent) information processing\" (Dafoe, 2018, p. 5), is rapidly advancing and has begun to take over tasks previously performed solely by humans (Rahwan et al., 2019). Algorithms are already assisting humans in writing text, such as autocompleting sentences in emails and even helping writers write novels (Streitfeld, 2018, pp.","authors":["Nils Kobis","Luca D Mossink"],"meta":["January 2021Computers in Human Behavior 114(2):106553","DOI:10.1016/j.chb.2020.106553","Project: Artificial Intelligence & Ethical Decision Making"],"references":["349365674_Moral_Uncanny_Valley_A_Robot's_Appearance_Moderates_How_its_Decisions_are_Judged","346419140_Artificial_intelligence_for_political_decision-making_in_the_European_Union_Effects_on_citizens'_perceptions_of_input_throughput_and_output_legitimacy","332636704_Machine_behaviour","343755943_Patent_claim_generation_by_fine-tuning_OpenAI_GPT-2","341692026_Factors_Influencing_Perceived_Fairness_in_Algorithmic_Decision-Making_Algorithm_Outcomes_Development_Procedures_and_Individual_Differences","338841804_Implications_of_AI_un-fairness_in_higher_education_admissions_the_effects_of_perceived_AI_un-fairness_on_exit_voice_and_organizational_reputation","338058725_This_AI_researcher_is_trying_to_ward_off_a_reproducibility_crisis","336996438_Hello_It's_GPT-2_-_How_Can_I_Help_You_Towards_the_Use_of_Pretrained_Language_Models_for_Task-Oriented_Dialogue_Systems","336773232_A_systematic_review_of_algorithm_aversion_in_augmented_decision_making","334487491_Task-Dependent_Algorithm_Aversion"]}