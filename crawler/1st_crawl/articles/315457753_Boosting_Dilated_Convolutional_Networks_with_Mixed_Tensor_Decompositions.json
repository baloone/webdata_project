{"id":"315457753_Boosting_Dilated_Convolutional_Networks_with_Mixed_Tensor_Decompositions","abstract":"Expressive efficiency is a concept that allows formally reasoning about the representational capacity of deep network architectures. A network architecture is expressively efficient with respect to an alternative architecture if the latter must grow super-linearly in order to represent functions realized by the former. A well-known example is the exponential expressive efficiency of depth, namely, that in many cases shallow networks must grow exponentially large in order to represent functions realized by deep networks. In this paper we study the expressive efficiency brought forth by the architectural feature of connectivity, motivated by the observation that nearly all state of the art networks these days employ elaborate connection schemes, running layers in parallel while splitting and merging them in various ways. A formal treatment of this question would shed light on the effectiveness of modern connectivity schemes, and in addition, could provide new tools for network design. We focus on dilated convolutional networks, a family of deep models gaining increased attention, underlying state of the art architectures like Google's WaveNet and ByteNet. By introducing and studying the concept of mixed tensor decompositions, we prove that interconnecting dilated convolutional networks can lead to expressive efficiency. In particular, we show that a single connection between intermediate layers can already lead to an almost quadratic gap, which in large-scale settings typically makes the difference between a model that is practical and one that is not.","authors":["Nadav Cohen","Ronen Tamari","Amnon Shashua"],"meta":["March 2017","Project: Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions"],"references":["308278012_Deep_Networks_with_Stochastic_Depth","308026508_WaveNet_A_Generative_Model_for_Raw_Audio","305196650_Going_deeper_with_convolutions","304018355_Exponential_expressivity_in_deep_neural_networks_through_transient_chaos","302305068_Multi-Scale_Context_Aggregation_by_Dilated_Convolutions","332138022_Beating_the_Perils_of_Non-Convexity_Guaranteed_Training_of_Neural_Networks_using_Tensor_Methods","319769990_I-theory_on_depth_vs_width_hierarchical_function_composition","301921832_Fully_convolutional_networks_for_semantic_segmentation","301898481_Training_Input-Output_Recurrent_Neural_Networks_through_Spectral_Methods","287250773_The_Power_of_Depth_for_Feedforward_Neural_Networks","281895600_On_the_Expressive_Power_of_Deep_Learning_A_Tensor_Analysis","281285245_The_Zero_Set_of_a_Polynomial","277411157_Deep_Learning","286594438_Large-Scale_Video_Classification_with_Convolutional_Neural_Networks","286512696_Deep_Residual_Learning_for_Image_Recognition"]}