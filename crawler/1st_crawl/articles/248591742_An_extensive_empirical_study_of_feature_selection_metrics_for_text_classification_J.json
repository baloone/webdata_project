{"id":"248591742_An_extensive_empirical_study_of_feature_selection_metrics_for_text_classification_J","abstract":"Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives-accuracy, F-measure, precision, and recall-since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair-e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin.","authors":["George Forman"],"meta":["March 2003Journal of Machine Learning Research 3"],"references":["223713209_Wrappers_for_Feature_Subset_Selection","221613897_Inductive_Learning_Algorithms_and_Representations_for_Text_Categorization","221345178_Feature_Selection_for_Unbalanced_Class_Distribution_and_Naive_Bayes","200110876_Data_Mining_Practical_Machine_Learning_Tools_and_Techniques_with_Java_Implementations","296011845_Centroid-Based_Document_Classification_Analysis_and_Experimental_Results","285278428_Text_Categorization_with_Support_Vector_Machines_Learning_with_Many_Relevant_Features","284657154_A_comparison_of_event_models_for_naive_bayes_text_classification_in_AAAI-98_workshop_on_learning_for_text_categorization","232463066_What_is_the_best_index_of_detectability","221996831_Learning_With_Kernels","220344030_Gene_Selection_for_Cancer_Classification_Using_Support_Vector_Machines"]}