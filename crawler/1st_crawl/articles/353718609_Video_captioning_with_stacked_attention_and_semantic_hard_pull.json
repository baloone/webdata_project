{"id":"353718609_Video_captioning_with_stacked_attention_and_semantic_hard_pull","abstract":"Video captioning, i.e. , the task of generating captions from video sequences creates a bridge between the Natural Language Processing and Computer Vision domains of computer science. The task of generating a semantically accurate description of a video is quite complex. Considering the complexity, of the problem, the results obtained in recent research works are praiseworthy. However, there is plenty of scope for further investigation. This paper addresses this scope and proposes a novel solution. Most video captioning models comprise two sequential/recurrent layers—one as a video-to-context encoder and the other as a context-to-caption decoder. This paper proposes a novel architecture, namely Semantically Sensible Video Captioning (SSVC) which modifies the context generation mechanism by using two novel approaches—“stacked attention” and “spatial hard pull”. As there are no exclusive metrics for evaluating video captioning models, we emphasize both quantitative and qualitative analysis of our model. Hence, we have used the BLEU scoring metric for quantitative analysis and have proposed a human evaluation metric for qualitative analysis, namely the Semantic Sensibility (SS) scoring metric. SS Score overcomes the shortcomings of common automated scoring metrics. This paper reports that the use of the aforementioned novelties improves the performance of state-of-the-art architectures.","authors":["Md. Mushfiqur Rahman","Thasin Abedin","Khondokar Prottoy","Ayana Moshruba"],"meta":["August 2021PeerJ Computer Science 7(6):e664","DOI:10.7717/peerj-cs.664"],"references":["334375968_TRECVID_2018_Benchmarking_Video_Activity_Detection_Video_Captioning_and_Matching_Video_Storytelling_Linking_and_Video_Search","343466692_Object_Relational_Graph_With_Teacher-Recommended_Learning_for_Video_Captioning","338513223_Memory-Attended_Recurrent_Network_for_Video_Captioning","338509479_Spatio-Temporal_Dynamics_and_Semantic_Attribute_Enriched_Visual_Encoding_for_Video_Captioning","336595182_Video_Description_A_Survey_of_Methods_Datasets_and_Evaluation_Metrics","335357365_Show_Tell_and_Summarize_Dense_Video_Captioning_Using_Visual_Cue_Aided_Sentence_Summarization","334115579_A_Call_for_Clarity_in_Reporting_BLEU_Scores","334008319_STAT_Spatial-Temporal_Attention_Mechanism_for_Video_Captioning","332673843_Video_Captioning_using_Deep_Learning_An_Overview_of_Methods_Datasets_and_Metrics","329747669_M3_Multimodal_Memory_Modelling_for_Video_Captioning","329747467_Reconstruction_Network_for_Video_Captioning","329743892_Interpretable_Video_Captioning_via_Trajectory_Structured_Localization","326365254_Video_Captioning_by_Adversarial_LSTM","325371574_Describing_Video_With_Attention-Based_Bidirectional_LSTM","322590488_Generating_High-Quality_and_Informative_Conversation_Responses_with_Sequence-to-Sequence_Models"]}