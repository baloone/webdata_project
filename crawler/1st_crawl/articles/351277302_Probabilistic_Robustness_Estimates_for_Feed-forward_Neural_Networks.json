{"id":"351277302_Probabilistic_Robustness_Estimates_for_Feed-forward_Neural_Networks","abstract":"Robustness of deep neural networks is a critical issue in practical applications. In the general case of feed-forward neural networks (including convolutional deep neural network architectures), under random noise attacks, we propose to study the probability that the output of the network deviates from its nominal value by a given threshold. We derive a simple concentration inequality for the propagation of the input uncertainty through the network using the Cramerâ€“Chernoff method and estimates of the local variation of the neural network mapping computed at the training points. We further discuss and exploit the resulting condition on the network to regularize the loss function during training. Finally, we assess the proposed tail probability estimates empirically on various public datasets and show that the observed robustness is very well estimated by the proposed method.","authors":["Nicolas P. Couellan"],"meta":["April 2021Neural networks: the official journal of the International Neural Network Society 142(113)","DOI:10.1016/j.neunet.2021.04.037"],"references":["349606277_The_Coupling_Effect_of_Lipschitz_Regularization_in_Neural_Networks","333760642_Universality_of_deep_convolutional_neural_networks","323758530_Deep_Distributed_Convolutional_Neural_Networks_Universality","301839500_TensorFlow_Large-Scale_Machine_Learning_on_Heterogeneous_Distributed_Systems","335679040_CNN-Cert_An_Efficient_Framework_for_Certifying_Robustness_of_Convolutional_Neural_Networks","320967338_The_Robustness_of_Deep_Networks_A_Geometrical_Perspective","320821437_Provable_defenses_against_adversarial_examples_via_the_convex_outer_adversarial_polytope","269935079_Adam_A_Method_for_Stochastic_Optimization","267128815_Concentration_Inequalities_A_nonasymptotic_theory_of_independence","265953591_An_Iteration_Method_for_the_solution_of_the_Eigenvalue_Problem_of_Linear_Differential_and_Integral_Operators"]}