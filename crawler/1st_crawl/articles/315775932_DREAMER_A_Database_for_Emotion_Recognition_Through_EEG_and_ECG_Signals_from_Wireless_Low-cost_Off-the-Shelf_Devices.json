{"id":"315775932_DREAMER_A_Database_for_Emotion_Recognition_Through_EEG_and_ECG_Signals_from_Wireless_Low-cost_Off-the-Shelf_Devices","abstract":"In this work, we present DREAMER, a multi-modal database consisting of electroencephalogram (EEG) and electrocardiogram (ECG) signals recorded during affect elicitation by means of audio-visual stimuli. Signals from 23 participants were recorded along with the participants self-assessment of their affective state after each stimuli, in terms of valence, arousal, and dominance. All the signals were captured using portable, wearable, wireless, low-cost and off-the-shelf equipment that has the potential to allow the use of affective computing methods in everyday applications. A baseline for participant-wise affect recognition using EEG and ECG -based features, as well as their fusion, was established through supervised classification experiments using Support Vector Machines (SVMs). The selfassessment of the participants was evaluated through comparison with the self-assessments from another study using the same audio-visual stimuli. Classification results for valence, arousal and dominance of the proposed database are comparable to the ones achieved for other databases that use non-portable, expensive, medical grade devices. These results indicate the prospects of using low-cost devices for affect recognition applications. The proposed database will be made publicly available in order to allow researchers to achieve a more thorough evaluation of the suitability of these capturing devices for affect recognition applications.","authors":["Stamos Katsigiannis","Naeem Ramzan"],"meta":["January 2018IEEE Journal of Biomedical and Health Informatics 22(1):98-107","DOI:10.1109/JBHI.2017.2688239"],"references":["312854561_EEGLAB_an_open_source_toolbox_for_analysis_of_single-trial_EEG_dynamics","271210862_DECAF_MEG-based_Multimodal_Database_for_Decoding_Affective_Physiological_Responses","260658405_Detecting_Naturalistic_Expressions_of_Nonbasic_Affect_Using_Physiological_Signals","260165618_P300_and_Emotiv_EPOC_Does_Emotiv_EPOC_capture_real_EEG","257393064_A_survey_of_methods_for_data_fusion_and_system_adaptation_using_autonomic_nervous_system_responses_in_physiological_computing","236604763_Validation_of_the_Emotiv_EPOCR_EEG_gaming_system_for_measuring_research_quality_auditory_ERPs","235361517_A_Circumplex_Model_of_Affect","232651717_DEAP_A_Database_for_Emotion_Analysis_Using_Physiological_Signals","228698490_Assessing_the_effectiveness_of_a_large_database_of_emotion-eliciting_films_A_new_tool_for_emotion_researchers","224226954_Continuous_Prediction_of_Spontaneous_Affect_from_Multiple_Cues_and_Modalities_in_Valence-Arousal_Space","224174390_A_3-D_Audio-Visual_Corpus_of_Affective_Communication","224144462_SHIMMERTM_-_A_wireless_sensor_platform_for_noninvasive_biomedical_research","222741832_Evidence_for_a_Three-Factor_Theory_of_Emotions","221264496_From_Physiological_Signals_to_Emotions_Implementing_and_Comparing_Selected_Methods_for_Feature_Extraction_and_Classification","220929585_A_Bimodal_Face_and_Body_Gesture_Database_for_Automatic_Analysis_of_Human_Nonverbal_Affective_Behavior","50851562_BioSig_The_Free_and_Open_Source_Software_Library_for_Biomedical_Signal_Processing","23493444_A_Survey_of_Affect_Recognition_Methods_Audio_Visual_and_Spontaneous_Expressions","19479584_Universals_and_Cultural_Differences_in_the_Judgments_of_Facial_Expressions_of_Emotion","15713974_The_Effects_of_Emotions_on_Short-Term_Power_Spectrum_Analysis_of_Heart_Rate_Variability","8601406_EEGLAB_an_open-source_toolbox_for_analysis_of_EEG_dynamics","285021679_Observations_SAM_The_self-assessment_manikin-_An_efficient_cross-cultural_measurement_of_emotional_response","277948473_Analysis_of_EEG_Signals_and_Facial_Expressions_for_Continuous_Emotion_Detection","270405962_The_Nature_of_Emotions","263711027_Ratings_for_emotion_film_clips","261160485_Multimedia_implicit_tagging_using_EEG_signals","257093268_Categorical_and_dimensional_affect_analysis_in_continuous_input_Current_trends_and_future_directions","235726247_Development_and_Validation_of_Brief_Measures_of_Positive_and_Negative_Affect_The_PANAS_Scales","224251573_A_Multi-Modal_Affective_Database_for_Affect_Recognition_and_Implicit_Tagging","221262340_The_Vera_am_Mittag_German_audio-visual_emotional_speech_database","38098777_Classifying_Affective_States_Using_Thermal_Infrared_Imaging_of_the_Human_Face","26807056_EEG_differences_in_children_between_eyes-closed_and_eyes-open_resting_conditions","19118308_EEG_correlates_of_emotional_tasks_related_to_attentional_demands","15235821_Measuring_Emotion_The_Self-Assessment_Manikin_and_the_Semantic_Differential","15034529_Strong_Evidence_for_Universals_in_Facial_Expressions_A_Reply_to_Russell's_Mistaken_Critique","14700871_Looking_at_pictures_Affective_facial_visceral_and_behavioral_reactions","8940656_Affective_neuroscience_and_psychophysiology_Toward_a_synthesis","7334683_Basic_emotions_are_associated_with_distinct_patterns_of_cardiorespiratory_activity_International_Journal_of_Psychophysiology_611_5-18","5934095_EEG_differences_between_eyes-closed_and_eyes-open_resting_conditions","3038108_A_real-time_QRS_detection_algorithm"]}