{"id":"312370190_scmamp_Statistical_Comparison_of_Multiple_Algorithms_in_Multiple_Problems","abstract":"Comparing the results obtained by two or more algorithms in a set of problems is a central task in areas such as machine learning or optimization. Drawing conclusions from these comparisons may require the use of statistical tools such as hypothesis testing. There are some interesting papers that cover this topic. In this manuscript we present scmamp, an R package aimed at being a tool that simplifies the whole process of analyzing the results obtained when comparing algorithms, from loading the data to the production of plots and tables. Comparing the performance of different algorithms is an essential step in many research and practical computational works. When new algorithms are proposed, they have to be compared with the state of the art. Similarly, when an algorithm is used for a particular problem, its performance with different sets of parameters has to be compared, in order to tune them for the best results. When the differences are very clear (e.g., when an algorithm is the best in all the problems used in the comparison), the direct comparison of the results may be enough. However, this is an unusual situation and, thus, in most situations a direct comparison may be misleading and not enough to draw sound conclusions; in those cases, the statistical assessment of the results is advisable. The statistical comparison of algorithms in the context of machine learning has been covered in several papers. In particular, the tools implemented in this package are those presented in Demšar (2006); García and Herrera (2008); García et al. (2010). Another good review that covers, among other aspects, the statistical assessment of the results in the context of supervised classification can be found in Santafé et al. (2015).","authors":["Borja Calvo","Guzmán Santafé"],"meta":["January 2016The R Journal 8(1):1-8","DOI:10.32614/RJ-2016-017"],"references":["254306218_An_Extension_on_Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets_for_all_Pairwise_Comparisons","235709946_STATService_Herramienta_de_analisis_estad'istico_como_soporte_para_la_investigacion_con_Metaheur'isticas","220176331_KEEL_A_Software_Tool_to_Assess_Evolutionary_Algorithms_for_Data_Mining_Problems","304506333_STAC_A_web_platform_for_the_comparison_of_algorithms_using_statistical_tests","285797770_Improvements_of_General_Multiple_Test_Procedures_for_Redundant_Systems_of_Hypotheses","280498447_FrogCOL_and_FrogMIS_new_decentralized_algorithms_for_finding_large_independent_sets_in_graphs","238355667_The_Use_of_Ranks_to_Avoid_the_Assumption_of_Normality_Implicit_in_the_Analysis_of_Variance","222646699_Advanced_nonparametric_tests_for_multiple_comparisons_in_the_design_of_experiments_in_computational_intelligence_and_data_mining_Experimental_analysis_of_power","220320196_Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets","216300794_A_Simple_Sequentially_Rejective_Multiple_Test_Procedure"]}