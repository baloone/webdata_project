{"id":"346679179_Robustness_and_Explainability_of_Artificial_Intelligence_from_technical_to_policy_solutions","abstract":"In the light of the recent advances in artificial intelligence (AI), the serious negative consequences of its use for EU citizens and organisations have led to multiple initiatives from the European Commission to set up the principles of a trustworthy and secure AI. Among the identified requirements, the concepts of robustness and explainability of AI systems have emerged as key elements for a future regulation of this technology.\nThis Technical Report by the European Commission Joint Research Centre (JRC) aims to contribute to this movement for the establishment of a sound regulatory framework for AI, by making the connection between the principles embodied in current regulations regarding to the cybersecurity of digital systems and the protection of data, the policy activities concerning AI, and the technical discussions within the scientific community of AI, in particular in the field of machine learning, that is largely at the origin of the recent advancements of this technology.\nThe individual objectives of this report are to provide a policy-oriented description of the current perspectives of AI and its implications in society, an objective view on the current landscape of AI, focusing of the aspects of robustness and explainability. This also include a technical discussion of the current risks associated with AI in terms of security, safety, and data protection, and a presentation of the scientific solutions that are currently under active development in the AI community to mitigate these risks.\nThis report puts forward several policy-related considerations for the attention of policy makers to establish a set of standardisation and certification tools for AI. First, the development of methodologies to evaluate the impacts of AI on society, built on the model of the Data Protection Impact Assessments (DPIA) introduced in the General Data Protection Regulation (GDPR), is discussed. Secondly, a focus is made on the establishment of methodologies to assess the robustness of systems that would be adapted to the context of use. This would come along with the identification of known vulnerabilities of AI systems, and the technical solutions that have been proposed in the scientific community to address them. Finally, the aspects of transparency and explainability of AI are discussed, including the explainability-by-design approaches for AI models.","authors":["Ronan Hamon","Henrik Junklewitz","Ignacio Sanchez"],"meta":["January 2020","DOI:10.2760/57493","Affiliation: European Commission"],"references":["336612303_Definitions_methods_and_applications_in_interpretable_machine_learning","327709435_Peeking_Inside_the_Black-Box_A_Survey_on_Explainable_Artificial_Intelligence_XAI","326023170_Making_machine_learning_robust_against_adversarial_inputs","323355774_Adversarial_Examples_that_Fool_both_Human_and_Computer_Vision","323302750_The_Malicious_Use_of_Artificial_Intelligence_Forecasting_Prevention_and_Mitigation","322976218_A_Survey_of_Methods_for_Explaining_Black_Box_Models","320796885_Counterfactual_Explanations_Without_Opening_the_Black_Box_Automated_Decisions_and_the_GDPR","320678967_Practical_Secure_Aggregation_for_Privacy-Preserving_Machine_Learning","320609325_One_Pixel_Attack_for_Fooling_Deep_Neural_Networks","318830044_Reinforcement_Learning_with_a_Corrupted_Reward_Channel","318370372_Safety_Verification_of_Deep_Neural_Networks","317821828_Explanation_in_Artificial_Intelligence_Insights_from_the_Social_Sciences","317002535_Membership_Inference_Attacks_Against_Machine_Learning_Models","315852155_Practical_Black-Box_Attacks_against_Machine_Learning","314153799_Adversarial_Reinforcement_Learning_in_a_Cyber_Security_Simulation","313394663_Reluplex_An_Efficient_SMT_Solver_for_Verifying_Deep_Neural_Networks","309101946_Talking_to_Bots_Symbiotic_Agency_and_the_Case_of_Tay","306304648_Distillation_as_a_Defense_to_Adversarial_Perturbations_Against_Deep_Neural_Networks","303942775_The_Mythos_of_Model_Interpretability","348496402_Adversarial_Examples_in_the_Physical_World","336215752_Algorithmic_Impact_Assessments_under_the_GDPR_Producing_Multi-layered_Explanations","333708433_A_General_Framework_for_Adversarial_Examples_with_Objectives","333626998_Data_Cleaning_for_Accurate_Fair_and_Robust_Models_A_Big_Data_-_AI_Integration_Approach","333069815_Stop_explaining_black_box_machine_learning_models_for_high_stakes_decisions_and_use_interpretable_models_instead","332958303_nGraph-HE_a_graph_compiler_for_deep_learning_on_homomorphically_encrypted_data","330268857_Model_Cards_for_Model_Reporting","329750836_Robust_Physical-World_Attacks_on_Deep_Learning_Visual_Classification","326854294_Audio_Adversarial_Examples_Targeted_Attacks_on_Speech-to-Text","326854186_Adversarial_Examples_for_Generative_Models","326193434_The_Right_to_Explanation_Explained","324459437_Robustness_of_deep_autoencoder_in_intrusion_detection_under_adversarial_contamination","322319200_Methodologic_Guide_for_Evaluating_Clinical_Performance_and_Effect_of_Artificial_Intelligence_Technology_for_Medical_Diagnosis_and_Prediction","322058778_Universal_Adversarial_Perturbations_Against_Semantic_Image_Segmentation","321174600_A_Layered_Model_for_AI_Governance","320836536_Efficient_Defenses_Against_Adversarial_Attacks","319770378_Explaining_and_harnessing_adversarial_examples","319770355_Generative_Adversarial_Nets","319394978_Artificial_Intelligence_---_A_Modern_Approach","317919653_Towards_Evaluating_the_Robustness_of_Neural_Networks","316505654_Deep_Text_Classification_Can_be_Fooled","309444608_Deep_Learning_with_Differential_Privacy","308061191_The_Algorithmic_Foundations_of_Differential_Privacy","306281834_Rethinking_the_Inception_Architecture_for_Computer_Vision","305342147_Why_Should_I_Trust_You_Explaining_the_Predictions_of_Any_Classifier","303032467_The_Limitations_of_Deep_Learning_in_Adversarial_Settings","301419711_Model_Inversion_Attacks_that_Exploit_Confidence_Information_and_Basic_Countermeasures","300925109_Machine_Learning_Classification_over_Encrypted_Data","292525996_Research_Priorities_for_Robust_and_Beneficial_Artificial_Intelligence","280940018_Pattern_Recognition_and_Machine_Learning","267665795_Understanding_machine_learning_From_theory_to_algorithms","265295439_ImageNet_Large_Scale_Visual_Recognition_Challenge","259701267_Best_Practices_for_Scientific_Computing","225124717_Calibrating_Noise_to_Sensitivity_in_Private_Data_Analysis","262173234_Adversarial_machine_learning","228095591_Poisoning_Attacks_against_Support_Vector_Machines"]}