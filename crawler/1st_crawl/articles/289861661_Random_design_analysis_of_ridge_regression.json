{"id":"289861661_Random_design_analysis_of_ridge_regression","abstract":"This work gives a simultaneous analysis of both the ordinary least squares estimator and the ridge regression estimator in the random design setting under mild assumptions on the covariate/response distributions. In particular, the analysis provides sharp results on the \"out-of-sample\" prediction error, as opposed to the \"in-sample\" (fixed design) error. The analysis also reveals the effect of errors in the estimated covariance structure, as well as the effect of modeling errors; neither of which effects are present in the fixed design setting. The proof of the main results are based on a simple decomposition lemma combined with concentration inequalities for random vectors and matrices.","authors":["D. Hsu","S.M. Kakade","T. Zhang"],"meta":["January 2012Journal of Machine Learning Research 23:9.1-9.24"],"references":["221497384_Optimal_Rates_for_Regularized_Least_Squares_Regression","266959677_Tail_inequalities_for_sums_of_random_matrices_that_depend_on_the_intrinsic_dimension","7752592_Learning_Bounds_for_Kernel_Regression_Using_Effective_Data_Dimensionality"]}