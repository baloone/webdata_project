{"id":"265554383_Sequence_to_Sequence_Learning_with_Neural_Networks","abstract":"Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT-14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.7 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a strong phrase-based\nSMT system achieves a BLEU score of 33.3 on the same dataset. When we used the\nLSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system,\nits BLEU score increases to 36.5, which beats the previous state of the art.\nThe LSTM also learned sensible phrase and sentence representations that are\nsensitive to word order and are relatively invariant to the active and the\npassive voice. Finally, we found that reversing the order of the words in all\nsource sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.","authors":["Ilya Sutskever","Oriol Vinyals","Quoc V. Le"],"meta":["September 2014Advances in Neural Information Processing Systems 4","SourcearXiv"],"references":["270877878_Fast_and_Robust_Neural_Network_Joint_Models_for_Statistical_Machine_Translation","265386055_Overcoming_the_Curse_of_Sentence_Length_for_Neural_Machine_Translation_using_Automatic_Segmentation","263086337_Edinburgh's_Phrase-based_Machine_Translation_Systems_for_WMT-14","243781690_Untersuchungen_zu_dynamischen_neuronalen_Netzen","221620298_LSTM_can_solve_hard_long_time_lag_problems","221618573_A_Neural_Probabilistic_Language_Model","221346365_Connectionist_temporal_classification_Labelling_unsegmented_sequence_data_with_recurrent_neural_'networks","51968606_Building_high-level_features_using_large_scale_unsupervised_learning","13853244_Long_Short-term_Memory","5583935_Learning_long-term_dependencies_with_gradient_descent_is_difficult","2985446_Gradient-Based_Learning_Applied_to_Document_Recognition","2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation","319770626_Multi-column_Deep_Neural_Networks_for_Image_Classification","319770183_Imagenet_classification_with_deep_convolutional_neural_networks","311469848_Recurrent_neural_network_based_language_model","289758666_Recurrent_continuous_translation_models","283112108_Joint_language_and_translation_modeling_with_recurrent_neural_networks","266458936_Statistical_Language_Models_Based_on_Neural_Networks","266030628_LSTM_Neural_Networks_for_Language_Modeling","265178583_Deep_Neural_Networks_for_Acoustic_Modeling_in_Speech_Recognition","255173850_Generating_Sequences_With_Recurrent_Neural_Networks","229091480_Learning_Representations_by_Back_Propagating_Errors","224226885_Context-Dependent_Pre-Trained_Deep_Neural_Networks_for_Large-Vocabulary_Speech_Recognition","2904844_On_Small_Depth_Threshold_Circuits"]}