{"id":"282060059_Contribution_to_Transfer_Entropy_Estimation_via_the_k-Nearest-Neighbors_Approach","abstract":"This paper deals with the estimation of transfer entropy based on the k-nearest neighbors (k-NN) method. To this end, we first investigate the estimation of Shannon entropy involving a rectangular neighboring region, as suggested in already existing literature, and develop two kinds of entropy estimators. Then, applying the widely-used error cancellation approach to these entropy estimators, we propose two novel transfer entropy estimators, implying no extra computational cost compared to existing similar k-NN algorithms. Experimental simulations allow the comparison of the new estimators with the transfer entropy estimator available in free toolboxes, corresponding to two different extensions to the transfer entropy estimation of the Kraskov-Stögbauer-Grassberger (KSG) mutual information estimator and prove the effectiveness of these new estimators.","authors":["Jie Zhu","Jean-Jcques Bellanger","Huazhong Shu","Régine Le Bouquin Jeannès"],"meta":["June 2015Entropy 17(6):4173-4201","DOI:10.3390/e17064173"],"references":["268079367_Efficient_Estimation_of_Mutual_Information_for_Strongly_Dependent_Variables","264790567_JIDT_An_Information-Theoretic_Toolkit_for_Studying_the_Dynamics_of_Complex_Systems","260586219_Direct_Causality_Detection_via_the_Transfer_Entropy_Approach","316233523_Specifying_the_directionality_of_fault_propagation_paths_using_transfer_entropy","288368445_Confounding_Effects_of_Indirect_Connections_on_Causality_Estimation","286303634_Transfer_entropy_in_neuroscience","278695757_Understanding_Complex_Systems","269416055_Bias_reduction_in_the_estimation_of_mutual_information","265654924_Some_modern_mathematics_for_physicists_and_other_outsiders_An_introduction_to_algebra_topology_and_functional_analysis_Vol_I_Algebra_topology_and_measure_theory","265472224_MuTE_A_MATLAB_Toolbox_to_Compare_Established_and_Novel_Estimators_of_the_Multivariate_Transfer_Entropy"]}