{"id":"343248033_Enhancing_Explainability_of_Neural_Networks_Through_Architecture_Constraints","abstract":"Prediction accuracy and model explainability are the two most important objectives when developing machine learning algorithms to solve real-world problems. Neural networks are known to possess good prediction performance but suffer from a lack of model interpretability. In this article, we propose to enhance the explainability of neural networks through the following architecture constraints: 1) sparse additive subnetworks; 2) projection pursuit with orthogonality constraint; and 3) smooth function approximation. It leads to an enhanced explainable neural network (ExNN) with a superior balance between prediction performance and model interpretability. We derive sufficient identifiability conditions for the proposed ExNN model. The multiple parameters are simultaneously estimated by a modified minibatch gradient descent method based on the backpropagation algorithm for calculating the derivatives and the Cayley transform for preserving the projection orthogonality. Through simulation study under six different scenarios, we compare the proposed method to several benchmarks, including least absolute shrinkage and selection operator, support vector machine, random forest, extreme learning machine, and multilayer perceptron. It is shown that the proposed ExNN model keeps the flexibility of pursuing high prediction accuracy while attaining improved interpretability. Finally, a real data example is employed as a showcase application.","authors":["Zebin Yang","Aijun Zhang","Agus Sudjianto"],"meta":["July 2020IEEE Transactions on Neural Networks and Learning Systems PP(99):1-12","DOI:10.1109/TNNLS.2020.3007259"],"references":["261597922_Generalised_additive_and_index_models_with_shape_constraints","257028373_Peeking_Inside_the_Black_Box_Visualizing_Statistical_Learning_With_Plots_of_Individual_Conditional_Expectation","228533892_A_feasible_method_for_optimization_with_orthogonality_constraints","225734295_The_Elements_of_Statistical_Learning_Data_Mining_Inference_and_Prediction","224881840_Sparse_Principal_Component_Analysis","220551236_MELM-GRBF_A_modified_version_of_the_extreme_learning_machine_for_generalized_radial_basis_function_neural_networks","215616968_Understanding_the_difficulty_of_training_deep_feedforward_neural_networks","4742165_Prediction_by_Supervised_Principal_Components","3301835_Regression_Modeling_in_Back-Propagation_and_Projection_Pursuit_Learning","342232632_Extreme_Learning_Machine_Theory_and_Applications","334276369_DARPA's_Explainable_Artificial_Intelligence_XAI_Program","320055611_A_Novel_Pruning_Algorithm_for_Smoothing_Feedforward_Neural_Networks_Based_on_Group_Lasso_Method","319769909_Distilling_the_Knowledge_in_a_Neural_Network","311925957_Visualizing_the_Effects_of_Predictor_Variables_in_Black_Box_Supervised_Learning_Models","305342147_Why_Should_I_Trust_You_Explaining_the_Predictions_of_Any_Classifier","289733708_Do_deep_nets_really_need_to_be_deep","280687718_Greedy_Function_Approximation_A_Gradient_Boosting_Machine","269935079_Adam_A_Method_for_Stochastic_Optimization","266509908_On_the_identifiability_of_additive_index_models","265263057_A_Note_on_the_Use_of_Principal_Components_in_Regression","260406235_Projection_Pursuit_Regression","243763658_Hornik_K_Approximation_Capabilities_of_Multilayer_Feedforward_Network_Neural_Networks_251-257","236736850_Spline_Models_of_Observational_Data","227701806_Sparse_Additive_Models","222491387_Semiparametric_Least_Squares_SLS_and_Weighted_SLS_Estimation_of_Single_Index_Models","220601503_Supervised_principal_component_analysis_Visualization_classification_and_regression_on_subspaces_and_submanifolds"]}