{"id":"318327308_Improvement_of_learning_for_CNN_with_ReLU_activation_by_sparse_regularization","abstract":"This paper introduces the sparse regularization forthe convolutional neural network (CNN) with the rectified linearunits (ReLU) in the hidden layers. By introducing the sparsenessfor the inputs of the ReLU, there is effect to push the inputs ofthe ReLU to zero in the learning process. Thus it is expectedthat the unnecessary increase of the outputs of the ReLU can beprevented. This is the similar effect with the Batch Normalization.Also the unnecessary negative values of the inputs of the ReLUcan be reduced by introducing the sparseness. This can improvethe generalization of the trained network. The relations betweenthe proposed approach and the Batch Normalization or themodifications of the activation function such as ExponentialLinear Unit (ELU) are also discussed. The effectiveness of the proposed method was confirmed through the detail experiments.","authors":["Hidenori Ide","Takio Kurita"],"meta":["May 2017","DOI:10.1109/IJCNN.2017.7966185","Conference: 2017 International Joint Conference on Neural Networks (IJCNN)","Project: Deep Learning"],"references":["319770387_Deep_Sparse_Rectifier_Neural_Networks","319770291_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition","319770183_Imagenet_classification_with_deep_convolutional_neural_networks","306281834_Rethinking_the_Inception_Architecture_for_Computer_Vision","306221946_Centering_neural_network_gradient_factors","289786324_Centering_Neural_Network_Gradient_Factors","285058764_Receptive_fields_binocular_interaction_and_functional_architecture_in_the_cat's_visual_cortex","283360188_Sparse_Coding_with_an_Overcomplete_Basis_Set_A_Strategy_Employed_by_V1","272194743_Batch_Normalization_Accelerating_Deep_Network_Training_by_Reducing_Internal_Covariate_Shift","265385906_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition"]}