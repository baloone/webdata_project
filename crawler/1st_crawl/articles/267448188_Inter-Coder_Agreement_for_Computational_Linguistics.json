{"id":"267448188_Inter-Coder_Agreement_for_Computational_Linguistics","abstract":"This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks -- but that their use makes the interpretation of the value of the coefficient even harder.","authors":["Ron Artstein","Massimo Poesio"],"meta":["December 2008Computational Linguistics 34(4):555-596","DOI:10.1162/coli.07-034-R2"],"references":["272581846_On_the_Reliability_of_Unitizing_Continuous_Data","269030981_The_reliability_of_anaphoric_annotation_reconsidered","342183905_A_Coefficient_of_Agreement_for_Nominal_Scales","313754895_The_kappa_statistic_a_second_look","313149148_Pragmatics_and_linguistics_Analysis_of_sentence_topics","290788047_Empirical_Analysis_of_Three_Dimensions_of_Spoken_Discourse_Segmentation_Coherence_and_Linguistic_Devices","285739795_Reliability_of_binary_attribute_data","284400357_Measures_of_the_Amount_of_Ecologic_Association_Between_Species","278670338_OntoNotes","275371833_Reliability_in_Content_Analysis_Some_Common_Misconceptions_and_Recommendations"]}