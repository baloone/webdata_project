{"id":"303655962_Stochastic_Optimization_for_Large-scale_Optimal_Transport","abstract":"Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation ; (b) entropic regularization of the primal OT problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional OT solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, apart from computing OT on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.","authors":["Genevay Aude","Marco Cuturi","Gabriel Peyr√©","Francis Bach"],"meta":["May 2016"],"references":["284576917_Glove_Global_Vectors_for_Word_Representation","278733698_Learning_with_a_Wasserstein_Loss","264498069_Non-parametric_Stochastic_Approximation_with_Large_Step_sizes","256476989_Minimizing_Finite_Sums_with_the_Stochastic_Average_Gradient","236736831_Acceleration_of_Stochastic_Approximation_by_Averaging","225904378_Asymptotic_analysis_of_the_exponential_penalty_trajectory_in_linear_programming","221653434_Incremental_approximate_matrix_factorization_for_speeding_up_support_vector_machines","220659330_The_Earth_Mover's_Distance_as_a_Metric_for_Image_Retrieval","309532479_Random_features_for_large_scale_kernel_machines","306146231_From_word_embeddings_to_document_distances","305336244_Erratum_to_Minimizing_finite_sums_with_the_stochastic_average_gradient","286513276_Convergence_of_Entropic_Schemes_for_Optimal_Transport_and_Gradient_Flows","279968704_Wasserstein_Training_of_Boltzmann_Machines","279421890_Support_Vector_Machines","247049035_On_the_transfer_of_masses","239230352_Topics_in_Optimal_Transportation_Theory","237053780_Sinkhorn_Distances_Lightspeed_Computation_of_Optimal_Transportation_Distances","227082096_Minkowski-Type_Theorems_and_Least-Squares_Clustering","223139045_On_the_scaling_of_multidimensional_matrices","221620515_Random_Features_for_Large-Scale_Kernel_Machines","220507073_A_Multiscale_Approach_to_Optimal_Transport","38365974_A_Relationship_Between_Arbitrary_Positive_Matrices_and_Doubly_Stochastic_Matrices","23635500_On_minimum_Kantorovich_distance_estimators"]}