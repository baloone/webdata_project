{"id":"309461050_Broad_Context_Language_Modeling_as_Reading_Comprehension","abstract":"Progress in text understanding has been driven by the availability of large datasets that test particular capabilities, like recent datasets for assessing reading comprehension. We focus here on the LAMBADA dataset, a word prediction task requiring broader context than the immediate sentence. We view the LAMBADA task as a reading comprehension problem and apply off-the-shelf comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 45.4%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed.","authors":["Zewei Chu","Hai Wang","Kevin Gimpel","David McAllester"],"meta":["October 2016"],"references":["306357482_Who_did_What_A_Large-Scale_Person-Centered_Cloze_Dataset","306093716_The_LAMBADA_dataset_Word_prediction_requiring_a_broad_discourse_context","305388870_Attention-over-Attention_Neural_Networks_for_Reading_Comprehension","283659163_The_Goldilocks_Principle_Reading_Children's_Books_with_Explicit_Memory_Representations","279068490_Aligning_Books_and_Movies_Towards_Story-Like_Visual_Explanations_by_Watching_Movies_and_Reading_Books","306094228_A_Thorough_Examination_of_the_CNNDaily_Mail_Reading_Comprehension_Task","306093209_Text_Understanding_with_the_Attention_Sum_Reader_Network","303821574_Gated-Attention_Readers_for_Text_Comprehension","286965813_MCTest_A_challenge_dataset_for_the_open-domain_machine_comprehension_of_text","278048272_Teaching_Machines_to_Read_and_Comprehend"]}