{"id":"344878544_Pathological_Visual_Question_Answering","abstract":"Is it possible to develop an \"AI Pathologist\" to pass the board-certified examination of the American Board of Pathology (ABP)? To build such a system, three challenges need to be addressed. First, we need to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Due to privacy concerns, pathology images are usually not publicly available. Besides, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. The second challenge is: since it is difficult to hire highly experienced pathologists to create pathology visual questions and answers, the resulting pathology VQA dataset may contain errors. Training pathology VQA models using these noisy or even erroneous data will lead to problematic models that cannot generalize well on unseen images. The third challenge is: the medical concepts and knowledge covered in pathology question-answer (QA) pairs are very diverse while the number of QA pairs available for modeling training is limited. How to learn effective representations of diverse medical concepts based on limited data is technically demanding. In this paper, we aim to address these three challenges. To our best knowledge, our work represents the first one addressing the pathology VQA problem. To deal with the issue that a publicly available pathology VQA dataset is lacking, we create PathVQA dataset. To address the second challenge, we propose a learning-by-ignoring approach. To address the third challenge, we propose to use cross-modal self-supervised learning. We perform experiments on our created PathVQA dataset and the results demonstrate the effectiveness of our proposed learning-by-ignoring method and cross-modal self-supervised learning methods.","authors":["Xuehai He","Zhuo Cai","Wenlan Wei","Yichen Zhang"],"meta":["October 2020","DOI:10.36227/techrxiv.13127537"],"references":["335716753_VQA-Med_Overview_of_the_Medical_Visual_Question_Answering_Task_at_ImageCLEF_2019","327588289_Making_the_V_in_VQA_Matter_Elevating_the_Role_of_Image_Understanding_in_Visual_Question_Answering","340642007_Perfect_Match_Self-Supervised_Embeddings_for_Cross-Modal_Retrieval","339561791_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning","338512165_Reinforced_Cross-Modal_Matching_and_Self-Supervised_Imitation_Learning_for_Vision-Language_Navigation","329743791_Unsupervised_Feature_Learning_via_Non-parametric_Instance_Discrimination","320967203_CLEVR_A_Diagnostic_Dataset_for_Compositional_Language_and_Elementary_Visual_Reasoning","320964614_Are_You_Smarter_Than_a_Sixth_Grader_Textbook_Question_Answering_for_Multimodal_Machine_Comprehension","317558625_Attention_Is_All_You_Need","314100361_Indoor_Segmentation_and_Support_Inference_from_RGBD_Images"]}