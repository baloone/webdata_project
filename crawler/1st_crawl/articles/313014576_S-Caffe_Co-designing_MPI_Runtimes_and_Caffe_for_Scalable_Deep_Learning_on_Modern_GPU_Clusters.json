{"id":"313014576_S-Caffe_Co-designing_MPI_Runtimes_and_Caffe_for_Scalable_Deep_Learning_on_Modern_GPU_Clusters","abstract":"Availability of large data sets like ImageNet and massively parallel computation support in modern HPC devices like NVIDIA GPUs have fueled a renewed interest in Deep Learning (DL) algorithms. This has triggered the development of DL frameworks like Caffe, Torch, TensorFlow, and CNTK. However, most DL frameworks have been limited to a single node. In order to scale out DL frameworks and bring HPC capabilities to the DL arena, we propose, S-Caffe; a scalable and distributed Caffe adaptation for modern multi-GPU clusters. With an in-depth analysis of new requirements brought forward by the DL frameworks and limitations of current communication runtimes, we present a co-design of the Caffe framework and the MVAPICH2-GDR MPI runtime. Using the co-design methodology, we modify Caffe's workflow to maximize the overlap of computation and communication with multi-stage data propagation and gradient aggregation schemes. We bring DL-Awareness to the MPI runtime by proposing a hierarchical reduction design that benefits from CUDA-Aware features and provides up to a massive 133x speedup over OpenMPI and 2.6x speedup over MVAPICH2 for 160 GPUs. S-Caffe successfully scales up to 160 K-80 GPUs for GoogLeNet (ImageNet) with a speedup of 2.5x over 32 GPUs. To the best of our knowledge, this is the first framework that scales up to 160 GPUs. Furthermore, even for single node training, S-Caffe shows an improvement of 14\\% and 9\\% over Nvidia's optimized Caffe for 8 and 16 GPUs, respectively. In addition, S-Caffe achieves up to 1395 samples per second for the AlexNet model, which is comparable to the performance of Microsoft CNTK.","authors":["Ammar Ahmad Awan","Khaled Hamidouche","Jahanzeb Maqbool Hashmi","D.K. Panda"],"meta":["January 2017ACM SIGPLAN Notices 52(8):193-205","DOI:10.1145/3018743.3018769","Conference: PPoPP 2017","Project: MVAPICH: MPI over InfiniBand, Omni-Path, Ethernet/iWARP, and RoCE"],"references":["311610099_FireCaffe_Near-Linear_Acceleration_of_Deep_Neural_Network_Training_on_Compute_Clusters","305196650_Going_deeper_with_convolutions","304163398_Deep_Learning_for_Identifying_Metastatic_Breast_Cancer","301842011_Distributed_TensorFlow_with_MPI","284476468_Comparative_Study_of_Caffe_Neon_Theano_and_Torch_for_Deep_Learning","283471410_FireCaffe_near-linear_acceleration_of_deep_neural_network_training_on_compute_clusters","266225209_Large_Scale_Distributed_Deep_Networks","264979485_Caffe_Convolutional_Architecture_for_Fast_Feature_Embedding","234817013_ParFiSys_a_parallel_file_system_for_MPP","233753224_Theano_new_features_and_speed_improvements","221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database","220951240_Overlapping_computation_and_communication_Barrier_algorithms_and_ConnectX-2_CORE-Direct_capabilities","220781718_Implementation_and_performance_analysis_of_non-blocking_collective_operations_for_MPI","319770291_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition","319770183_Imagenet_classification_with_deep_convolutional_neural_networks","313390438_Advances_in_Neural_Information_Processing_Systems","306218037_Learning_multiple_layers_of_features_from_tiny_images","306209959_Project_adam_Building_an_efficient_and_scalable_deep_learning_training_system","301282080_GeePS_scalable_deep_learning_on_distributed_GPUs_with_a_GPU-specialized_parameter_server","287762956_Deep_learning_with_COTS_HPC_systems","284218707_Why_M_Heads_are_Better_than_One_Training_a_Diverse_Ensemble_of_Deep_Networks","267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks","265385906_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition","261880160_One_weird_trick_for_parallelizing_convolutional_neural_networks","235622476_CUDA_programming_guide","222411269_General_purpose_molecular_dynamics_simulations_fully_implemented_on_graphics_processing_units","2865385_Torch_A_Modular_Machine_Learning_Software_Library"]}