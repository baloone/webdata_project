{"id":"334960361_JUST_at_ImageCLEF_2019_Visual_Question_Answering_in_the_Medical_Domain","abstract":"This paper describes our method for the Medical Domain Visual Question Answering (VQA-Med) Task of ImageCLEF 2019. The aim is to build a model that is able to answer questions about medical images. Our proposed model consists of sub-models, each specializing in answering a specific type of questions. Specifically, the sub-models we have are: \"plane\" model, \"organ systems\" model, \"modality\" models , and \"abnormality\" models. All of these models are basically image classification models based on pre-trained VGG16 network. We do not rely on the questions for the answers prediction since the questions on each type are repetitive. However, we do rely on them to determine the suitable model to be used for producing the answers and determine the suitable answer format. Our best model achieves 57% accuracy and 0.591 BLEU score.","authors":["Aisha Al-Sadi","Bashar Talafha","Mahmoud Al-Ayyoub","Yaser Jararweh"],"meta":["August 2019","Conference: CLEF 2019"],"references":["328491475_NLM_at_ImageCLEF_2018_Visual_Question_Answering_in_the_Medical_Domain","327867799_Deep_Neural_Networks_and_Decision_Tree_Classifier_for_Visual_Question_Answering_in_the_Medical_Domain","327834258_Overview_of_ImageCLEF_2018_Medical_Domain_Visual_Question_Answering_Task","326718868_JUST_at_VQA-Med_A_VGG-Seq2Seq_Model","311609041_Deep_Residual_Learning_for_Image_Recognition"]}