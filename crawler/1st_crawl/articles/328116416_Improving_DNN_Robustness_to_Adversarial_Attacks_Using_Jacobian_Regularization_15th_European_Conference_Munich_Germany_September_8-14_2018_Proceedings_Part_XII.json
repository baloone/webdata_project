{"id":"328116416_Improving_DNN_Robustness_to_Adversarial_Attacks_Using_Jacobian_Regularization_15th_European_Conference_Munich_Germany_September_8-14_2018_Proceedings_Part_XII","abstract":"Deep neural networks have lately shown tremendous performance in various applications including vision and speech processing tasks. However, alongside their ability to perform these tasks with such high accuracy, it has been shown that they are highly susceptible to adversarial attacks: a small change in the input would cause the network to err with high confidence. This phenomenon exposes an inherent fault in these networks and their ability to generalize well. For this reason, providing robustness to adversarial attacks is an important challenge in networks training, which has led to extensive research. In this work, we suggest a theoretically inspired novel approach to improve the networks’ robustness. Our method applies regularization using the Frobenius norm of the Jacobian of the network, which is applied as post-processing, after regular training has finished. We demonstrate empirically that it leads to enhanced robustness results with a minimal change in the original network’s accuracy.","authors":["Daniel Jakubovitz","Raja Giryes"],"meta":["September 2018","DOI:10.1007/978-3-030-01258-8_32","In book: Computer Vision – ECCV 2018 (pp.525-541)"],"references":["325000455_Towards_Robust_Deep_Neural_Networks_with_BANG","323248809_Knock_Knock_Who's_There_Membership_Inference_on_Aggregate_Location_Data","319642944_Ensemble_Methods_as_a_Defense_to_Adversarial_Perturbations_Against_Deep_Neural_Networks","313713721_On_Detecting_Adversarial_Perturbations","306304648_Distillation_as_a_Defense_to_Adversarial_Perturbations_Against_Deep_Neural_Networks","301839500_TensorFlow_Large-Scale_Machine_Learning_on_Heterogeneous_Distributed_Systems","284788388_The_Limitations_of_Deep_Learning_in_Adversarial_Settings","284219659_Understanding_Adversarial_Training_Increasing_Local_Stability_of_Neural_Nets_through_Robust_Optimization","284097112_Distillation_as_a_Defense_to_Adversarial_Perturbations_against_Deep_Neural_Networks","279632719_Distributional_Smoothing_with_Virtual_Adversarial_Training","269935591_Explaining_and_Harnessing_Adversarial_Examples","259440613_Intriguing_properties_of_neural_networks","322058622_Adversarial_Image_Perturbation_for_Privacy_Protection_A_Game_Theory_Perspective","322057978_SafetyNet_Detecting_and_Rejecting_Adversarial_Examples_Robustly","322057883_Adversarial_Examples_Detection_in_Deep_Networks_with_Convolutional_Filter_Statistics","321325128_Improving_the_Adversarial_Robustness_and_Interpretability_of_Deep_Neural_Networks_by_Regularizing_their_Input_Gradients","320968636_Universal_Adversarial_Perturbations","320727277_Certifiable_Distributional_Robustness_with_Principled_Adversarial_Training","320671312_MagNet_A_Two-Pronged_Defense_against_Adversarial_Examples","319770131_DeepFool_a_simple_and_accurate_method_to_fool_deep_neural_networks","317919653_Towards_Evaluating_the_Robustness_of_Neural_Networks","317673614_Towards_Deep_Learning_Models_Resistant_to_Adversarial_Attacks","317187337_Analysis_of_universal_adversarial_perturbations","317161380_Robust_Large_Margin_Deep_Neural_Networks","317101045_Formal_Guarantees_on_the_Robustness_of_a_Classifier_against_Adversarial_Manipulation","316598963_Parseval_Networks_Improving_Robustness_to_Adversarial_Examples","315764926_SafetyNet_Detecting_and_Rejecting_Adversarial_Examples_Robustly","311610675_DeepFool_A_Simple_and_Accurate_Method_to_Fool_Deep_Neural_Networks","311482688_TensorFlow_learning_functions_at_scale","306226844_Towards_Evaluating_the_Robustness_of_Neural_Networks","303032467_The_Limitations_of_Deep_Learning_in_Adversarial_Settings","269935079_Adam_A_Method_for_Stochastic_Optimization","228095666_Estimating_the_Hessian_by_Back-propagating_Curvature"]}