{"id":"258082948_Relative_Deviation_Learning_Bounds_and_Generalization_with_Unbounded_Loss_Functions","abstract":"We present an extensive analysis of relative deviation bounds, including\ndetailed proofs of two-sided inequalities and their implications. We also give\ndetailed proofs of two-sided generalization bounds that hold in the general\ncase of unbounded loss functions, under the assumption that a moment of the\nloss is bounded. These bounds are useful in the analysis of importance\nweighting and other learning tasks such as unbounded regression.","authors":["Corinna Cortes","Spencer Greenberg","Mehryar Mohri"],"meta":["January 2019Annals of Mathematics and Artificial Intelligence 85(3)","DOI:10.1007/s10472-018-9613-y","SourcearXiv"],"references":["258229331_Risk_bounds_for_statistical_learning","316806101_Springer_Series_in_Statistics","313148460_A_course_on_empirical_processes","313107373_Direct_importance_estimation_with_model_selection_and_its_application_to_covariate_shift_adaptation","297995041_Discussion_Local_Rademacher_complexities_and_oracle_inequalities_in_risk_minimization","267169218_Estimation_of_dependences_based_on_empirical_data_Transl_from_the_Russian_by_Samuel_Kotz","265669279_Convergence_of_Stochastic_Process","265366875_Local_Rademacher_complexities_and_oracle_inequalities_in_risk_minimization_2004_IMS_Medallion_Lecture_With_discussions_and_rejoinder","259992771_Domain_adaptation_and_sample_bias_correction_theory_and_algorithm_for_regression","259528880_Learning_without_Concentration"]}