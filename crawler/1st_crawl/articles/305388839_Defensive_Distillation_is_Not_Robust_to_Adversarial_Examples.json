{"id":"305388839_Defensive_Distillation_is_Not_Robust_to_Adversarial_Examples","abstract":"We show that defensive distillation is not secure: it is no more resistant to targeted misclassification attacks than unprotected neural networks.","authors":["Nicholas Carlini","David Wagner"],"meta":["July 2016"],"references":["306304648_Distillation_as_a_Defense_to_Adversarial_Perturbations_Against_Deep_Neural_Networks","301839500_TensorFlow_Large-Scale_Machine_Learning_on_Heterogeneous_Distributed_Systems","284097112_Distillation_as_a_Defense_to_Adversarial_Perturbations_against_Deep_Neural_Networks","259440613_Intriguing_properties_of_neural_networks","2985446_Gradient-Based_Learning_Applied_to_Document_Recognition","303032467_The_Limitations_of_Deep_Learning_in_Adversarial_Settings","247931959_The_mnist_database_of_handwritten_digits"]}