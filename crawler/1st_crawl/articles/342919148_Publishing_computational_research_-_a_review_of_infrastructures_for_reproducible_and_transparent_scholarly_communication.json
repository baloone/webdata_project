{"id":"342919148_Publishing_computational_research_-_a_review_of_infrastructures_for_reproducible_and_transparent_scholarly_communication","abstract":"Background: \nThe trend toward open science increases the pressure on authors to provide access to the source code and data they used to compute the results reported in their scientific papers. Since sharing materials reproducibly is challenging, several projects have developed solutions to support the release of executable analyses alongside articles.\n\nMethods: \nWe reviewed 11 applications that can assist researchers in adhering to reproducibility principles. The applications were found through a literature search and interactions with the reproducible research community. An application was included in our analysis if it (i) was actively maintained at the time the data for this paper was collected, (ii) supports the publication of executable code and data, (iii) is connected to the scholarly publication process. By investigating the software documentation and published articles, we compared the applications across 19 criteria, such as deployment options and features that support authors in creating and readers in studying executable papers.\n\nResults: \nFrom the 11 applications, eight allow publishers to self-host the system for free, whereas three provide paid services. Authors can submit an executable analysis using Jupyter Notebooks or R Markdown documents (10 applications support these formats). All approaches provide features to assist readers in studying the materials, e.g., one-click reproducible results or tools for manipulating the analysis parameters. Six applications allow for modifying materials after publication.\n\nConclusions: \nThe applications support authors to publish reproducible research predominantly with literate programming. Concerning readers, most applications provide user interfaces to inspect and manipulate the computational analysis. The next step is to investigate the gaps identified in this review, such as the costs publishers have to expect when hosting an application, the consideration of sensitive data, and impacts on the review process.","authors":["Markus Konkol","Daniel NÃ¼st","Laura Goulier"],"meta":["July 2020Research Integrity and Peer Review 5(10)","DOI:10.1186/s41073-020-00095-y"],"references":["346420086_Physics_Examples_for_Reproducible_Analysis","345149002_Reproducible_research_and_GIScience_an_evaluation_using_AGILE_conference_papers","337053051_Publishers'_Responsibilities_in_Promoting_Data_Quality_and_Reproducibility","336016769_CODECHECK_An_open-science_initiative_to_facilitate_sharing_of_computer_programs_and_results_presented_in_scientific_publications","335862922_REANA_A_System_for_Reusable_Research_Data_Analyses","333796190_Creating_Interactive_Scientific_Publications_using_Bindings","340661626_Open_Chemistry_JupyterLab_REST_and_Quantum_Chemistry","340129712_Certify_reproducibility_with_confidential_data","338246493_A_toast_to_the_error_detectors","337202752_Computational_Reproducibility_via_Containers_in_Psychology"]}