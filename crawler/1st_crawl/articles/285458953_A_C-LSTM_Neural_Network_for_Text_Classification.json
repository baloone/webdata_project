{"id":"285458953_A_C-LSTM_Neural_Network_for_Text_Classification","abstract":"Neural network models have been demonstrated to be capable of achieving\nremarkable performance in sentence and document modeling. Convolutional neural\nnetwork (CNN) and recurrent neural network (RNN) are two mainstream\narchitectures for such modeling tasks, which adopt totally different ways of\nunderstanding natural languages. In this work, we combine the strengths of both\narchitectures and propose a novel and unified model called C-LSTM for sentence\nrepresentation and text classification. C-LSTM utilizes CNN to extract a\nsequence of higher-level phrase representations, and are fed into a long\nshort-term memory recurrent neural network (LSTM) to obtain the sentence\nrepresentation. C-LSTM is able to capture both local features of phrases as\nwell as global and temporal sentence semantics. We evaluate the proposed\narchitecture on sentiment classification and question classification tasks. The\nexperimental results show that the C-LSTM outperforms both CNN and LSTM and can\nachieve excellent performance on these tasks.","authors":["Chunting Zhou","Chonglin Sun","Zhiyuan Liu","Francis C. M. Lau"],"meta":["November 2015","SourcearXiv"],"references":["275280239_Self-Adaptive_Hierarchical_Sentence_Model","274499291_Discriminative_Neural_Sentence_Modeling_by_Tree-Based_Convolution","273067823_Improved_Semantic_Representations_From_Tree-Structured_Long_Short-Term_Memory_Networks","266201822_Natural_Language_Processing_Almost_from_Scratch","262877889_Learning_Phrase_Representations_using_RNN_Encoder-Decoder_for_Statistical_Machine_Translation","308872979_Convolutional_Long_Short-Term_Memory_fully_connected_Deep_Neural_Networks","307955489_Distributed_representations_of_words_and_phrases_and_their_compositionality","301446024_Document_Modeling_with_Gated_Recurrent_Neural_Network_for_Sentiment_Classification","281145157_Molding_CNNs_for_text_Non-linear_non-consecutive_convolutions","270878508_Parsing_with_Compositional_Vector_Grammars"]}