{"id":"272195367_Gated_Feedback_Recurrent_Neural_Networks","abstract":"In this work, we propose a novel recurrent neural network (RNN) architecture.\nThe proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of\nstacking multiple recurrent layers by allowing and controlling signals flowing\nfrom upper recurrent layers to lower layers using a global gating unit for each\npair of layers. The recurrent signals exchanged between layers are gated\nadaptively based on the previous hidden states and the current input. We\nevaluated the proposed GF-RNN with different types of recurrent units, such as\ntanh, long short-term memory and gated recurrent units, on the tasks of\ncharacter-level language modeling and Python program evaluation. Our empirical\nevaluation of different RNN units, revealed that in both tasks, the GF-RNN\noutperforms the conventional approaches to build deep stacked RNNs. We suggest\nthat the improvement arises because the GF-RNN can adaptively assign different\nlayers to different timescales and layer-to-layer interactions (including the\ntop-down ones which are not usually present in a stacked RNN) by learning to\ngate these interactions.","authors":["Junyoung Chung","Caglar Gulcehre","Kyunghyun Cho","Y. Bengio"],"meta":["February 2015","SourcearXiv"],"references":["266485700_SUBWORD_LANGUAGE_MODELING_WITH_NEURAL_NETWORKS","265252627_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate","262877889_Learning_Phrase_Representations_using_RNN_Encoder-Decoder_for_Statistical_Machine_Translation","319770465_Sequence_to_Sequence_Learning_with_Neural_Networks","303256841_Theano_a_CPU_and_GPU_math_expression_compiler","285228693_Training_and_analysing_deep_recurrent_neural_networks","269935079_Adam_A_Method_for_Stochastic_Optimization","266458936_Statistical_Language_Models_Based_on_Neural_Networks","265554383_Sequence_to_Sequence_Learning_with_Neural_Networks","265469170_Recurrent_Neural_Network_Regularization"]}