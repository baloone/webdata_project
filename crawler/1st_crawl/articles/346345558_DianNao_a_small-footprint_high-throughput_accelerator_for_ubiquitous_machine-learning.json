{"id":"346345558_DianNao_a_small-footprint_high-throughput_accelerator_for_ubiquitous_machine-learning","abstract":"Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.\nUntil now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy.\nWe show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.","authors":["Tianshi Chen","Zidong Du","Ninghui Sun","Jia Wang"],"meta":["April 2014ACM SIGPLAN Notices 49(4):269-284","DOI:10.1145/2644865.2541967"],"references":["262289160_Learning_deep_structured_semantic_models_for_web_search_using_clickthrough_data","254006874_QSCORES_Trading_Dark_Silicon_for_Scalable_Energy_Efficiency_with_Quasi-Specific_Cores","224264100_A_digital_neurosynaptic_core_using_embedded_crossbar_memory_with_45pJ_per_spike_in_45nm","221574266_Reconciling_Specialization_and_Flexibility_Through_Compound_Circuits","221531727_SpiNNaker_Mapping_Neural_Networks_onto_a_Massively-Parallel_Chip_Multiprocessor","221345414_An_empirical_evaluation_of_deep_architectures_on_problems_with_many_factors_of_variation","221005540_McPAT_An_integrated_power_area_and_timing_modeling_framework_for_multicore_and_manycore_architectures","220884781_The_PARSEC_benchmark_suite_Characterization_and_architectural_implications","220770895_Understanding_sources_of_inefficiency_in_general-purpose_chips","220365880_A_2014_GOPS_496_mW_Real-Time_Multi-Object_Recognition_Processor_With_Bio-Inspired_Neural_Perception_Engine","216792688_NeuFlow_A_Runtime-Reconfigurable_Dataflow_Processor_for_Vision","11253286_Draghici_S_On_the_Capabilities_of_Neural_Networks_using_Limited_Precision_Weights_Neural_Networks_153_395-414","6576879_Robust_Object_Recognition_with_Cortex-Like_Mechanisms","6527229_Dynamically_Reconfigurable_Silicon_Array_of_Spiking_Neurons_With_Conductance-Based_Synapses","2985446_Gradient-Based_Learning_Applied_to_Document_Recognition","285621547_Dark_silicon_and_the_end_of_multicore_scaling_SIGARCH_Comput","271513141_Support_Vector_Networks","262174473_Convolution_Engine_Balancing_Efficiency_and_Flexibility_in_Specialized_Computing","261446913_BenchNN_On_the_broad_potential_application_scope_of_hardware_neural_network_accelerators","254005623_Accelerating_neuromorphic_vision_algorithms_for_recognition","225943507_An_Efficient_Hardware_Architecture_for_a_Neural_Network_Activation_Function_Generator","224393341_Bridging_the_computation_gap_between_programmable_processors_and_hardwired_accelerators","223778353_Software_Assistance_for_Data_Caches","221005576_Low-power_high-performance_analog_neural_branch_prediction","3043079_Finite_precision_error_analysis_of_neural_network_hardwareimplementations"]}