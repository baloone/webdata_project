{"id":"233923318_Alternating_Maximization_Unifying_Framework_for_8_Sparse_PCAFormulations_and_Efficient_Parallel_Codes","abstract":"Given a multivariate data set, sparse principal component analysis (SPCA)\naims to extract several linear combinations of the variables that together\nexplain the variance in the data as much as possible, while controlling the\nnumber of nonzero loadings in these combinations. In this paper we consider 8\ndifferent optimization formulations for computing a single sparse loading\nvector; these are obtained by combining the following factors: we employ two\nnorms for measuring variance (L2, L1) and two sparsity-inducing norms (L0, L1),\nwhich are used in two different ways (constraint, penalty). Three of our\nformulations, notably the one with L0 constraint and L1 variance, have not been\nconsidered in the literature. We give a unifying reformulation which we propose\nto solve via a natural alternating maximization (AM) method. We show the the AM\nmethod is nontrivially equivalent to GPower (Journ\\'{e}e et al; JMLR\n11:517--553, 2010) for all our formulations. Besides this, we provide 24\nefficient parallel SPCA implementations: 3 codes (multi-core, GPU and cluster)\nfor each of the 8 problems. Parallelism in the methods is aimed at i) speeding\nup computations (our GPU code can be 100 times faster than an efficient serial\ncode written in C++), ii) obtaining solutions explaining more variance and iii)\ndealing with big data problems (our cluster code is able to solve a 357 GB\nproblem in about a minute).","authors":["Peter Richtárik","Martin Takáč","Selin Damla AHIPASAOGLU"],"meta":["September 2021Optimization and Engineering 22(3)","DOI:10.1007/s11081-020-09562-3","SourcearXiv"],"references":["301481259_The_Sparse_Principal_Component_Analysis_Problem_Optimality_Conditions_and_Algorithms","283500707_Decomposition_into_Low-rank_plus_Additive_Matrices_for_BackgroundForeground_Separation_A_Review_for_a_Comparative_Evaluation_with_a_Large-Scale_Dataset","281200449_Sparse_PCA_for_High-Dimensional_Data_With_Outliers","330071329_Certifiably_optimal_sparse_principal_component_analysis","327767128_Statistical_Learning_with_Sparsity_The_Lasso_and_Generalizations","327250509_Dual_smoothing_and_value_function_techniques_for_variational_matrix_decomposition","286168863_Fantope_projection_and_selection_A_near-optimal_convex_relaxation_of_sparse_PCA","272521947_NP-Hardness_and_Inapproximability_of_Sparse_PCA","266241850_Finding_Sparse_Approximations_to_Extreme_Eigenvectors_Generalized_Power_Method_for_Sparse_PCA_and_Extensions","263696764_Principal_Component_Analysis_Springer-Verlag_2nd_Ed_New_York"]}