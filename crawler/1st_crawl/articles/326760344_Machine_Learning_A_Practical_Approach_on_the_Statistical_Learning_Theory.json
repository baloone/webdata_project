{"id":"326760344_Machine_Learning_A_Practical_Approach_on_the_Statistical_Learning_Theory","abstract":"This book presents the Statistical Learning Theory in a detailed and easy to understand way, by using practical examples, algorithms and source codes. It can be used as a textbook in graduation or undergraduation courses, for self-learners, or as reference with respect to the main theoretical concepts of Machine Learning. Fundamental concepts of Linear Algebra and Optimization applied to Machine Learning are provided, as well as source codes in R, making the book as self-contained as possible.\nIt starts with an introduction to Machine Learning concepts and algorithms such as the Perceptron, Multilayer Perceptron and the Distance-Weighted Nearest Neighbors with examples, in order to provide the necessary foundation so the reader is able to understand the Bias-Variance Dilemma, which is the central point of the Statistical Learning Theory.\nAfterwards, we introduce all assumptions and formalize the Statistical Learning Theory, allowing the practical study of different classification algorithms. Then, we proceed with concentration inequalities until arriving to the Generalization and the Large-Margin bounds, providing the main motivations for the Support Vector Machines.\nFrom that, we introduce all necessary optimization concepts related to the implementation of Support Vector Machines. To provide a next stage of development, the book finishes with a discussion on SVM kernels as a way and motivation to study data spaces and improve classification results.","authors":["Rodrigo Mello","Moacir Ponti"],"meta":["January 2018","DOI:10.1007/978-3-319-94989-5","ISBN: 978-3-319-94988-8"],"references":["321347527_Providing_theoretical_learning_guarantees_to_Deep_Learning_Networks","269776374_A_Survey_on_Data_Stream_Clustering_and_Classification","262191158_Nonlinear_Modeling_Estimation_and_Predictive_Control_in_APMonitor","261393553_Combining_Classifiers_From_the_Creation_of_Ensembles_to_the_Decision_Fusion","237374298_Based_on_the_Appendix_of_the_textbook_A_Probabilistic_Theory_of_Pattern_Recognition","228057795_A_New_Polynomial-Time_Algorithm_for_Linear_Programming-II","225414061_On_Projected_Newton_Barrier_Methods_for_Linear_Programming_and_an_Equivalence_to_Karmarkar's_Projective_Method","220320792_Characterization_Stability_and_Convergence_of_Hierarchical_Clustering_Methods","23545348_Penalised_spline_support_vector_classifiers_Computational_issues","2865065_Efficient_SVM_Training_Using_Low-Rank_Kernel_Representations","310839612_Applying_a_kernel_function_on_time-dependent_data_to_provide_supervised-learning_guarantees","307881957_LIBSVM_A_library_for_support_vector_machines","301761348_Using_Dynamical_Systems_Tools_to_Detect_Concept_Drift_in_Data_Streams","268045726_Computational_Experience_in_Solving_Linear_Programs","265675726_Linear_Programming_2_Theory_and_Extensions","265619909_Nonlinear_programming_sequential_unconstrained_minimization_techniques_Unabridged_corrected_republication","253116803_Reply_to_a_letter_by_Weissman_on_Stirling's_approximation","243489760_An_Improved_Analytical_Approximation_to_n","239557089_The_perceptron_-_a_perceiving_and_recognizing_automaton","238878759_Historical_Development_of_the_Newton-Raphson_Method","234793335_A_theory_of_the_learnable","233784971_On_the_Uniform_Convergence_of_Relative_Frequencies_of_Events_to_Their_Probabilities","230876473_Prediction_and_Entropy_of_Printed_English","224982370_Neural_Networks_-_A_Comprehensive_Foundation","220133628_Interior-Point_Methods_for_Massive_Support_Vector_Machines"]}