{"id":"311221666_A_differential_equation_for_modeling_Nesterov's_accelerated_gradient_method_Theory_and_insights","abstract":"We derive a second-order ordinary differential equation (ODE) which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.","authors":["Weijie Su","Saxon Boyd","E.J. Cand√®s"],"meta":["September 2016"],"references":["265777799_Nonlinear_Optimization","263967823_SLOPE_--_Adaptive_Variable_Selection_via_Convex_Optimization","261655866_Dynamical_systems_approach_to_constrained_minimization","260365607_An_adaptive_accelerated_first-order_method_for_convex_optimization","285906939_Numerical_optimization","265456293_Optimization_and_dynamical_systems_Foreword_by_R_W_Brockett","261051968_A_smooth_vector_field_for_quadratic_programming","260365721_On_accelerated_proximal_gradient_methods_for_convex-concave_optimization","243787569_Introductory_Lectures_on_Convex_Optimization_A_Basic_Course","236736838_Minimization_Methods_for_Non-Differentiable_Functions"]}