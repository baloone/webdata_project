{"id":"320796639_Still_not_systematic_after_all_these_years_On_the_compositional_skills_of_sequence-to-sequence_recurrent_networks","abstract":"Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb \"dax,\" he or she can immediately understand the meaning of \"dax twice\" or \"sing and dax.\" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply \"mix-and-match\" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the \"dax\" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.","authors":["Brenden M. Lake","Marco Baroni"],"meta":["October 2017"],"references":["318742071_Neural_Semantic_Parsing_over_Multiple_Knowledge-bases","301879687_Building_Machines_That_Learn_and_Think_Like_People","264093538_Generalisation_towards_Combinatorial_Productivity_in_Language_Acquisition_by_Simple_Recurrent_Networks","220233697_Lack_of_combinatorial_productivity_in_language_processing_with_simple_recurrent_networks","28762598_Are_Feedforward_and_Recurrent_Networks_Systematic_Analysis_and_Implications_for_a_Connectionist_Cognitive_Architecture","13853244_Long_Short-term_Memory","332087361_The_Algebraic_Mind_Integrating_Connectionism_and_Cognitive_Science","319770465_Sequence_to_Sequence_Learning_with_Neural_Networks","309091100_Hybrid_computing_using_a_neural_network_with_dynamic_external_memory","308646556_Google's_Neural_Machine_Translation_System_Bridging_the_Gap_between_Human_and_Machine_Translation","306093723_Language_to_Logical_Form_with_Neural_Attention","306093537_Data_Recombination_for_Neural_Semantic_Parsing","290310918_Getting_Real_about_Systematicity","284787959_A_Roadmap_towards_Machine_Intelligence","228003061_Generalization_and_Connectionist_Language_Learning","222449846_Finding_Structure_in_Time","221078613_Strong_Systematicity_in_Sentence_Processing_by_an_Echo_State_Network","23769937_Connectionist_semantic_systematicity","20704850_Connectionism_and_Cognitive_Architecture_A_Critical_Analysis","13395667_Rethinking_Eliminative_Connectionism","2775093_A_Learning_Algorithm_for_Continually_Running_Fully_Recurrent_Neural_Networks","2379165_Symbolically_Speaking_A_Connectionist_Model_of_Sentence_Production"]}