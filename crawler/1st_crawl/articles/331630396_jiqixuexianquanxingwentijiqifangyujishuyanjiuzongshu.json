{"id":"331630396_jiqixuexianquanxingwentijiqifangyujishuyanjiuzongshu","abstract":"Machine learning has already become one of the most widely used techniques in the field of computer science, and it has been widely applied in image processing, natural language processing, network security and other fields. However, there has been many security threats that need to be overcome on current machine learning algorithms and training data set, which will affect the security of several practical applications, such as facial detection, malware detection and automatic driving, etc. According to the known security threats, which aim to a variety of machine learning algorithms, such as the support vector machine (SVM) classifier, clustering and deep neural networks, this paper introduces the issues that happen in the training, testing/inference phase of machine learning, which include privacy leaking and attacks of poisoning, evasion, impersonate and inversion based on the adversarial samples. Then, this paper sums up the machine learning adversary model as well as its safety assessment mechanism and concludes a certain number of countermeasures and privacy protection techniques on training and testing processes. Finally, this paper looks forward some correlative problems worthy of further discussion","authors":["Pan Li","Wentao Zhao","Qiang Liu","Jianjing Cui"],"meta":["January 2018","DOI:10.3778/j.issn.1673-9418.1708038"],"references":["309797568_Delving_into_Transferable_Adversarial_Examples_and_Black-box_Attacks","309448167_Accessorize_to_a_Crime_Real_and_Stealthy_Attacks_on_State-of-the-Art_Face_Recognition","308027534_Stealing_Machine_Learning_Models_via_Prediction_APIs","307560165_Deep_neural_networks_are_easily_fooled_High_confidence_predictions_for_unrecognizable_images","306474561_Towards_Bayesian_Deep_Learning_A_Framework_and_Some_Existing_Methods","306304648_Distillation_as_a_Defense_to_Adversarial_Perturbations_Against_Deep_Neural_Networks","319770131_DeepFool_a_simple_and_accurate_method_to_fool_deep_neural_networks","314259698_Generative_Poisoning_Attack_Method_Against_Neural_Networks","313857643_Generating_Adversarial_Malware_Examples_for_Black-Box_Attacks_Based_on_GAN","305886295_Randomized_Prediction_Games_for_Adversarial_Machine_Learning","305186613_Adversarial_examples_in_the_physical_world","305388839_Defensive_Distillation_is_Not_Robust_to_Adversarial_Examples","303032467_The_Limitations_of_Deep_Learning_in_Adversarial_Settings","301534715_Privacy_in_Pharmacogenetics_An_End-to-End_Case_Study_of_Personalized_Warfarin_Dosing","301419711_Model_Inversion_Attacks_that_Exploit_Confidence_Information_and_Basic_Countermeasures"]}