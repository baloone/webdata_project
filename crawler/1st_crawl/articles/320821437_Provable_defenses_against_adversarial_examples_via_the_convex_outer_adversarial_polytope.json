{"id":"320821437_Provable_defenses_against_adversarial_examples_via_the_convex_outer_adversarial_polytope","abstract":"We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations (on the training data; for previously unseen examples, the approach will be guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well). The basic idea of the approach is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a toy 2D robust classification task, and on a simple convolutional architecture applied to MNIST, where we produce a classifier that provably has less than 8.4% test error for any adversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$. This represents the largest verified network that we are aware of, and we discuss future challenges in scaling the approach to much larger domains.","authors":["J. Zico Kolter","Eric Wong"],"meta":["November 2017"],"references":["320163363_Ground-Truth_Adversarial_Examples","318392300_NO_Need_to_Worry_about_Adversarial_Examples_in_Object_Detection_in_Autonomous_Vehicles","318370372_Safety_Verification_of_Deep_Neural_Networks","313394663_Reluplex_An_Efficient_SMT_Solver_for_Verifying_Deep_Neural_Networks","309448167_Accessorize_to_a_Crime_Real_and_Stealthy_Attacks_on_State-of-the-Art_Face_Recognition","306304648_Distillation_as_a_Defense_to_Adversarial_Perturbations_Against_Deep_Neural_Networks","320037488_Formal_Verification_of_Piece-Wise_Linear_Feed-Forward_Neural_Networks","319770378_Explaining_and_harnessing_adversarial_examples","318671233_Synthesizing_Robust_Adversarial_Examples","317919653_Towards_Evaluating_the_Robustness_of_Neural_Networks","317820650_An_approach_to_reachability_analysis_for_feed-forward_ReLU_neural_networks","317673614_Towards_Deep_Learning_Models_Resistant_to_Adversarial_Attacks","220685215_Near-Optimal_Signal_Recovery_From_Random_Projections_Universal_Encoding_Strategies","1741729_Robustness_and_Regularization_of_Support_Vector_Machines"]}