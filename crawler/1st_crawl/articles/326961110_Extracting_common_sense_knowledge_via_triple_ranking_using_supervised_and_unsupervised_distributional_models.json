{"id":"326961110_Extracting_common_sense_knowledge_via_triple_ranking_using_supervised_and_unsupervised_distributional_models","abstract":"In this paper we are concerned with developing information extraction models that support the extraction of common sense knowledge from a combination of unstructured and semi-structured datasets. Our motivation is to extract manipulation-relevant knowledge that can support robots' action planning. We frame the task as a relation extraction task and, as proof-of-concept, validate our method on the task of extracting two types of relations: locative and instrumental relations. The locative relation relates objects to the prototypical places where the given object is found or stored. The second instrumental relation relates objects to their prototypical purpose of use. While we extract these relations from text, our goal is not to extract specific textual mentions, but rather, given an object as input, extract a ranked list of locations and uses ranked by 'prototypicality'. We use distributional methods in embedding space, relying on the well-known skip-gram model to embed words into a low-dimensional distributional space, using cosine similarity to rank the various candidates. In addition, we also present experiments that rely on the vector space model NASARI, which compute embeddings for disambiguated concepts and are thus semantically aware. While this distributional approach has been published before, we extend our framework by additional methods relying on neural networks that learn a score to judge whether a given candidate pair actually expresses a desired relation. The network thus learns a scoring function using a supervised approach. While we use a ranking-based evaluation, the supervised model is trained using a binary classification task. The resulting score from the neural network and the cosine similarity in the case of the distributional approach are both used to compute a ranking. We compare the different approaches and parameterizations thereof on the task of extracting the above mentioned relations. We show that the distributional similarity approach performs very well on the task. The best performing parameterization achieves an NDCG of 0.913, a [email protected] of 0.400 and a [email protected] of 0.423. The performance of the supervised learning approach, in spite of having being trained on positive and negative examples of the relation in question, is not as good as expected and achieves an NCDG of 0.908, a [email protected] of 0.454 and a [email protected] of 0.387, respectively.","authors":["Soufian Jebbara","Valerio Basile","Elena Cabrio","Philipp Cimiano"],"meta":["August 2018Semantic Web 10(1521):1-20","DOI:10.3233/SW-180302","Project: Autonomous Learning of the Meaning of Objects"],"references":["324764911_Never-ending_learning","319207032_Knowledge_Graph_Embedding_by_Translating_on_Hyperplanes","317488035_Learning_Entity_and_Relation_Embeddings_for_Knowledge_Resolution","313312767_Categorical_perception_effects_in_connectionist_models","319770439_Efficient_Estimation_of_Word_Representations_in_Vector_Space","319770263_Reasoning_With_Neural_Tensor_Networks_for_Knowledge_Base_Completion","313750295_Subsequence_kernels_for_relation_extraction","313348019_To_Cognize_is_to_Categorize_Cognition_is_Categorization","313346285_Categorical_Perception","309694004_Populating_a_Knowledge_Base_with_Object-Location_Relations_Using_Distributional_Semantics"]}