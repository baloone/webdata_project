{"id":"333469047_The_Frontier_of_SGD_and_Its_Variants_in_Machine_Learning","abstract":"A Numerical optimization is a classical field in operation research and computer science, which has been widely used in areas such as physics and economics. Although optimization algorithms have achieved great success for plenty of applications, handling the big data in the best fashion possible is a very inspiring and demanding challenge in the artificial intelligence era. Stochastic gradient descent (SGD) is pretty simple but surprisingly, highly effective in machine learning models, such as support vector machine (SVM) and deep neural network (DNN). Theoretically, the performance of SGD for convex optimization is well understood. But, for the non-convex setting, which is very common for the machine learning problems, to obtain the theoretical guarantee for SGD and its variants is still a standing problem. In the paper, we do a survey about the SGD and its variants such as Momentum, ADAM and SVRG, differentiate their algorithms and applications and present some recent breakthrough and open problems.","authors":["Juan Du"],"meta":["May 2019Journal of Physics Conference Series 1229(1):012046","DOI:10.1088/1742-6596/1229/1/012046"],"references":["301878704_Stochastic_Variance_Reduction_for_Nonconvex_Optimization","301839639_A_Variational_Perspective_on_Accelerated_Methods_in_Optimization","265218016_Incremental_Gradient_Subgradient_and_Proximal_Methods_for_Convex_Optimization_A_Survey","263582586_SAGA_A_Fast_Incremental_Gradient_Method_With_Support_for_Non-Strongly_Convex_Composite_Objectives","326848519_Katyusha_The_first_direct_acceleration_of_stochastic_gradient_methods","319770184_Speech_Recognition_With_Deep_Recurrent_Neural_Networks","317633546_Finding_approximate_local_minima_faster_than_gradient_descent","313857927_A_Hitting_Time_Analysis_of_Stochastic_Gradient_Langevin_Dynamics","313501588_Accelerating_stochastic_gradient_descent_using_predictive_variance_reduction","269935079_Adam_A_Method_for_Stochastic_Optimization"]}