{"id":"273787871_Optimizing_Neural_Networks_with_Kronecker-factored_Approximate_Curvature","abstract":"We propose an efficient method for approximating natural gradient descent in\nneural networks which we call Kronecker-factored Approximate Curvature (K-FAC).\nK-FAC is based on an efficiently invertible approximation of a neural network's\nFisher information matrix which is neither diagonal nor low-rank, and in some\ncases is completely non-sparse. It is derived by approximating various large\nblocks of the Fisher (corresponding to entire layers) as factoring as Kronecker\nproducts between two much smaller matrices. While only several times more\nexpensive to compute than the plain stochastic gradient, the updates produced\nby K-FAC make much more progress optimizing the objective, which results in an\nalgorithm that can be much faster than stochastic gradient descent with\nmomentum in practice. And unlike some previously proposed approximate\nnatural-gradient/Newton methods such as Hessian-free methods, K-FAC works very\nwell in highly stochastic optimization regimes.","authors":["James Martens","Roger Grosse"],"meta":["March 2015","SourcearXiv"],"references":["243648538_Some_methods_of_speeding_up_the_convergence_of_iteration_methods","236735729_No_More_Pesky_Learning_Rates","234824887_Solution_of_the_Sylvester_matrix_equation_AXB_CXD_E","228102719_Improving_neural_networks_by_preventing_co-adaptation_of_feature_detectors","216792889_Improving_the_Convergence_of_Back-Propagation_Learning_with_Second-Order_Methods","24294555_An_Improved_Newton_Iteration_for_the_Generalized_Inverse_of_a_Matrix_with_Applications","11294744_Fast_Curvature_Matrix-Vector_Products_for_Second-Order_Gradient_Descent","2820996_On_Natural_Learning_and_Pruning_in_Multilayered_Perceptrons","319770628_Methods_of_Information_Geometry_Translations_of_Mathematical_Monographs_Tanslations_of_Mathematical_Monographs","319770539_Information-Geometric_Optimization_Algorithms_A_Unifying_Picture_via_Invariance_Principles","306221946_Centering_neural_network_gradient_factors","305890965_Computational_Methods_for_Linear_Matrix_Equations","264957612_Matrix_equation_XABXC","242122613_Sharpness_in_Rates_of_Convergence_For_CG_and_Symmetric_Lanczos_Methods1","239028212_Experiments_on_Learning_by_Back_Propagation","224133501_A_tutorial_on_stochastic_approximation_algorithms_for_training_Restricted_Boltzmann_Machines_and_Deep_Belief_Nets","221662616_Covariance_Estimation_The_GLM_and_Regularization_Perspectives","221619092_Topmoumoute_Online_Natural_Gradient_Algorithm","221345102_Deep_learning_via_Hessian-free_optimization","221344668_Learning_Recurrent_Neural_Networks_with_Hessian-Free_Optimization","30967986_Joint_mean-covariance_models_with_applications_to_longitudinal_data_Unconstrained_parameterisation","12177687_Adaptive_natural_gradient_learning_algorithms_for_various_stochastic_models","6912170_Reducing_the_Dimensionality_of_Data_with_Neural_Networks","2433873_Natural_Gradient_Works_Efficiently_in_Learning"]}