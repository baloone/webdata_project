{"id":"334205052_Enhancing_clinical_concept_extraction_with_contextual_embeddings","abstract":"Objective: \nNeural network-based representations (\"embeddings\") have dramatically advanced natural language processing (NLP) tasks, including clinical NLP tasks such as concept extraction. Recently, however, more advanced embedding methods and representations (eg, ELMo, BERT) have further pushed the state of the art in NLP, yet there are no common best practices for how to integrate these representations into clinical tasks. The purpose of this study, then, is to explore the space of possible options in utilizing these new models for clinical concept extraction, including comparing these to traditional word embedding methods (word2vec, GloVe, fastText).\n\nMaterials and methods: \nBoth off-the-shelf, open-domain embeddings and pretrained clinical embeddings from MIMIC-III (Medical Information Mart for Intensive Care III) are evaluated. We explore a battery of embedding methods consisting of traditional word embeddings and contextual embeddings and compare these on 4 concept extraction corpora: i2b2 2010, i2b2 2012, SemEval 2014, and SemEval 2015. We also analyze the impact of the pretraining time of a large language model like ELMo or BERT on the extraction performance. Last, we present an intuitive way to understand the semantic information encoded by contextual embeddings.\n\nResults: \nContextual embeddings pretrained on a large clinical corpus achieves new state-of-the-art performances across all concept extraction tasks. The best-performing model outperforms all state-of-the-art methods with respective F1-measures of 90.25, 93.18 (partial), 80.74, and 81.65.\n\nConclusions: \nWe demonstrate the potential of contextual embeddings through the state-of-the-art performance these methods achieve on clinical concept extraction. Additionally, we demonstrate that contextual embeddings encode valuable semantic information not accounted for in traditional word representations.","authors":["Yuqi Si","Jingqi Wang","Hua Xu","Kirk Roberts"],"meta":["July 2019Journal of the American Medical Informatics Association 26(11)","DOI:10.1093/jamia/ocz096"],"references":["335750430_BioBERT_a_pre-trained_biomedical_language_representation_model_for_biomedical_text_mining","331439649_A_Frame-Based_NLP_System_for_Cancer-Related_Information_Extraction","328505204_Using_Clinical_Natural_Language_Processing_for_Health_Outcomes_Research_Overview_and_Actionable_Suggestions_for_Future_Advances","326446325_Named_Entity_Recognition_using_Neural_Networks_for_Clinical_Notes","323664116_Deep_learning_with_word_embeddings_improves_biomedical_named_entity_recognition","322886200_A_Comparison_of_Word_Embeddings_for_the_Biomedical_Natural_Language_Processing","321198377_Clinical_Information_Extraction_Applications_A_Literature_Review","318200350_Entity_recognition_from_clinical_texts_via_recurrent_neural_network","334601558_Publicly_Available_Clinical","325445489_Deep_Contextualized_Word_Representations","317088438_A_Study_of_Neural_Word_Embeddings_for_Named_Entity_Recognition_in_Clinical_Text","317088189_Recognizing_Disjoint_Clinical_Concepts_in_Clinical_Text_Using_Machine_Learning-based_Methods","318029292_Recurrent_neural_networks_with_specialized_word_embeddings_for_health-domain_named-entity_recognition","306247934_Corpus_Domain_Effects_on_Distributional_Semantic_Modeling_of_Medical_Terms","305388986_Enriching_Word_Vectors_with_Subword_Information"]}