{"id":"320626962_Rethinking_generalization_requires_revisiting_old_ideas_statistical_mechanics_approaches_and_complex_learning_behavior","abstract":"We describe an approach to understand the peculiar and counterintuitive generalization properties of deep neural networks. The approach involves going beyond worst-case theoretical capacity control frameworks that have been popular in machine learning in recent years to revisit old ideas in the statistical mechanics of neural networks. Within this approach, we present a prototypical Very Simple Deep Learning (VSDL) model, whose behavior is controlled by two control parameters, one describing an effective amount of data, or load, on the network (that decreases when noise is added to the input), and one with an effective temperature interpretation (that increases when algorithms are early stopped). Using this model, we describe how a very simple application of ideas from the statistical mechanics theory of generalization provides a strong qualitative description of recently-observed empirical results regarding the inability of deep neural networks not to overfit training data, discontinuous learning and sharp transitions in the generalization properties of learning algorithms, etc.","authors":["Charles H. Martin","Michael W. Mahoney"],"meta":["October 2017"],"references":["319210145_A_Capacity_Scaling_Law_for_Artificial_Neural_Networks","317241165_Deep_Learning_is_Robust_to_Massive_Label_Noise","314182281_Opening_the_Black_Box_of_Deep_Neural_Networks_via_Information","309460742_Universal_adversarial_perturbations","308264858_On_Large-Batch_Training_for_Deep_Learning_Generalization_Gap_and_Sharp_Minima","320441847_Generalization_in_Deep_Learning","319770239_Identifying_and_attacking_the_saddle_point_problem_in_high-dimensional_non-convex_optimization","317932215_Spectrally-normalized_margin_bounds_for_neural_networks","315096447_Sharp_Minima_Can_Generalize_For_Deep_Nets","310122390_Understanding_deep_learning_requires_rethinking_generalization"]}