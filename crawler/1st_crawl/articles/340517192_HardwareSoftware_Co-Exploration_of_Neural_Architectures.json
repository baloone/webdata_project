{"id":"340517192_HardwareSoftware_Co-Exploration_of_Neural_Architectures","abstract":"We propose a novel hardware and software co-exploration framework for efficient neural architecture search (NAS). Different from existing hardware-aware NAS which assumes a fixed hardware design and explores the neural architecture search space only, our framework simultaneously explores both the architecture search space and the hardware design space to identify the best neural architecture and hardware pairs that maximize both test accuracy and hardware efficiency. Such a practice greatly opens up the design freedom and pushes forward the Pareto frontier between hardware efficiency and test accuracy for better design tradeoffs. The framework iteratively performs a two-level (fast and slow) exploration. Without lengthy training, the fast exploration can effectively fine-tune hyperparameters and prune inferior architectures in terms of hardware specifications, which significantly accelerates the NAS process. Then, the slow exploration trains candidates on a validation set and updates a controller using the reinforcement learning to maximize the expected accuracy together with the hardware efficiency. In this paper, we demonstrate that the co-exploration framework can effectively expand the search space to incorporate models with high accuracy, and we theoretically show that the proposed two-level optimization can efficiently prune inferior solutions to better explore the search space. Experimental results on ImageNet show that the co-exploration NAS can find solutions with the same accuracy, 35.24% higher throughput, 54.05% higher energy efficiency, compared with the hardware-aware NAS.","authors":["Weiwen Jiang","Lei Yang","Edwin Hsing-Mean Sha","Qingfeng Zhuge"],"meta":["April 2020IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems PP(99):1-1","DOI:10.1109/TCAD.2020.2986127"],"references":["341402156_Architecture_Search_of_Dynamic_Cells_for_Semantic_Video_Segmentation","333333819_FPGADNN_Co-Design_An_Efficient_Design_Methodology_for_IoT_Intelligence_on_the_Edge","333154942_An_Efficient_Mapping_Approach_to_Large-Scale_DNNs_on_Multi-FPGA_Architectures","328784456_DNNBuilder_an_Automated_Tool_for_Building_High-Performance_DNN_Hardware_Accelerators_for_FPGAs","328778467_TGPA_tile-grained_pipeline_architecture_for_low_latency_CNN_inference","325673550_Efficient_Architecture_Search_by_Network_Transformation","324658095_Serving_DNNs_in_Real_Time_at_Datacenter_Scale_with_Project_Brainwave","321902062_Automated_flow_for_compressing_convolution_neural_networks_for_efficient_edge-computation_with_FPGA","309738510_Designing_Neural_Network_Architectures_using_Reinforcement_Learning","305727136_Energy-Efficient_CNN_Implementation_on_a_Deeply_Pipelined_FPGA_Cluster","338510249_MnasNet_Platform-Aware_Neural_Architecture_Search_for_Mobile","338509389_FBNet_Hardware-Aware_Efficient_ConvNet_Design_via_Differentiable_Neural_Architecture_Search","338503878_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision","336769519_Accuracy_vs_Efficiency_Achieving_Both_through_FPGA-Implementation_Aware_Neural_Architecture_Search","330893774_Performance_Modeling_for_CNN_Inference_Accelerators_on_FPGA","329745708_Learning_Transferable_Architectures_for_Scalable_Image_Recognition","329480997_Design_Flow_of_Accelerating_Hybrid_Extremely_Low_Bit-Width_Neural_Network_in_Embedded_FPGA","329478022_A_Framework_for_Acceleration_of_CNN_Training_on_Deeply-Pipelined_FPGA_Clusters_with_Work_and_Weight_Load_Balancing","327806515_FPDeep_Acceleration_and_Load_Balancing_of_CNN_Training_on_FPGA_Clusters","326565045_A_Configurable_Cloud-Scale_DNN_Processor_for_Real-Time_AI","326487709_Heterogeneous_FPGA-based_Cost-Optimal_Design_for_Timing-Constrained_CNNs","323991839_DNN_ENGINE_A_16nm_Sub-uJ_Deep_Neural_Network_Inference_Accelerator_for_the_Embedded_Masses","323822730_A_65nm_1Mb_nonvolatile_computing-in-memory_ReRAM_macro_with_sub-16ns_multiply-and-accumulate_for_binary_DNN_AI_edge_processors","323257005_Horovod_fast_and_easy_distributed_deep_learning_in_TensorFlow","322201973_On_the_Design_of_Minimal-Cost_Pipeline_Systems_Satisfying_HardSoft_Real-Time_Constraints","320798539_Hierarchical_Representations_for_Efficient_Architecture_Search","319858919_ScaleDeep_A_Scalable_Compute_Architecture_for_Learning_and_Evaluating_Deep_Networks","309738632_Neural_Architecture_Search_with_Reinforcement_Learning","304758513_Maximizing_CNN_Accelerator_Efficiency_Through_Resource_Partitioning","301367952_Optimizing_FPGA-based_Accelerator_Design_for_Deep_Convolutional_Neural_Networks","279964320_Simple_statistical_gradient-following_algorithms_for_connectionist_reinforcement_learning","272194743_Batch_Normalization_Accelerating_Deep_Network_Training_by_Reducing_Internal_Covariate_Shift","269935079_Adam_A_Method_for_Stochastic_Optimization","221345737_Rectified_Linear_Units_Improve_Restricted_Boltzmann_Machines_Vinod_Nair"]}