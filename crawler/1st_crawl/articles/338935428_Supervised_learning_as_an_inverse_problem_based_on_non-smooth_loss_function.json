{"id":"338935428_Supervised_learning_as_an_inverse_problem_based_on_non-smooth_loss_function","abstract":"This paper is concerned by solving supervised machine learning problem as an inverse problem. Recently, many works have focused on defining a relationship between supervised learning and the well-known inverse problems. However, this connection between the learning problem and the inverse one has been done in the particular case where the inverse problem is reformulated as a minimization problem with a quadratic cost functional (\\(L^2\\) cost functional). Although, it is well known that the cost functional can be \\(L^1\\), \\(L^2\\) or any positive function that measures the gap between the predicted data and the observed one. Indeed, the use of \\(L^1\\) loss function for supervised learning problem gives more consistent results (see Rosasco et al. in Neural Comput 16:1063–1076, 2004). This strengthens the idea of reformulating the inverse problem, associated to machine learning problem, into a minimization problem using \\( L^{1}\\) functional. However, the \\(L^{1}\\) loss function is non-differentiable, which precludes the use of standard optimization tools. To overcome this difficulty, we propose in this paper a new technique of approximation based on the reformulation of the associated inverse problem into a minimizing one of a slanting cost functional Chen et al. (MIS Q Manag Inf Syst 36:1165–1188, 2012), which is solved using Tikhonov regularization and Newton’s method. This approach leads to an efficient numerical algorithm allowing us to solve supervised learning problem in the most general framework. To confirm this, we present some numerical results showing the efficiency of the proposed approach. Furthermore, the numerical experiment validation is made through academic and real-life data. Thus, the comparison with existing methods and numerical stability of the algorithm is presented in order to show that our approach is better in terms of convergence speed and quality of predicted models.","authors":["Soufiane Lyaqini","Mohamed Quafafou","Mourad Nachaoui","Abdelkrim Chakib"],"meta":["August 2020Knowledge and Information Systems 62(10)","DOI:10.1007/s10115-020-01439-2"],"references":["320726784_SGDLibrary_A_MATLAB_library_for_stochastic_optimization_algorithms","313247181_IQN_An_Incremental_Quasi-Newton_Method_with_Local_Superlinear_Convergence_Rate","299377679_Big_Data_Optimization_Recent_Developments_and_Challenges","346677891_An_Introduction_to_the_Mathematical_Theory_of_Inverse_Problems","327538858_Hierarchical_extreme_learning_machine_based_image_denoising_network_for_visual_Internet_of_Things","318566518_An_Oblique_Elliptical_Basis_Function_Network_Approach_for_Supervised_Learning_Applications","313501588_Accelerating_stochastic_gradient_descent_using_predictive_variance_reduction","284679162_Business_Intelligence_and_Analytics_From_Big_Data_to_Big_Impact","283408378_Lectures_on_Cauchy's_problem_in_linear_partial_differential_equations","283098736_Continuous_Reinforcement_Learning_to_Robust_Fault_Tolerant_Control_for_a_Class_of_Unknown_Nonlinear_Systems","273490686_Off-the-person_electrocardiography_performance_assessment_and_clinical_correlation","268426733_Supervised_learning_as_an_inverse_problem","278660085_Stochastic_Gradient_Descent_Tricks","278651717_The_Nature_of_Statistical_Learning_Theory","268647176_Solutions_of_ill-posed_problems_Translation_editor_Frity_John"]}