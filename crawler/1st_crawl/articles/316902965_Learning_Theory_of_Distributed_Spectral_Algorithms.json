{"id":"316902965_Learning_Theory_of_Distributed_Spectral_Algorithms","abstract":"Spectral algorithms have been widely used and studied in learning theory and inverse problems. This paper is concerned with distributed spectral algorithms, for handling big data, based on a divide-and-conquer approach. We present a learning theory for these distributed kernel-based learning algorithms in a regression framework including nice error bounds and optimal minimax learning rates achieved by means of a novel integral operator approach and a second order decomposition of inverse operators. Our quantitative estimates are given in terms of regularity of the regression function, effective dimension of the reproducing kernel Hilbert space, and qualification of the filter function of the spectral algorithm. They do not need any eigenfunction or noise conditions and are better than the existing results even for the classical family of spectral algorithms.","authors":["Zheng-Chu Guo","Shaobo Lin","Ding-Xuan Zhou"],"meta":["May 2017Inverse Problems 33(7)","DOI:10.1088/1361-6420/aa72b2"],"references":["306186572_Distributed_Learning_with_Regularized_Least_Squares","275478292_Regularization_schemes_for_minimum_error_entropy_principle","267557636_Cross-validation_based_Adaptation_for_Regularization_Operators_in_Learning","259693635_Spectral_Algorithms_for_Supervised_Learning","237324432_Shannon_sampling_and_function_reconstruction_from_point_values","311673035_Regularization_of_inverse_problems","289861661_Random_design_analysis_of_ridge_regression","283230557_Distributed_stochastic_optimization_and_learning","267129785_Learning_Theory_An_Approximation_Theory_Viewpoint","265103071_Random_Design_Analysis_of_Ridge_Regression"]}