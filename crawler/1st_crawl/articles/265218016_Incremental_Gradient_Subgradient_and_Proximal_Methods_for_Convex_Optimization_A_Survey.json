{"id":"265218016_Incremental_Gradient_Subgradient_and_Proximal_Methods_for_Convex_Optimization_A_Survey","abstract":"We survey incremental methods for minimizing a sum m i=1 f i (x) consisting of a large number of convex component functions f i . Our methods consist of iterations applied to single components, and have proved very effective in practice. We introduce a unified algorithmic framework for a variety of such methods, some involving gradient and subgradient iterations, which are known, and some involving combinations of subgradient and proximal methods, which are new and offer greater flexibility in exploiting the special structure of f i . We provide an analysis of the convergence and rate of convergence properties of these methods, including the advantages offered by randomization in the selection of components. We also survey applications in inference/machine learning, signal processing, and large-scale and distributed optimization.","authors":["Dimitri P. Bertsekas"],"meta":["July 2015Optimization 2010"],"references":["313725747_Pseudogradient_adaptation_and_training_algorithms","268248877_Introduction_to_optimization_Vvedenie_v_optimizatsiyu","267192634_Geometric_methods_and_optimization_problems_Kluwer_Dordrecht_1999_429_pp","285677677_Augmented_Lagrangian_Methods_Applications_to_the_Numerical_Solution_of_Boundary-Value_Problems","275624749_Discrete-Parameter_Martingales","268545710_An_iteration_method_in_the_problem_of_approximating_functions_from_a_finite_number_of_observations","267430930_Convex_optimization_theory","267078790_Stochastic_approximation_A_dynamical_systems_viewpoint","260365721_On_accelerated_proximal_gradient_methods_for_convex-concave_optimization","260365606_A_method_for_unconstrained_convex_minimization_problem_with_the_rate_of_convergence"]}