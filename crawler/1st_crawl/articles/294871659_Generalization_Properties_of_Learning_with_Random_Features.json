{"id":"294871659_Generalization_Properties_of_Learning_with_Random_Features","abstract":"We study the generalization properties of regularized learning with random features in the statistical learning theory framework. We show that optimal learning errors can be achieved with a number of features smaller than the number of examples. As a byproduct, we also show that learning with random features can be seen as a form of regularization, rather than only a way to speed up computations.","authors":["Alessandro Rudi","Lorenzo Rosasco"],"meta":["December 2017","Conference: NIPS"],"references":["329652278_Learning_with_Kernels_Support_Vector_Machines_Regularization_Optimization_and_Beyond","313100237_Kernel_methods_for_deep_learning","311673035_Regularization_of_inverse_problems","309532479_Random_features_for_large_scale_kernel_machines","309066273_Quasi-Monte_Carlo_feature_maps_for_shift-invariant_Kernels","307559609_Fastfood_-_Computing_hilbert_space_expansions_in_loglinear_time","287789944_Divide_and_conquer_kernel_ridge_regression","287631220_Random_Laplace_Feature_Maps_for_Semigroup_Kernels_on_Histograms","285045925_Nystrom_method_vs_random_Fourier_features_A_theoretical_and_empirical_comparison","284582018_Estimating_the_approximation_error_in_learning_theory"]}