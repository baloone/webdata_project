{"id":"344868509_Detecting_Backdoor_Attacks_via_Class_Difference_in_Deep_Neural_Networks","abstract":"A backdoor attack implies that deep neural networks misrecognize data that have a specific trigger by additionally training the malicious training data, including the specific trigger to the deep neural network model. In this method, the deep neural network correctly recognizes normal data without triggers, but the network misrecognizes data containing a specific trigger as a target class chosen by the attacker. In this paper, I propose a defense method against backdoor attacks using a detection model. This method detects the backdoor sample by comparing the output result of the target model with that of the model that trained the original secure training dataset. This is a defense method without trigger reverse or access to the entire training dataset. As an experimental environment, I used the Tensorflow machine-learning library, MNIST, and Fashion-MNIST as datasets. The results show that when the partial training data for the detection model are 200, the proposed method showed detection rates of 70.1% and 74.4% for the backdoor samples in MNIST and Fashion-MNIST, respectively.","authors":["Hyun Kwon"],"meta":["October 2020IEEE Access 8(2020):191049-191056","DOI:10.1109/ACCESS.2020.3032411"],"references":["332584393_BadNets_Evaluating_Backdooring_Attacks_on_Deep_Neural_Networks","320570614_Facial_Attributes_Accuracy_and_Adversarial_Robustness","319312259_Fashion-MNIST_a_Novel_Image_Dataset_for_Benchmarking_Machine_Learning_Algorithms","303657108_TensorFlow_A_system_for_large-scale_machine_learning","356283848_Spectral_Signatures_in_Backdoor_Attacks","335867853_Neural_Cleanse_Identifying_and_Mitigating_Backdoor_Attacks_in_Neural_Networks","332812185_Hardware_Trojan_Design_on_Neural_Networks","323249035_Trojaning_Attack_on_Neural_Networks","269935079_Adam_A_Method_for_Stochastic_Optimization","265385906_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition"]}