{"id":"319121606_AffectNet_A_Database_for_Facial_Expression_Valence_and_Arousal_Computing_in_the_Wild","abstract":"Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.","authors":["Ali Mollahosseini","Behzad Hasani","Mohammad H Mahoor"],"meta":["August 2017IEEE Transactions on Affective Computing PP(99)","DOI:10.1109/TAFFC.2017.2740923"],"references":["311491540_AVEC_2015_The_5th_International_AudioVisual_Emotion_Challenge_and_Workshop","310824054_AVEC_2016_Depression_Mood_and_Emotion_Recognition_Workshop_and_Challenge","308453418_Video-based_emotion_recognition_using_CNN-RNN_and_C3D_hybrid_networks","308304376_EmotioNet_An_Accurate_Real-Time_Algorithm_for_the_Automatic_Annotation_of_a_Million_Facial_Expressions_in_the_Wild","301896305_AVEC_2016_-_Depression_Mood_and_Emotion_Recognition_Workshop_and_Challenge","301422384_Multimodal_Affective_Dimension_Prediction_Using_Deep_Bidirectional_Long_Short-Term_Memory_Recurrent_Neural_Networks","287702683_AVEC_2014_-_3D_dimensional_affect_and_depression_recognition_challenge","271427134_Bidirectional_Warping_of_Active_Appearance_Model","264979485_Caffe_Convolutional_Architecture_for_Fast_Feature_Embedding","262313360_Emotion_Recognition_In_The_Wild_Challenge_2013","262157517_AVEC_2013_-_The_continuous_AudioVisual_Emotion_and_depression_recognition_challenge","261468526_Affectiva-MIT_Facial_Expression_Dataset_AM-FED_Naturalistic_and_Spontaneous_Facial_Expressions_Collected_In-the-Wild","261121552_Introducing_the_RECOLA_multimodal_corpus_of_remote_collaborative_and_affective_interactions","259972003_Facing_Imbalanced_Data_-_Recommendations_for_the_Use_of_Performance_Metrics","248703363_DISFA_A_spontaneous_facial_action_intensity_database","248036385_The_timing_of_facial_motion_in_posed_and_spontaneous_smiles","243964130_Challenges_in_Representation_Learning_A_Report_on_Three_Machine_Learning_Contests","235361517_A_Circumplex_Model_of_Affect","232651717_DEAP_A_Database_for_Emotion_Analysis_Using_Physiological_Signals","225215404_Beyond_Accuracy_F-Score_and_ROC_A_Family_of_Discriminant_Measures_for_Performance_Evaluation","224251574_The_Belfast_Induced_Natural_Emotion_Database","224248863_The_SEMAINE_Database_Annotated_Multimodal_Records_of_Emotionally_Colored_Conversations_between_a_Person_and_a_Limited_Agent","224226954_Continuous_Prediction_of_Spontaneous_Affect_from_Multiple_Cues_and_Modalities_in_Valence-Arousal_Space","224165246_The_Extended_Cohn-Kanade_Dataset_CK_A_complete_dataset_for_action_unit_and_emotion-specified_expression","223609202_User_and_context_adaptive_neural_networks_for_emotion_recognition","222574859_Facial_expression_recognition_based_on_Local_Binary_Patterns_A_comprehensive_study","221622197_AVEC_2011-The_First_International_AudioVisual_Emotion_Challenge","221429947_Static_facial_expression_analysis_in_tough_conditions_Data_evaluation_protocol_and_benchmark","319770820_Histograms_of_Oriented_Gradients_for_Human_Detection","319770183_Imagenet_classification_with_deep_convolutional_neural_networks","316440968_Facial_Expression_Recognition_from_World_Wild_Web","312538118_Support-vector_networks","312288334_Facial_Affect_In-the-Wild_A_Survey_and_a_New_Database","311450635_Recognition_Action_Units_for_Facial_Expression_Analysis","303563902_Going_deeper_in_facial_expression_recognition_using_deep_neural_networks","302975280_Support_vector_regression_machines","291950088_300_Faces_In-The-Wild_Challenge_database_and_results","281327886_Histograms_of_Oriented_Gradients_for_Human_Detection","276113061_Facial_expression_recognition_using_lp-norm_MKL_multiclass-SVM","263564119_DeepFace_Closing_the_Gap_to_Human-Level_Performance_in_Face_Verification","261564629_PCA-based_dictionary_building_for_accurate_facial_expression_recognition_via_sparse_representation","261409794_Compound_facial_expressions_of_emotion","260658392_Seeing_Stars_of_Valence_and_Arousal_in_Blog_Posts","259335300_DeepPose_Human_Pose_Estimation_via_Deep_Neural_Networks","247725173_Estimating_the_Reliability_Systematic_Error_and_Random_Error_of_Interval_Data","243779859_EMFACS-7_Emotional_Facial_Action_Coding_System","239537771_Facial_action_coding_system_A_technique_for_the_measurement_of_facial_movement","237000269_Deep_Learning_using_Linear_Support_Vector_Machines","224541268_Learning_from_Imbalanced_Data","223913638_Oriented_principal_component_analysis_for_large_margin_classifiers","221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database","221052154_Spontaneous_vs_posed_facial_behavior_Automatic_analysis_of_brow_actions","220928050_Audio-Visual_Classification_and_Fusion_of_Spontaneous_Affective_Data_in_Likelihood_Space","220660094_Robust_Real-Time_Face_Detection","220343922_Support_Vector_Network"]}