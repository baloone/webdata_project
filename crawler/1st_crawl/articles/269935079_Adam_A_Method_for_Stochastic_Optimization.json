{"id":"269935079_Adam_A_Method_for_Stochastic_Optimization","abstract":"We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions. The method is straightforward to implement\nand is based an adaptive estimates of lower-order moments of the gradients. The\nmethod is computationally efficient, has little memory requirements and is well\nsuited for problems that are large in terms of data and/or parameters. The\nmethod is also ap- propriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The method exhibits invariance to diagonal\nrescaling of the gradients by adapting to the geometry of the objective\nfunction. The hyper-parameters have intuitive interpretations and typically\nrequire little tuning. Some connections to related algorithms, on which Adam\nwas inspired, are discussed. We also analyze the theoretical convergence\nproperties of the algorithm and provide a regret bound on the convergence rate\nthat is comparable to the best known results under the online convex\noptimization framework. We demonstrate that Adam works well in practice when\nexperimentally compared to other stochastic optimization methods.","authors":["Diederik Kingma","Jimmy Ba"],"meta":["December 2014","SourcearXiv"],"references":["258816388_Revisiting_Natural_Gradient_for_Deep_Networks","236735729_No_More_Pesky_Learning_Rates","228102719_Improving_neural_networks_by_preventing_co-adaptation_of_feature_detectors","220873867_Learning_Word_Vectors_for_Sentiment_Analysis","220320677_Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization","319770229_Auto-Encoding_Variational_Bayes","319770184_Speech_Recognition_With_Deep_Recurrent_Neural_Networks","267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks","260637318_Deep_Neural_Networks_for_Acoustic_Modeling_in_Speech_Recognition_The_Shared_Views_of_Four_Research_Groups","255173850_Generating_Sequences_With_Recurrent_Neural_Networks","233981807_ADADELTA_An_adaptive_learning_rate_method","6912170_Reducing_the_Dimensionality_of_Data_with_Neural_Networks","2885544_Online_Convex_Programming_and_Generalized_Infinitesimal_Gradient_Ascent","2765136_RPROP_-_A_Fast_Adaptive_Learning_Algorithm","2433873_Natural_Gradient_Works_Efficiently_in_Learning"]}