{"id":"2262533_VC_Dimension_of_Neural_Networks","abstract":". This paper presents a brief introduction to Vapnik-Chervonenkis (VC) dimension, a quantity which characterizes the difficulty of distribution-independent learning. The paper establishes various elementary results, and discusses how to estimate the VC dimension in several examples of interest in neural network theory. 1 Introduction In this expository paper, we present a brief introduction to the subject of computing and estimating the VC dimension of neural network architectures. We provide precise definitions and prove several basic results, discussing also how one estimates VC dimension in several examples of interest in neural network theory. We do not address the learning and estimation-theoretic applications of VC dimension. (Roughly, the VC dimension is a number which helps to quantify the difficulty when learning from examples. The sample complexity, that is, the number of \"learning instances\" that one must be exposed to, in order to be reasonably certain to derive accurate p...","authors":["Eduardo D Sontag"],"meta":["August 1998"],"references":["262293293_Shattering_All_Sets_of_'k'_Points_in_General_Position_Requires_k_-_12_Parameters","313148460_A_course_on_empirical_processes","312446515_The_uniform_convergence_of_frequencies_of_the_appearance_of_events_to_their_probabilities_in_Russian","288373415_What_size_net_gives_valid_generalization","243658817_A_Theory_of_Learning_and_Generalization","238990297_Decision_theoretic_generalization_of_the_pac_model_for_neural_net_and_other_learning_applications","236736807_Estimation_of_Dependences_Based_on_Empirical_Data","234810272_A_Theory_of_Learning_and_Generalization_With_Applications_to_Neural_Networks_and_Control_Systems","233784971_On_the_Uniform_Convergence_of_Relative_Frequencies_of_Events_to_Their_Probabilities","226821868_Bounding_the_Vapnik-Chervonenkis_dimension_of_concept_classes_parameterized_by_real_numbers"]}