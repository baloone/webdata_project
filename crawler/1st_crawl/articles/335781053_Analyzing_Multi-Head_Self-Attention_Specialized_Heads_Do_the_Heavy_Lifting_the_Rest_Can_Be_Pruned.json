{"id":"335781053_Analyzing_Multi-Head_Self-Attention_Specialized_Heads_Do_the_Heavy_Lifting_the_Rest_Can_Be_Pruned","authors":["Elena Voita","David Talbot","Fedor Moiseev","Rico Sennrich"],"meta":["August 2019","DOI:10.18653/v1/P19-1580","Conference: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"],"references":["334117983_An_Analysis_of_Attention_Mechanisms_The_Case_of_Word_Sense_Disambiguation_in_Neural_Machine_Translation","332341730_Context-Aware_Neural_Machine_Translation_Learns_Anaphora_Resolution","328733327_Why_Self-Attention_A_Targeted_Evaluation_of_Neural_Machine_Translation_Architectures","324166896_Training_Tips_for_the_Transformer_Model","311648037_How_Grammatical_is_Character-level_Neural_Machine_Translation_Assessing_MT_Quality_with_Contrastive_Translation_Pairs","277248670_On_Pixel-Wise_Explanations_for_Non-Linear_Classifier_Decisions_by_Layer-Wise_Relevance_Propagation","272091377_The_Stanford_CoreNLP_Natural_Language_Processing_Toolkit","334116565_An_Analysis_of_Encoder_Representations_in_Transformer-Based_Machine_Translation","334116154_The_Importance_of_Being_Recurrent_for_Modeling_Hierarchical_Structure","320241826_To_prune_or_not_to_prune_exploring_the_efficacy_of_pruning_for_model_compression","319770118_The_Concrete_Distribution_A_Continuous_Relaxation_of_Discrete_Random_Variables","319770117_Categorical_Reparameterization_with_Gumbel-Softmax","317558625_Attention_Is_All_You_Need","311990242_Does_String-Based_Neural_MT_Learn_Source_Syntax","309729965_Assessing_the_Ability_of_LSTMs_to_Learn_Syntax-Sensitive_Dependencies","306093632_Neural_Machine_Translation_of_Rare_Words_with_Subword_Units","262991675_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models"]}