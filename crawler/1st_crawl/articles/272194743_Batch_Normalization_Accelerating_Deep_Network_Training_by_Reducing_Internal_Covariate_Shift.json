{"id":"272194743_Batch_Normalization_Accelerating_Deep_Network_Training_by_Reducing_Internal_Covariate_Shift","abstract":"Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch}. Batch Normalization allows us to use much higher learning rates\nand be less careful about initialization. It also acts as a regularizer, in\nsome cases eliminating the need for Dropout. Applied to a state-of-the-art\nimage classification model, Batch Normalization achieves the same accuracy with\n14 times fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters.","authors":["Sergey Ioffe","Christian Szegedy"],"meta":["February 2015","SourcearXiv"],"references":["267515250_Parallel_training_of_DNNs_with_Natural_Gradient_and_Parameter_Averaging","266225209_Large_Scale_Distributed_Deep_Networks","234841757_A_Convergence_Analysis_of_Log-Linear_Training","234140324_Knowledge_Matters_Importance_of_Prior_Information_for_Optimization","233730646_On_the_difficulty_of_training_Recurrent_Neural_Networks","231556969_Deep_Learning_Made_Easier_by_Linear_Transformations_in","230710850_Improving_predictive_inference_under_covariate_shift_by_weighting_the_log-likelihood_function","221362377_Nonlinear_Image_Representation_Using_Divisive_Normalization","220320677_Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization","215616968_Understanding_the_difficulty_of_training_deep_feedforward_neural_networks","2985446_Gradient-Based_Learning_Applied_to_Document_Recognition","319770315_Deep_Image_Scaling_up_Image_Recognition","319770257_Exact_solutions_to_the_nonlinear_dynamics_of_learning_in_deep_linear_neural_networks","286794765_Dropout_A_Simple_Way_to_Prevent_Neural_Networks_from_Overfitting","286271944_On_the_importance_of_initialization_and_momentum_in_deep_learning","272027131_Delving_Deep_into_Rectifiers_Surpassing_Human-Level_Performance_on_ImageNet_Classification","266030539_Mean-normalized_stochastic_gradient_for_large-scale_deep_learning","228726409_A_literature_survey_on_domain_adaptation_of_statistical_classifiers","221345737_Rectified_Linear_Units_Improve_Restricted_Boltzmann_Machines_Vinod_Nair","220656864_Critical_Points_of_the_Singular_Value_Decomposition","12375179_Oja_E_Independent_Component_Analysis_Algorithms_and_Applications_Neural_Networks_134-5_411-430"]}