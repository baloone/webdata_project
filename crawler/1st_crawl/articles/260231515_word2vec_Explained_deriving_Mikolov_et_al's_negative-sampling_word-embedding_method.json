{"id":"260231515_word2vec_Explained_deriving_Mikolov_et_al's_negative-sampling_word-embedding_method","abstract":"The word2vec software of Tomas Mikolov and colleagues\n(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and\nprovides state-of-the-art word embeddings. The learning models behind the\nsoftware are described in two research papers. We found the description of the\nmodels in these papers to be somewhat cryptic and hard to follow. While the\nmotivations and presentation may be obvious to the neural-networks\nlanguage-modeling crowd, we had to struggle quite a bit to figure out the\nrationale behind the equations.\nThis note is an attempt to explain equation (4) (negative sampling) in\n\"Distributed Representations of Words and Phrases and their Compositionality\"\nby Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.","authors":["Yoav Goldberg","Omer Levy"],"meta":["February 2014","SourcearXiv"],"references":["257882504_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality","234131319_Efficient_Estimation_of_Word_Representations_in_Vector_Space","319770369_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality"]}