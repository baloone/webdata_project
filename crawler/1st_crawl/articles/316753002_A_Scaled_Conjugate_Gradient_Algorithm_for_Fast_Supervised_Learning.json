{"id":"316753002_A_Scaled_Conjugate_Gradient_Algorithm_for_Fast_Supervised_Learning","abstract":"p>A supervised learning algorithm (Scaled Conjugate Gradient, SCG) with superlinear convergence rate is introduced. The algorithm is based upon a class of optimization techniques well known in numerical analysis as the Conjugate Gradient Methods. SCG uses second order information from the neural network but requires only O(N) memory usage, where N is the number of weights in the network. The performance of SCG is benchmarked against the performance of the standard backpropagation algorithm (BP), the conjugate gradient backpropagation (CGB) and the one-step Broyden-Fletcher-Goldfarb-Shanno memoryless quasi-Newton algorithm (BFGS). SCG yields a speed-up of at least an order of magnitude relative to BP. The speed-up depends on the convergence criterion, i.e., the bigger demand for reduction in error the bigger the speed-up. SCG is fully automated including no user dependent parameters and avoids a time consuming line-search, which CGB and BFGS use in each iteration in order to determine an appropriate step size.\n\nIncorporating problem dependent structural information in the architecture of a neural network often lowers the overall complexity. The smaller the complexity of the neural network relative to the problem domain, the bigger the possibility that the weight space contains long ravines characterized by sharp curvature. While BP is inefficient on these ravine phenomena, it is shown that SCG handles them effectively.</p","authors":["Martin MÃ¸ller"],"meta":["November 1990DAIMI Report Series 19(339)","DOI:10.7146/dpb.v19i339.6570"],"references":["317083799_Linear_and_Nonlinear_Programming","237127809_Accelerated_backpropagation_learning_Two_optimization_methods","285034882_Optimization_methods_for_back-propagation_Automatic_parameter_tuning_and_faster_convergence","285031508_Scaling_relationships_in_back-propagation_learning_Dependence_on_training_set_size","284578281_Generalization_and_network_design_strategies","273093091_Conjugate_Direction_Methods_in_Optimization","263866420_SUPERVISED_LEARNING_ON_LARGE_REDUNDANT_TRAINING_SETS","251755630_A_learning_algorithm_for_multilayered_neural_networks_a_Newton_method_using_automatic_differentiation","243765705_Connectionist_learning_procedures_Artificial_Intelligence_40_185-234","243743707_Learning_Internal_Representations_by_Error_Propagation","242411110_Safeguarded_steplength_algorithms_for_optimization_using_descent_methods","227201278_Restart_Procedures_for_the_Conjugate_Gradient_Method"]}