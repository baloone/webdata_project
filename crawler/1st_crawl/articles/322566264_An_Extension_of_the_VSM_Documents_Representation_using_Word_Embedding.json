{"id":"322566264_An_Extension_of_the_VSM_Documents_Representation_using_Word_Embedding","abstract":"In this paper, we will present experiments that try to integrate the power of Word Embedding representation in real problems for documents classification. Word Embedding is a new tendency used in the natural language processing domain that tries to represent each word from the document in a vector format. This representation embeds the semantically context in that the word occurs more frequently. We include this new representation in a classical VSM document representation and evaluate it using a learning algorithm based on the Support Vector Machine. This new added information makes the classification to be more difficult because it increases the learning time and the memory needed. The obtained results are slightly weaker comparatively with the classical VSM document representation. By adding the WE representation to the classical VSM representation we want to improve the current educational paradigm for the computer science students which is generally limited to the VSM representation.","authors":["Daniel Ionel Morariu","Lucian Vintan","Radu George Cretulescu"],"meta":["December 2017","DOI:10.1515/cplbu-2017-0033"],"references":["257882504_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality","255820377_Software_Framework_for_Topic_Modelling_with_Large_Corpora","234131319_Efficient_Estimation_of_Word_Representations_in_Vector_Space","221618573_A_Neural_Probabilistic_Language_Model","329652278_Learning_with_Kernels_Support_Vector_Machines_Regularization_Optimization_and_Beyond","319770439_Efficient_Estimation_of_Word_Representations_in_Vector_Space","316407064_An_Extension_of_the_VSM_Documents_Representation","246666948_Learning_distributed_representations_of_concepts_from_relational_data_using_linear_relational_embedd","243767323_Learning_with_kernels_Support_Vector_Machines","221995776_An_Introduction_to_Support_Vector_Machines"]}