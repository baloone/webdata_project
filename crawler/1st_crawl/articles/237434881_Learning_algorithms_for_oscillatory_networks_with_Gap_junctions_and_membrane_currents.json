{"id":"237434881_Learning_algorithms_for_oscillatory_networks_with_Gap_junctions_and_membrane_currents","abstract":"One of the most important problems for studying neural network models is the adjustment of parametas. Here we show how to formulate the problem as the minimization of the dilferslee betwen two limit cycles. The backprop%ation method for leaming algorithms is described .ss the application of grsdient descent to an eror fundion that computes this difference. A mathematical formulation is given that is applicable to any type of network model, and is applied to several models. For example, when learning in a network in which dl cells have a common, adjustable, bias current, the value of the bias ie adjustcd at a rate proportional to the difference between the sum of the target outputs and the sum of the actual outputs. When learning in a network of n cells where a target output is given for every cell. the learning algorithm splits into R indepndent leaming algorithms, one per cell. For networks containing gap junctions, a gap junction is modelled D conductance times the potential difference between the two adjacent cells. The requirement that a conductme g munt be positive is enionzed by replacing g by a functbn pos(g*) whose value is always positive, for example cxp(O.lg*), and deriving an algorithm that adjusts the parameta g* in place of g. When target output is specified for every cell in a network with gap junctions. the learning algorithm splits into fewer independent componeds, one for each gap-mnneaed subset of the network. The lemming algorithm for II gspmnneeted set of cells cannot be paralklized further. As a find example, a leaming algwithm is derived for a mutually inhibitory twc- cell network in which each cell has a membrane current. This generalized approach to backpropagation allows one to derive a learning algorithm for alntoat any model neural network given in tem of differmtid equations. It will be an essential tool for adjusting parmetem in small but complex network models.","authors":["Peter Rowat","Allen Selverston"],"meta":["February 1991Network Computation in Neural Systems 2(1):17-41","DOI:10.1088/0954-898X/2/1/002","Project: dynamics of central pattern generators"],"references":["238833309_Learning_State_Space_Trajectories_in_Recurrent_Neural_Networks","20585326_The_Assembly_of_Ionic_Currents_in_a_Thalamic_Neuron_III_The_Seven-_Dimensional_Model","16609657_Neurons_With_Graded_Response_Have_Collective_Computational_Properties_Like_Those_of_Two-State_Neurons","243732587_Generalization_of_Back-Propagation_to_Recurrent_Neural_Networks","229091480_Learning_Representations_by_Back_Propagating_Errors","223164217_Adaptive_neural_oscillator_using_continuous-time_back-propagation_learning","222357795_Parameter_fitting_in_dynamic_models","20969919_A_logical_calculus_of_the_ideas_immanent_in_nervous_activity_1943","3474433_An_Active_Pulse_Transmission_Line_Simulating_A_Nerve_Axon"]}