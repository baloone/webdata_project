{"id":"257459311_Statistical_estimation_and_testing_via_the_ordered_L1_norm","abstract":"We introduce a novel method for sparse regression and variable selection,\nwhich is inspired by modern ideas in multiple testing. Imagine we have\nobservations from the linear model y = X beta + z, then we suggest estimating\nthe regression coefficients by means of a new estimator called the ordered\nlasso, which is the solution to minimize 0.5 ||y - Xb\\|_2^2 + lambda_1 |b|_(1)\n+ lambda_2 |b|_(2) + ... + lambda_p |b|_(p); here, lambda_1 >= \\lambda_2 >= ...\n>= \\lambda_p >= 0 and |b|_(1) >= |b|_(2) >= ... >= |b|_(p) is the order\nstatistic of the magnitudes of b. The regularizer is an ordered L1 norm which\npenalizes the regression coefficients according to their rank: the higher the\nrank, the larger the penalty. This is similar to the famous BHq procedure\n[Benjamini and Hochberg, 1995], which compares the value of a test statistic\ntaken from a family to a critical threshold that depends on its rank in the\nfamily. The ordered lasso is a convex program and we demonstrate an efficient\nalgorithm for computing the solution. We prove that for orthogonal designs with\np variables, taking lambda_i = F^{-1}(1-q_i) (F is the cdf of the errors), q_i\n= iq/(2p), controls the false discovery rate (FDR) for variable selection. When\nthe design matrix is nonorthogonal there are inherent limitations on the FDR\nlevel and the power which can be obtained with model selection methods based on\nL1-like penalties. However, whenever the columns of the design matrix are not\nstrongly correlated, we demonstrate empirically that it is possible to select\nthe parameters lambda_i as to obtain FDR control at a reasonable level as long\nas the number of nonzero coefficients is not too large. At the same time, the\nprocedure exhibits increased power over the lasso, which treats all\ncoefficients equally. The paper illustrates further estimation properties of\nthe new selection rule through comprehensive simulation studies.","authors":["Ma≈Çgorzata Bogdan","Ewout van den Berg","Weijie Su","Emmanuel Candes"],"meta":["October 2013","SourcearXiv"],"references":["260542289_Asymptotic_Analysis_of_MAP_Estimation_via_the_Replica_Method_and_Applications_to_Compressed_Sensing","312453300_Proximal_algorithms","304824205_Regression_Shrinkage_and_Selection_via_the_LASSO","271196714_New_Look_at_Statistical-Model_Identification","270051795_A_second_generation_human_haplotype_map_of_over_31_million_SNPs","269634058_Robust_Uncertainty_Principles_Exact_Signal_Frequency_Information","257291946_A_Wavelet_Tour_of_Signal_Processing_The_Sparse_Way","254439642_False_discovery_rate_for_scanning_statistics","243787569_Introductory_Lectures_on_Convex_Optimization_A_Basic_Course","239222885_Exact_signal_reconstruction_from_highly_incomplete_frequency_information","227598972_L1-Penalization_for_Mixture_Regression_Models","225384392_Gaussian_Model_Selection","235357297_A_significance_test_for_the_LASSO","227375784_Regression_shrinkage_selection_via_the_LASSO","222496977_Adaptive_thresholding_of_wavelet_coefficients"]}