{"id":"321630690_The_reproducibility_of_research_and_the_misinterpretation_of_p_-values","abstract":"We wish to answer this question: If you observe a ‘significant’ p-value after doing a single unbiased experiment, what is the probability that your result is a false positive? The weak evidence provided by p-values between 0.01 and 0.05 is explored by exact calculations of false positive risks. When you observe p = 0.05, the odds in favour of there being a real effect (given by the likelihood ratio) are about 3: 1. This is far weaker evidence than the odds of 19 to 1 that might, wrongly, be inferred from the p-value. And if you want to limit the false positive risk to 5%, you would have to assume that you were 87% sure that there was a real effect before the experiment was done. If you observe p = 0.001 in a well-powered experiment, it gives a likelihood ratio of almost 100: 1 odds on there being a real effect. That would usually be regarded as conclusive. But the false positive risk would still be 8% if the prior probability of a real effect were only 0.1. And, in this case, if you wanted to achieve a false positive risk of 5% you would need to observe p = 0.00045. It is recommended that the terms ‘significant’ and ‘non-significant’ should never be used. Rather, p-values should be supplemented by specifying the prior probability that would be needed to produce a specified (e.g. 5%) false positive risk. It may also be helpful to specify the minimum false positive risk associated with the observed p-value. Despite decades of warnings, many areas of science still insist on labelling a result of p < 0.05 as ‘statistically significant’. This practice must contribute to the lack of reproducibility in some areas of science. This is before you get to the many other well-known problems, like multiple comparisons, lack of randomization and p-hacking. Precise inductive inference is impossible and replication is the only way to be sure. Science is endangered by statistical misunderstanding, and by senior people who impose perverse incentives on scientists.","authors":["David Colquhoun"],"meta":["December 2017Royal Society Open Science 4(12):171085","DOI:10.1098/rsos.171085"],"references":["318642148_Redefine_Statistical_Significance","315955414_The_ASA's_p_-value_statement_one_year_on","314971049_Why_clinical_trial_outcomes_fail_to_translate_into_benefits_for_patients","303698773_The_Natural_Selection_of_Bad_Science","287772256_How_to_get_good_science","313467264_The_ASA's_Statement_on_p-Values_Context_Process_and_PurposeRonald_L_Wasserstein_American_Statistical_Association_732_North_Washington_Street_Alexandria_VA_22314-1943Correspondenceronamstatorg_Nicole_A","312918187_Testing_precise_hypotheses_with_discussion","305690770_Bayesian_Statistical_Inference_in_Ion-Channel_Models_with_Exact_Missed_Event_Correction","297657015_The_ASA's_Statement_on_p-Values_Context_Process_and_Purpose","284211049_Toward_evidence-based_medical_statistics"]}