{"id":"319256075_Super-Convergence_Very_Fast_Training_of_Residual_Networks_Using_Large_Learning_Rates","abstract":"In this paper, we show a phenomenon where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods, which we named \"super-convergence\". One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate. Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. We also provide an explanation for the benefits of a large learning rate using a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence.","authors":["Leslie Smith","Nicholay Topin"],"meta":["August 2017"],"references":["278660085_Stochastic_Gradient_Descent_Tricks"]}