{"id":"333863766_Fusion_confusion_exploring_ambisonic_spatial_localisation_for_audio-visual_immersion_using_the_McGurk_effect","abstract":"Virtual Reality (VR) is attracting the attention of application developers for purposes beyond entertainment including serious games, health, education and training. By including 3D audio the overall VR quality of experience (QoE) will be enhanced through greater immersion. Better understanding the perception of spatial audio localisation in audio-visual immersion is needed especially in streaming applications where bandwidth is limited and compression is required. This paper explores the impact of audio-visual fusion on speech due to mismatches in a perceived talker location and the corresponding sound using a phenomenon known as the McGurk effect and binaurally rendered Ambisonic spatial audio. The illusion of the McGurk effect happens when a sound of a syllable paired with a video of a second syllable, gives the perception of a third syllable. For instance the sound of /ba/ dubbed in video of /ga/ will lead to the illusion of hearing /da/. Several studies investigated factors involved in the McGurk effect, but a little has been done to understand the audio spatial effect on this illusion. 3D spatial audio generated with Ambisonics has been shown to provide satisfactory QoE with respect to localisation of sound sources which makes it suitable for VR applications but not for audio visual talker scenarios. In order to test the perception of the McGurk effect at different direction of arrival (DOA) of sound, we rendered Ambisonics signals at the azimuth of 0째, 30째, 60째, and 90째 to both the left and right of the video source. The results show that the audio visual fusion significantly affects the perception of the speech. Yet the spatial audio does not significantly impact the illusion. This finding suggests that precise localisation of speech audio might not be as critical for speech intelligibility. It was found that a more significant factor was the intelligibility of speech itself.","authors":["Abubakr Siddig","Alessandro Ragano","Hamed Jahromi","Andrew Hines"],"meta":["June 2019","DOI:10.1145/3304113.3326112","Conference: the 11th ACM Workshop"],"references":["324785721_Streaming_VR_for_immersion_Quality_aspects_of_compressed_spatial_audio","318137386_Forty_Years_After_Hearing_Lips_and_Seeing_Voices_the_McGurk_Effect_Revisited","315907865_Assessing_the_Role_of_the_'Unity_Assumption'_on_Multisensory_Integration_A_Review","306293378_Spatial_Frequency_Requirements_and_Gaze_Strategy_in_Visual-Only_and_Audiovisual_Speech_Perception","271473943_The_role_of_sound_intensity_and_stop-consonant_voicing_on_McGurk_fusions_and_combinations","327642335_AMBIQUAL_-_a_full_reference_objective_quality_metric_for_ambisonic_spatial_audio","317639751_Fixation_Prediction_for_360_Video_Streaming_in_Head-Mounted_Virtual_Reality","311778884_Enhancing_Our_Lives_with_Immersive_Virtual_Reality","274086958_Variability_and_stability_in_the_McGurk_effect_contributions_of_participants_stimuli_time_and_response_type","232598601_Visual_Contribution_to_Speech_Intelligibility_in_Noise"]}