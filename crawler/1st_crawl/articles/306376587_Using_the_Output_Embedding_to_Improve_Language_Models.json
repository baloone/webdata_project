{"id":"306376587_Using_the_Output_Embedding_to_Improve_Language_Models","abstract":"We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. In addition, we offer a new method of regularizing the output embedding. These methods lead to a significant reduction in perplexity.","authors":["Ofir Press","Lior Wolf"],"meta":["August 2016"],"references":["283723770_An_Unsupervised_Model_for_Instance_Level_Subcategorization_Acquisition","275974238_A_Fixed-Size_Encoding_Method_for_Variable-Length_Sequences_with_its_Application_to_Neural_Network_Language_Models","265252627_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate","264826516_SimLex-999_Evaluating_Semantic_Models_With_Genuine_Similarity_Estimation","262877889_Learning_Phrase_Representations_using_RNN_Encoder-Decoder_for_Statistical_Machine_Translation","254464318_Large-scale_learning_of_word_relatedness_with_constraints","234131319_Efficient_Estimation_of_Word_Representations_in_Vector_Space","221346280_Practical_solutions_to_the_problem_of_diagonal_dominance_in_kernel_document_clustering","220873867_Learning_Word_Vectors_for_Sentiment_Analysis","270878536_Better_Word_Representations_with_Recursive_Neural_Networks_for_Morphology","260231515_word2vec_Explained_deriving_Mikolov_et_al's_negative-sampling_word-embedding_method","255563511_Context_Dependent_Recurrent_Neural_Network_Language_Model","238198632_Building_a_large_natural_language_corpus_of_English_The_Penn_Treebank"]}