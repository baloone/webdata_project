{"id":"337920608_The_Unbearable_Shallow_Understanding_of_Deep_Learning","abstract":"This paper analyzes the rapid and unexpected rise of deep learning within Artificial Intelligence and its applications. It tackles the possible reasons for this remarkable success, providing candidate paths towards a satisfactory explanation of why it works so well, at least in some domains. A historical account is given for the ups and downs, which have characterized neural networks research and its evolution from “shallow” to “deep” learning architectures. A precise account of “success” is given, in order to sieve out aspects pertaining to marketing or sociology of research, and the remaining aspects seem to certify a genuine value of deep learning, calling for explanation. The alleged two main propelling factors for deep learning, namely computing hardware performance and neuroscience findings, are scrutinized, and evaluated as relevant but insufficient for a comprehensive explanation. We review various attempts that have been made to provide mathematical foundations able to justify the efficiency of deep learning, and we deem this is the most promising road to follow, even if the current achievements are too scattered and relevant for very limited classes of deep neural models. The authors’ take is that most of what can explain the very nature of why deep learning works at all and even very well across so many domains of application is still to be understood and further research, which addresses the theoretical foundation of artificial learning, is still very much needed.","authors":["Alessio Plebe","Giorgio Mario Grasso"],"meta":["December 2019Minds and Machines 29(4):1-39","DOI:10.1007/s11023-019-09512-8"],"references":["328106290_Computational_Functionalism_for_the_Deep_Learning_Era","326701821_The_Organization_and_Operation_of_Inferior_Temporal_Cortex","326670737_40_years_of_cognitive_architectures_core_cognitive_abilities_and_practical_applications","345780249_Cerebral_Cortex_Principles_of_Operation","344473593_Statistical_Physics_Statics_Dynamics_and_Renormalization","339789954_Finding_structure_in_time","337775006_Realizing_Data_Features_by_Deep_Nets","333975473_Exploring_spatiotemporal_neural_dynamics_of_the_human_visual_cortex","329740202_Squeeze-and-Excitation_Networks","326659300_Invariant_Recognition_Shapes_Neural_Representations_of_Visual_Input","326555945_The_artificial_intelligence_renaissance_Deep_learning_and_the_road_to_human-Level_machine_intelligence","326381669_Large-Scale_High-Resolution_Comparison_of_the_Core_Visual_Object_Recognition_Behavior_of_Humans_Monkeys_and_State-of-the-Art_Deep_Artificial_Neural_Networks","326348914_Doing_the_Impossible_Why_Neural_Networks_Can_Be_Trained_at_All","325966541_Topological_properties_of_the_set_of_functions_generated_by_neural_networks_of_fixed_size","326526043_The_Foundations_of_Deep_Learning_with_a_Path_Towards_General_Intelligence_11th_International_Conference_AGI_2018_Prague_Czech_Republic_August_22-25_2018_Proceedings"]}