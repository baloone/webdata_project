{"id":"301815018_The_Variable_Metric_Forward-Backward_Splitting_Algorithm_Under_Mild_Differentiability_Assumptions","abstract":"We study the variable metric forward-backward splitting algorithm for convex minimization problems without the standard assumption of the Lipschitz continuity of the gradient. In this setting, we prove that, by requiring only mild assumptions on the smooth part of the objective function and using several types of backtracking line search procedures for determining either the gradient descent stepsizes, or the relaxation parameters, one still obtains weak convergence of the iterates and convergence in the objective function values. Moreover, the $o(1/k)$ convergence rate in the function values is obtained if slightly stronger differentiability assumptions are added. Our results extend and unify several studies on variable metric proximal/projected gradient methods. We also illustrate several scenarios where the proposed algorithms can be applied, including problems that involves Banach spaces and smooth functions of divergence type.","authors":["Saverio Salzo"],"meta":["April 2016SIAM Journal on Optimization 27(4):33","DOI:10.1137/16M1073741"],"references":["307204874_On_the_convergence_of_the_forward_backward_splitting_method_with_linesearches","282867192_Modified_Fejer_Sequences_and_Applications","277603404_Variable_Metric_Inexact_Line-Search-Based_Methods_for_Nonsmooth_Optimization","275670577_Splitting_Methods_with_Variable_Metric_for_Kurdyka-Lojasiewicz_Functions_and_General_Convergence_Rates","270824676_On_the_convergence_of_the_forward-backward_splitting_method_with_linesearches","268743112_Variable_Metric_Forward-Backward_Algorithm_for_Minimizing_the_Sum_of_a_Differentiable_Function_and_a_Convex_Function","315573860_Convex_Analysis_and_Monotone_Operator_Theory_in_Hilbert_Spaces","285906939_Numerical_optimization","281488455_New_convergence_results_for_the_scaled_gradient_projection_method","278762029_Signal_recovery_by_proximal_forwardC_backward_splitting"]}