{"id":"303822096_Unifying_Count-Based_Exploration_and_Intrinsic_Motivation","abstract":"We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use sequential density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary sequential density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.","authors":["Marc G. Bellemare","Sriram Srinivasan","Georg Ostrovski","Tom Schaul"],"meta":["June 2016"],"references":["301847678_Asynchronous_Methods_for_Deep_Reinforcement_Learning","292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search","284219262_Prioritized_Experience_Replay","319770330_Prioritized_Experience_Replay","313093649_Dynamic_programming_Mineola_NY_Dover_Publications_Berkes_P_Orban_G_Lengyel_M_Fiser","287250334_Increasing_the_Action_Gap_New_Operators_for_Reinforcement_Learning","286302318_State_of_the_Art_Control_of_Atari_Games_Using_Shallow_Reinforcement_Learning","286182155_Skip_context_tree_switching","283613414_An_intrinsic_reward_mechanism_for_efficient_exploration","282403663_Variational_Information_Maximisation_for_Intrinsically_Motivated_Reinforcement_Learning"]}