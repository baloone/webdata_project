{"id":"221497234_Stochastic_Linear_Optimization_under_Bandit_Feedback","abstract":"In the classical stochastic k-armed bandit problem, ineachofasequenceofT rounds, adecisionmaker chooses one of k arms and incurs a cost chosen from an unknown distribution associated with that arm. The goal is to minimize regret, defined as the difference between the cost incurred by the algo- rithm and the optimal cost. In the linear optimization version of this problem (firstconsideredbyAuer(2002)), weviewthearms as vectors in Rn, and require that the costs be lin- ear functions of the chosen vector. As before, it is assumed that the cost functions are sampled in- dependently from an unknown distribution. In this setting, the goal is to find algorithms whose run- ning time and regret behave well as functions of the number of rounds T and the dimensionality n (rather than the number of arms, k, which may be exponential in n or even infinite). We give a nearly complete characterization of this problem in terms of both upper and lower bounds for the regret. In certain special cases (such as when the decision region is a polytope), the regret is polylog(T). In general though, the optimal re- gret is ( p T) â€” our lower bounds rule out the possibility of obtaining polylog(T) rates in gen- eral. We present two variants of an algorithm based on the idea of \"upper confidence bounds.\" The first, due to Auer (2002), but not fully analyzed, obtains regret whose dependence on n and T are both es- sentially optimal, but which may be computation- ally intractable when the decision set is a polytope. The second version can be efficiently implemented when the decision set is a polytope (given as an in- tersection of half-spaces), but gives up a factor of p n in the regret bound. Our results also extend to the setting where the set of allowed decisions may change over time.","authors":["Varsha Dani","Thomas P. Hayes","Sham M. Kakade"],"meta":["January 2008","SourceDBLP","Conference: 21st Annual Conference on Learning Theory - COLT 2008, Helsinki, Finland, July 9-12, 2008"],"references":["220618396_Computationally_Related_Problems","220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem","2427808_Associative_Reinforcement_Learning_using_Linear_Probabilistic_Concepts","247441127_Sample_Mean_Based_Index_Policies_with_Olog_n_Regret_for_the_Multi-Armed_Bandit_Problem","239292007_Asymptotically_efficient_adaptive_allocation_rules1","233820480_Bandit_Problems_Sequential_Allocation_of_Experiments","221619250_The_Price_of_Bandit_Information_for_Online_Optimization","221591339_Adaptive_routing_with_end-to-end_feedback_Distributed_learning_and_geometric_approaches","220320299_Using_Confidence_Bounds_for_Exploitation-Exploration_Trade-offs","201976486_Some_Aspects_of_the_Sequential_Design_of_Experiments","38362521_On_Tail_Probabilities_for_Martingales"]}