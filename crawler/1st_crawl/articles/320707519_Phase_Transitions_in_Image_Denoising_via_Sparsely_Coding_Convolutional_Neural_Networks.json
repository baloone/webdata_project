{"id":"320707519_Phase_Transitions_in_Image_Denoising_via_Sparsely_Coding_Convolutional_Neural_Networks","abstract":"Neural networks are analogous in many ways to spin glasses, systems which are known for their rich set of dynamics and equally complex phase diagrams. We apply well-known techniques in the study of spin glasses to a convolutional sparsely encoding neural network and observe power law finite-size scaling behavior in the sparsity and reconstruction error as the network denoises 32$\\times$32 RGB CIFAR-10 images. This finite-size scaling indicates the presence of a continuous phase transition at a critical value of this sparsity. By using the power law scaling relations inherent to finite-size scaling, we can determine the optimal value of sparsity for any network size by tuning the system to the critical point and operate the system at the minimum denoising error.","authors":["Jacob Carroll","Nils Carlson","Garrett Kenyon"],"meta":["October 2017"],"references":["306218037_Learning_multiple_layers_of_features_from_tiny_images","292215395_Neural_networks_and_physical_systems_with_emergent_collective_computational_abilities","286491554_Critical_dynamics_A_field_theory_approach_to_equilibrium_and_non-equilibrium_scaling_behavior","263201073_Replicating_Kernels_with_a_Short_Stride_Allows_Sparse_Reconstructions_with_Fewer_Independent_Kernels","260869557_Introduction_to_The_Theory_of_Neural_Computation","244445521_The_Theory_of_Spin_Glasses_and_Neural_Networks","235409941_Scaling_and_Renormalization_in_Statistical_Physics","5414092_Sparse_Coding_via_Thresholding_and_Local_Competition_in_Neural_Circuits"]}