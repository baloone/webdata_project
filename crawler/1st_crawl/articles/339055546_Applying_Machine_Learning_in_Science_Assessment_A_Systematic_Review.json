{"id":"339055546_Applying_Machine_Learning_in_Science_Assessment_A_Systematic_Review","abstract":"Machine learning (ML) is an emergent computerized technology that relies on algorithms built by 'learning' from training data rather than 'instruction', and which holds great potential to revolutionize science assessment. This study systematically reviewed the efforts in literature aiming to apply ML in science assessment through a triangle framework with technical, validity, and pedagogical features on three vertices. After a comprehensive search and filter of the related literature, we included 49 articles in the review study. We found that a majority of the studies focused on the validity vertex, as compared to the other two vertices. The existing studies primarily involve text recognition, classification, and scoring with an emphasis on constructing scientific explanations, with a vast range of human-machine agreement measures. To achieve the agreement measures, most of the studies employed a cross-validation method, rather than self-or split-validation. One of the primary contributions of employing ML is that such an approach allows many complex assessments (e.g., constructed responses) to be used by teachers without the burden of human scoring, which saves both time and cost. Even though many platforms used a variety of algorithms, most studies used supervised ML, which relies on Extraction of attributes from student work that was first coded by humans to achieve automaticity, rather than semi-or unsupervised ML. We found that 24 out of the 49 studies were explicitly embedded in science learning activities, such as scientific inquiry and argumentation, potentially to provide learning feedback or learning guidance. Few studies directly studied how to use ML-based assessments to support teachers. This study identifies existing research gaps and suggests that all three vertices of the ML triangle should be addressed in future assessment studies, with an emphasis on the pedagogy and technology features.","authors":["Xiaoming Zhai","Yue Yin","James W. Pellegrino","Kevin C Haudek"],"meta":["February 2020Studies in Science Education","DOI:10.1080/03057267.2020.1735757","Project: DUE 1323162"],"references":["333639641_Applying_Machine_Learning_in_Science_Assessment_Opportunity_and_Challenge","331747562_Automated_text_scoring_and_real-time_adjustable_feedback_Supporting_revision_of_scientific_arguments_involving_uncertainty","324946497_AI_researchers_allege_that_machine_learning_is_alchemy","324930071_Has_artificial_intelligence_become_alchemy","332824319_Guiding_collaborative_revision_of_science_explanations","329940047_Educational_Assessment_What_to_Watch_in_a_Rapidly_Changing_World","326589854_A_peer_assessment_method_to_provide_feedback_consistent_grading_and_reduce_students'_burden_in_massive_teaching_settings","326484287_Designing_educational_systems_to_support_enactment_of_the_Next_Generation_Science_Standards","324645832_Designing_evaluating_and_deploying_automated_scoring_systems_with_validity_in_mind_Methodological_design_decisions","324555641_The_effectiveness_of_machine_score-ability_ratings_in_predicting_automated_scoring_performance"]}