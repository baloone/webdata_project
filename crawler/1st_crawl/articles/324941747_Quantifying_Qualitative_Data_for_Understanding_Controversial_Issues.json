{"id":"324941747_Quantifying_Qualitative_Data_for_Understanding_Controversial_Issues","abstract":"Understanding public opinion on complex controversial issues such as 'Legalization of Marijuana' and 'Gun Rights' is of considerable importance for a number of objectives such as identifying the most divisive facets of the issue, developing a consensus, and making informed policy decisions. However, an individual's position on a controversial issue is often not just a binary support-or-oppose stance on the issue, but rather a conglomerate of nuanced opinions and beliefs on various aspects of the issue. These opinions and beliefs are often expressed qualitatively in free text in issue-focused surveys or on social media. However, quantifying vast amounts of qualitative information remains a significant challenge. The goal of this work is to provide a new approach for quantifying qualitative data for the understanding of controversial issues. First, we show how we can engage people directly through crowdsourcing to create a comprehensive dataset of assertions (claims, opinions, arguments, etc.) relevant to an issue. Next, the assertions are judged for agreement and strength of support or opposition, again by crowdsourcing. The collected Dataset of Nuanced Assertions on Controversial Issues (NAoCI dataset) consists of over 2,000 assertions on sixteen different controversial issues. It has over 100,000 judgments of whether people agree or disagree with the assertions, and of about 70,000 judgments indicating how strongly people support or oppose the assertions. This dataset allows for several useful analyses that help summarize public opinion. Across the sixteen issues, we find that when people judge a large set of assertions they often do not disagree with the individual assertions that the opposite side makes, but that they differently judge the relative importance of these assertions. We show how assertions that cause dissent or consensus can be identified by ranking the whole set of assertions based on the collected judgments. We also show how free-text assertions in social media can be analyzed in conjunction with the crowdsourced information to quantify and summarize public opinion on controversial issues.","authors":["Michael Wojatzki","Saif M. Mohammad","Torsten Zesch","Svetlana Kiritchenko"],"meta":["May 2018","Conference: LREC 2018, Eleventh International Conference on Language Resources and EvaluationAt: Miyazaki, Japan"],"references":["324834218_SemEval-2018_Task_1_Affect_in_Tweets","318741851_Best-Worst_Scaling_More_Reliable_than_Rating_Scales_A_Case_Study_on_Sentiment_Intensity_Annotation","311316893_Overview_of_NLPCC_Shared_Task_4_Stance_Detection_in_Chinese_Microblogs","305342140_Capturing_Reliable_Fine-Grained_Sentiment_Associations_by_Crowdsourcing_and_Best-Worst_Scaling","305334597_SemEval-2016_Task_6_Detecting_Stance_in_Tweets","311990092_Identifying_Stance_by_Analyzing_Political_Discourse_on_Twitter","306094415_Summarizing_Multi-Party_Argumentative_Conversations_in_Reader_Comment_on_News","306093899_Which_argument_is_more_convincing_Analyzing_and_predicting_convincingness_of_Web_arguments_using_bidirectional_LSTM","305622270_Sentiment_Analysis_and_Opinion_Mining","301404993_Abstractive_Summarization_of_Product_Reviews_Using_Discourse_Structure"]}