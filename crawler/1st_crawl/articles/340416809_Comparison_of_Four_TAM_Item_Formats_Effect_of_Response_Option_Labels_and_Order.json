{"id":"340416809_Comparison_of_Four_TAM_Item_Formats_Effect_of_Response_Option_Labels_and_Order","abstract":"The purpose of this research was to investigate the effect of manipulating item formats for a revised version of the Technology Acceptance Model (TAM) questionnaire. The TAM has 12 items, six assessing perceived usefulness (PU) and six assessing perceived ease of use (PEU). Its original purpose was to quantify likelihood of technology acceptance, using an item format with end anchors of \"Likely\" on the left and \"Unlikely\" on the right, and seven response options (from left to right) of \"Extremely, Quite, Slightly, Neither, Slightly, Quite, Extremely.\" To revise the TAM to measure user experience (UX), it is necessary to change the items from assessments of likelihood to agreement. In this study, 546 participants rated their experience using IBM Notes with one of four versions of the modified TAM created by crossing two independent variables: Response format (labels or numbers) and Response order (increasing from right-to-left or from left-to-right), with participants about evenly divided among the four formats. A check on ratings of overall experience showed no significant difference as a function of format group, with similar nonsignificant results for the overall TAM scores. An analysis of variance examining the main effects and interaction of the two independent variables (response format and order) on TAM scores was also nonsignificant at similar levels. Factor analyses for each version produced the same alignment of items with the PU and PEU factors, consistent with the item alignment in the original TAM research. Beta weights for regression models predicting likelihood-of-use and overall experience from PU and PEU ratings were very similar for all four versions. The results indicate that the item format differences did not lead to any important differences in the magnitude or structure of TAM measurement, but there were significantly more response errors when the magnitude of agreement increased from right to left.","authors":["James R. Lewis"],"meta":["August 2019"],"references":["330224903_Comparison_of_Item_Formats_Agreement_vs_Item-Specific_Endpoints","321278772_Revisiting_the_Factor_Structure_of_the_System_Usability_Scale","319394819_SUS_--_a_quick_and_dirty_usability_scale","314261952_User_Experience_Rating_Scales_with_7_11_or_101_Points_Does_It_Matter","270605799_The_Effect_of_Labeling_and_Numbering_of_Response_Scales_on_the_Likelihood_of_Response_Bias","262344995_UMUX-LITE_when_there's_no_time_for_the_SUS","230786756_Psychometric_evaluation_of_the_post-study_system_usability_questionnaire_The_PSSUQ","221516410_When_designing_usability_questionnaires_does_it_hurt_to_be_positive","328360884_Measuring_Perceived_Usability_SUS_UMUX_and_CSUQ_Ratings_for_Four_Everyday_Products","322348620_Measuring_Perceived_Usability_The_CSUQ_SUS_and_UMUX","312984568_Perceived_usefulness_perceived_ease_and_user_acceptance_of_information_technology","281161362_Measuring_Perceived_Usability_The_SUS_UMUX-LITE_and_AltUsability","272830039_Quantifying_the_User_Experience_Practical_Statistics_for_User_Research","222425108_Abstract_Empirical_evaluation_of_the_revised_end_user_computing_acceptance_model","51173310_Studies_Comparing_Numerical_Rating_Scales_Verbal_Rating_Scales_and_Visual_Analogue_Scales_for_Assessment_of_Pain_Intensity_in_Adults_A_Systematic_Literature_Review","36313145_Measurement_of_computer_user_satisfaction"]}